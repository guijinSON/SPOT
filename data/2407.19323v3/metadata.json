{
  "doi": "2407.19323v3",
  "content": [
    {
      "type": "text",
      "text": "Reconstructing textureless areas in MVS poses challenges due to the absence of reliable pixel correspondences within fixed patch. Although certain methods employ patch deformation to expand the receptive field, their patches mistakenly skip depth edges to calculate areas with depth discontinuity, thereby causing ambiguity. Consequently, we introduce Multi-granularity Segmentation Prior Multi-View Stereo (MSP-MVS). Specifically, we first propose multi-granularity segmentation prior by integrating multi-granularity depth edges to restrict patch deformation within homogeneous areas. Moreover, we present anchor equidistribution that bring deformed patches with more uniformly distributed anchors to ensure an adequate coverage of their own homogeneous areas. Furthermore, we introduce iterative local search optimization to represent larger patch with sparse representative candidates, significantly boosting the expressive capacity for each patch. The state-of-the-art results on ETH3D and Tanks & Temples benchmarks demonstrate the effectiveness and robust generalization ability of our proposed method.\n\nMulti-view Stereo (MVS) aims to rebuild dense 3D scenes or objects from a series of calibrated RGB images. It has a broad range of applications in fields such as virtual reality, 3D printing, and autonomous driving, leading to numerous efficient MVS frameworks (Kuhn et al. 2017; Shen 2013; Zheng et al. 2014; Furukawa and Ponce 2010). Nonetheless, despite the rapidly improving accuracy of the latest methods (Yao et al. 2018; Luo et al. 2019), these techniques frequently face challenges when dealing with textureless areas.\n\nRecently, PatchMatch Stereo based-methods (Heise et al. 2013; Kuhn, Lin, and Erdler 2019) have accomplished remarkable successes in reconstructing dense 3D models from large-scale imagery. These methods typically follow an organized pipeline that includes initialization, propagation and refinement, positioning them uniquely suited for reconstructing scenes with unstructured viewpoints. However, due to the unreliable photometric consistency, most methods (Schönberger et al. 2016; Xu et al. 2020; Yuan et al. 2024a) still exhibit unsatisfactory performance when dealing with large textureless areas.\n\nSome traditional methods (Xu and Tao 2019, 2020; Xu et al. 2022) leverage multi-scale geometric consistency and triangulated planar prior to address this challenge. Through obtaining planar hypotheses at coarser scales and propagating reliable ones into finer scales, these methods manage to effectively reconstruct partial textureless areas. Yet, the integration of larger textureless areas remains problematic due to their limited patch sizes. Partial approaches (Zhang et al. 2015; Romanoni and Matteucci 2019; Kuhn, Lin, and Erdler 2019) attempt to utilize superpixels for 3D planar surface fitting to alleviate the problem. However, constraints such as a fixed threshold and strict assumption of data distribution result in poor planarization for certain superpixels. APD-MVS (Wang et al. 2023) utilizes an adaptive deformable patch with cascade architecture. Nevertheless, excessive PatchMatch iterations often make the process time-consuming, leading it impractical for application on large-scale datasets.\n\nSome learning-based methods, such as (Yang et al. 2020; Luo et al. 2019) leverage a coarse-to-fine cascading architecture to address the issue of textureless areas. By refining and upsampling depth maps at low resolution, these methods can effectively expand the receptive field while decreasing the cost volumes. However, these methods typically require substantial memory and time consumption. To circumvent this, several methods like (Wang et al. 2021) attempt to employ lightweight gated recurrent units (GRU) to encode the depth probability distribution for each pixel. However, the adoption of GRU makes capacity for spatial recognition becomes constrained, thereby exhibit limitations in reconstructing textureless areas. Other methods, like (Sormann et al. 2023; Ma et al. 2021) introduce an epipolar module to assemble high-resolution images into limited-size cost volumes. However, they often exhibit limited generalization, inhibiting their application in reconstructing scenarios with diverse characteristics.\n\nLarge man-made scenarios are frequently characterized by textureless areas across varying scales, which can typically be predicted through their surrounding reliable pixels. Therefore, an inspiring idea is to identify textureless areas across different scales and extract their reliable pixels for"
    },
    {
      "type": "text",
      "text": "plane fitting, thereby achieving both reconstruction of textureless areas and refinement of detailed regions.\n\nTherefore, we propose TSAR-MVS, a generalized post-processing MVS method that sequentially applies confidence-based outliers filtering, superpixel-based local refinement, and feature-based area generation to planarize textureless areas. Its distinctive design enables it to be applied after most methods and enhance their reconstruction results. Although ACMP (Xu and Tao 2020) constructs triangulation planar priors, it not only heavily relies on the precision of each reliable pixel but also struggles from incomplete image coverage. Differently, our method can exploit more robust and comprehensive plane prior insight, thus can recover precise and complete planes for reconstruction. Moreover, unlike TAPA-MVS (Romanoni and Matteucci 2019) which are limited to planarizing localized areas, our textureless-aware segmentation can extract textureless areas across broader scales, thereby not limited by superpixels size.\n\nSpecifically, we first introduce a joint hypothesis filter consisting of a confidence estimator and a disparity discontinuity detector to provisionally remove outliers. Inspired by (Kuhn et al. 2020), we first combine three confidence features extracted from the matching cost to filter wrong estimates. Furthermore, we maintain areas with continuous disparity, underpinning the assumption brought by (Hirschmuller 2008). Through integrating both confidence estimator and disparity discontinuities into an aggregation score, we can treat small disparity discontinuities as outliers, while preserving reliable object edges which are continuous in disparity.\n\nSubsequently, to propagate the confident pixels with correct depth, we propose an iterative correlation refinement procedure. In each iteration, we first cluster pixels with similar perceptual features into same superpixels, then we apply RANSAC to planarize each superpixel, resulting in pixels within a superpixel sharing a consistent depth. Following this, a pixel-wise weighted median filtering (WMF) is used to propagate the correct depth to the surrounding areas. This iterative process is conducted over several rounds to ensure the propagation of reliable depth. Finally, another WMF is adopted to interpolate the depths and normals of the remaining pixels.\n\nAdditionally, we propose a textureless-aware segmentation method combining edge detection and line detection. Typically, in man-made scenarios (Schops et al. 2017; Strecha et al. 2008; Knapitsch et al. 2017), pixels within the same textureless area exhibit a high degree of similarity. Thus, we attempt to separate textureless areas and independently estimate the depth of their homogeneous surfaces. Specifically, we first apply the Roberts edge detection to extract high-frequency information, followed by the Hough line detection that enhances region discrimination by concatenating separated edges into lines. By aggregating residual pixels into miscellaneous areas, we can distinguish and further planarize them, thereby improving reconstructing result.\n\nExperiments on extensive datasets, including the ETH3D high-resolution dataset, Tanks & Temples dataset and Strecha dataset demonstrate that our method significantly outperforms other MVS methods. Furthermore, we demonstrate that our method can be incorporated as a component into other reconstruction pipelines to improve performance.\n\nIn summary, our contributions are as follows:\n1) We propose a novel joint hypothesis filter by adaptly merging a confidence estimator and disparity discontinuities to yield an aggregation score that effectively eliminates outliers.\n2) We present an iterative correlation refinement procedure that iteratively employs RANSAC and WMF to refine pixel estimates based on the surrounding confidence neighbors.\n3) We introduce a textureless-aware segmentation technique that merges edge and line detection, thereby achieving the discrimination and planarization of textureless areas.\n4) Through extensive experiments, we verified that our method achieves state-of-the-art results and possesses strong generalization capability.\n\nThe fundamental concept of PatchMatch (Barnes et al. 2009) employs random initialization, propagation and refinement to match approximate patch pairs between images. PMS (Bleyer, Rhemann, and Rother 2011) pioneered to introduce this in stereo vision, facilitating the emergence of numerous further algorithms. For acceleration, Gipuma (Galliani, Lasinger, and Schindler 2015) proposes checkerboard diffusion propagation to achieve algorithm deployment on GPUs. To reconstruct textureless areas, ACMM (Xu and Tao 2019) presents both multi-scale architecture and multi-view consistency for guidance, improved by ACMMP (Xu et al. 2022) which incorporates a probabilistic graphical model as plane priors for reconstruction. Different, TAPA-MVS (Romanoni and Matteucci 2019), PCF-MVS (Kuhn, Lin, and Erdler 2019) and TSAR-MVS (Yuan et al. 2024b) attempt to employ superpixel planarization to reconstruct textureless areas, while their performance are severely affected by limited superpixel size. Moreover, MAR-MVS (Xu et al. 2020) utilizes differential geometry and epipolar constraints to mitigates matching ambiguity. HPM-MVS (Ren et al. 2023) further introduces non-local extensible sampling patterns to avoid locally optimal solutions within textureless areas.\n\nConcerning patch deformation, APD-MVS (Wang et al. 2023) separated patches of unreliable pixels into several outward-spreading anchors with high reliability. Differently, SD-MVS (Yuan et al. 2024a) adopts the SAM (Kirillov et al."
    },
    {
      "type": "text",
      "text": "2023) to extract depth edges for patch deformation, while its single-granularity lacks accuracy for patch deformation.\n\nMVSNet (Yao et al. 2018) is the first to utilizes differentiable 3D cost volumes to construct deep neural network, thereby becoming the foundation for abundant subsequent methods. To reduce memory, Cas-MVSNet (Yang et al. 2020) adopted a coarse-to-fine strategy to retain depths across multi-scale. IterMVS-LS (Wang et al. 2022a) leveraged a lightweight probability estimator to encodes depth distribution during regularization. For feature extraction, AAR-MVSNet (Wei et al. 2021) employed deformable convolutions to achieve adaptive feature aggregation. Differently, MVSTER (Wang et al. 2022b) leveraged the transformer structure to introduce the multi-head attention mechanism. Concerning geometrical topology, Geo-MVSNet (Zhang et al. 2023) proposed a two-branch geometry fusion network to enhance geometry perception. Regarding surface topology, RA-MVSNet (Zhang, Zhu, and Lin 2023) associated hypothesis planes with surface patches to enhance perception field. However, most learning-based methods suffer from either unaffordable memory usage or limited generalization capabilities, leaving room for further improvement.\n\nDue to the absence of depth edge as constraints, APD-MVS erroneously skips depth edges to select anchor within heterogeneous areas, thereby causing potential inaccuracy. While by introducing semantic awareness and granularity richness, Semantic-SAM can achieve panoramic segmentation at any granularity level, which contains potential depth edges. Therefore, we adopt the Semantic-SAM to extract multi-granularity depth edges for patch deformation.\n\nSince APD-MVS lacks depth edge guidance, its anchors typically skip depth edges to be selected within green heterogeneous areas, causing potential inaccuracy. Therefore, we attempt to extract depth edges as guidance for patch deformation. However, without accurate depth maps, it is rather difficult to extract depth edges. Although depth edges and image edges have a certain degree of co-occurrence, they are easily confused by factors such as illumination, shadow, occlusion, etc. Differently, by introducing semantic awareness and granularity richness, Semantic-SAM can achieve panoramic segmentation at any granularity level, which may contain potential depth edges. Therefore, we attempt to adopt Semantic-SAM to extract multi-granularity depth edges for patch deformation.\n\nAlthough edge-confined patch deformation effectively improves the reliability of deformed patches compared to APD-MVS, it may cause the loss of anchors within sectors fully enclosed by depth edges. Addressing this, we propose adaptive equidistribution, which uniformly divides the homogeneous area of each unreliable pixel into eight sectors based on the number of its surrounding reliable pixels to recover previously-discarded anchors.\n\nWe evaluate our work on ETH3D benchmark (Schops et al. 2017) and Tanks & Temples benchmark (TNT) (Knapitsch et al. 2017). We compare our work against learning-based methods including PatchMatchNet (Wang et al. 2021), IterMVS-LS (Wang et al. 2022a), MVSTER (Wang et al. 2022b), EPP-MVSNet (Ma et al. 2021), EP-Net (Su and Tao 2023) and traditional methods including PCF-MVS (Kuhn, Lin, and Erdler 2019), ACMM (Xu and Tao 2019), ACMMP (Xu et al. 2022), SD-MVS (Yuan et al. 2024a), APD-MVS (Wang et al. 2023) and HPM-MVS (Ren et al. 2023).\n\nQualitative results on ETH3D are illustrated. It is evident that our method delivers the most complete and realistic reconstructed point clouds, especially in large textureless areas like the floors and walls, without introducing detail distortions. More qualitative results on the ETH3D and the TNT dataset can be referred in supplementary material.\n\nQuantitative results on the ETH3D and the TNT dataset are respectively presented. Note that the first group is learning-based methods and the second is traditional methods. Meanwhile, we mark the best results in bold and the second-best results in red. Our method achieves the highest F1 score and completeness on the ETH3D dataset and the TNT Intermediate dataset, validating its state-of-the-art performance and strong generalization capability. Meanwhile, our method achieves the second best results in the TNT advanced datasets, falling short by less than 0.3% in F1 score compared to EPNet (Su and Tao 2023). Furthermore, we achieve comparative runtime compared to SOTA methods like APD-MVS and HPM-MVS. A detailed memory and runtime comparison is shown in supplementary material.\n\nMulti-granularity Segmentation Prior  Regarding segmentation prior, we separately remove SAM prior (w/o. SAM), multi-granularity SAM prior (w/o. Her), aggregation (w/o. Agr.), and edge correction (w/o. CRF.). Compared to w/o. SAM, w/o. Her. which only introduce single-granularity SAM prior led to notable improvements, validating the effectiveness of depth edges for patch deformation. Additionally, Both w/o. Her. and w/o. Agr. produce similar better results than w/o. CRF., yet are inferior to MSP-MVS, indicating that both multi-granularity SAM prior and aggregation are equally crucial than edge correction.\n\nAnchor Equidistribution  Concerning anchor selection, we respectively remove sector averaging (w/o. Avg.), anchor clustering (w/o. Cls.) and both (w/o. Equ.). w/o. Equ. has neither sector averaging nor anchor clustering achieves the worst results. w/o. Cls. slightly outperformed w/o. Avg., yet fell short to MSP-MVS, implying that sector averaging contribute more than anchor clustering."
    },
    {
      "type": "text",
      "text": "Iterative Local Search Optimization  For candidate optimization, we respectively eliminated the variance component in ILS (w/o. Var.), the cost component in ILS (w/o. Cst.), and both (w/o. ILS). Due to the absence of ILS, w/o. ILS achieves the poorest outcomes. w/o. Var. yields better results than w/o. Cst., suggesting that cost plays a more critical role than variance during candidate optimization.\n\nIn this paper, we introduce MSP-MVS, a novel MVS method that leverages multi-granularity segmentation prior to constraint patch deformation within homogeneous areas. Specifically, we first leverage Semantic-SAM to extract multi-granularity depth edges and further aggregate and correct them for accurate patch deformation. Subsequently, we introduce anchor equidistribution to offer patch more uniformly distributed anchors. Moreover, we present ILS optimization to represent patches with sparse representative candidates. The state-of-the-art results on the ETH3D and Tanks & Temples benchmarks validate the effectiveness of our proposed method. In the future, we attempt to leverage Semantic-SAM to extract various 3D instances from point clouds, thereby facilitating object-level scene editing and improving scene’s interactivity.\n\nBarnes, C.; Shechtman, E.; Finkelstein, A.; and Goldman, D. B. 2009. PatchMatch: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3): 24.\n\nBleyer, M.; Rhemann, C.; and Rother, C. 2011. PatchMatch Stereo - Stereo Matching with Slanted Support Windows. In Hoey, J.; McKenna, S. J.; and Trucco, E., eds., British Mach. Vis. Conf. (BMVC), 1–11.\n\nFurukawa, Y.; and Ponce, J. 2010. Accurate, Dense, and Robust Multiview Stereopsis. IEEE Trans. Pattern Anal. Mach. Intell., 32(8): 1362–1376.\n\nGalliani, S.; Lasinger, K.; and Schindler, K. 2015. Massively Parallel Multiview Stereopsis by Surface Normal Diffusion. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV).\n\nHeise, P.; Klose, S.; Jensen, B.; and Knoll, A. 2013. Pm-Huber: Patchmatch with Huber Regularization for Stereo Matching. In Proceedings of the IEEE International Conference on Computer Vision, 2360–2367.\n\nHirschmuller, H. 2008. Stereo Processing by Semiglobal Matching and Mutual Information. IEEE Trans. Pattern Anal. Mach. Intell., 30(2): 328–341.\n\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-Y.; Dollár, P.; and Girshick, R. 2023. Segment Anything. arxiv:2304.02643.\n\nKnapitsch, A.; Park, J.; Zhou, Q.-Y.; and Koltun, V. 2017. Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction. ACM Transactions on Graphics (ToG), 36(4): 1–13.\n\nKuhn, A.; Hirschmüller, H.; Scharstein, D.; and Mayer, H. 2017. A TV Prior for High-Quality Scalable Multi-View Stereo Reconstruction. Int. J. Comput. Vis., 124: 2–17.\n\nKuhn, A.; Lin, S.; and Erdler, O. 2019. Plane Completion and Filtering for Multi-View Stereo Reconstruction. In Pattern Recognition, 18–32.\n\nKuhn, A.; Sormann, C.; Rossi, M.; Erdler, O.; and Fraundorfer, F. 2020. DeepC-MVS: Deep Confidence Prediction for Multi-View Stereo Reconstruction. In Int. Conf. 3D Vis. (3DV), 404–413.\n\nLuo, K.; Guan, T.; Ju, L.; Huang, H.; and Luo, Y. 2019. P-MVSNet: Learning Patch-Wise Matching Confidence Aggregation for Multi-View Stereo. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 10451–10460.\n\nMa, X.; Gong, Y.; Wang, Q.; Huang, J.; Chen, L.; and Yu, F. 2021. Epp-Mvsnet: Epipolar-assembling Based Depth Prediction for Multi-View Stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 5732–5740.\n\nRen, C.; Xu, Q.; Zhang, S.; and Yang, J. 2023. Hierarchical Prior Mining for Non-Local Multi-View Stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 3611–3620.\n\nRomanoni, A.; and Matteucci, M. 2019. Tapa-Mvs: Textureless-aware Patchmatch Multi-View Stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 10413–10422.\n\nSchönberger, J. L.; Zheng, E.; Frahm, J.-M.; and Pollefeys, M. 2016. Pixelwise View Selection for Unstructured Multi-View Stereo. In Proc. Eur. Conf. Comput. Vis. (ECCV), 501–518.\n\nSchops, T.; Schonberger, J. L.; Galliani, S.; Sattler, T.; Schindler, K.; Pollefeys, M.; and Geiger, A. 2017. A Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR).\n\nShen, S. 2013. Accurate Multiple View 3D Reconstruction Using Patch-Based Stereo for Large-Scale Scenes. IEEE Trans. Image Process., 22(5): 1901–1914.\n\nSormann, C.; Santellani, E.; Rossi, M.; Kuhn, A.; and Fraundorfer, F. 2023. DELS-MVS: Deep Epipolar Line Search for Multi-View Stereo. In Proc. IEEE/CVF Winter Conf. Appl. of Comput. Vis. (WACV), 3086–3095.\n\nStrecha, C.; von Hansen, W.; Van Gool, L.; Fua, P.; and Thoennessen, U. 2008. On benchmarking camera calibration and multi-view stereo for high resolution imagery. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 1–8.\n\nSu, W.; and Tao, W. 2023. Efficient Edge-Preserving Multi-View Stereo Network for Depth Estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 2348–2356.\n\nWang, F.; Galliani, S.; Vogel, C.; and Pollefeys, M. 2022a. IterMVS: Iterative probability estimation for efficient multi-view stereo. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 8606–8615."
    },
    {
      "type": "text",
      "text": "Wang, F.; Galliani, S.; Vogel, C.; Speciale, P.; and Pollefeys, M. 2021. PatchmatchNet: Learned Multi-View Patchmatch Stereo. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 14194–14203.\n\nWang, X.; Zhu, Z.; Huang, G.; Qin, F.; Ye, Y.; He, Y.; Chi, X.; and Wang, X. 2022b. MVSTER: Epipolar Transformer for Efficient Multi-View Stereo. In European Conference on Computer Vision, 573–591. Springer.\n\nWang, Y.; Zeng, Z.; Guan, T.; Yang, W.; Chen, Z.; Liu, W.; Xu, L.; and Luo, Y. 2023. Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 1621–1630.\n\nWei, Z.; Zhu, Q.; Min, C.; Chen, Y.; and Wang, G. 2021. Aa-Rmvsnet: Adaptive Aggregation Recurrent Multi-View Stereo Network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 6187–6196.\n\nXu, Q.; Kong, W.; Tao, W.; and Pollefeys, M. 2022. Multi-Scale Geometric Consistency Guided and Planar Prior Assisted Multi-View Stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\nXu, Q.; and Tao, W. 2019. Multi-Scale Geometric Consistency Guided Multi-View Stereo. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR).\n\nXu, Q.; and Tao, W. 2020. Planar Prior Assisted PatchMatch Multi-View Stereo. In Proc. of the AAAI Conf. Artif. Intell. (AAAI).\n\nXu, Z.; Liu, Y.; Shi, X.; Wang, Y.; and Zheng, Y. 2020. MARMVS: Matching Ambiguity Reduced Multiple View Stereo for Efficient Large Scale Scene Reconstruction. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 5980–5989.\n\nYang, J.; Mao, W.; Alvarez, J. M.; and Liu, M. 2020. Cost Volume Pyramid Based Depth Inference for Multi-View Stereo. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 4876–4885.\n\nYao, Y.; Luo, Z.; Li, S.; Fang, T.; and Quan, L. 2018. MVS-Net: Depth Inference for Unstructured Multi-view Stereo. In Proc. Eur. Conf. Comput. Vis. (ECCV).\n\nYuan, Z.; Cao, J.; Li, Z.; Jiang, H.; and Wang, Z. 2024a. SD-MVS: Segmentation-driven Deformation Multi-View Stereo with Spherical Refinement and EM Optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 38(7): 6871–6880.\n\nYuan, Z.; Cao, J.; Wang, Z.; and Li, Z. 2024b. Tsar-Mvs: Textureless-aware Segmentation and Correlative Refinement Guided Multi-View Stereo. Pattern Recognition, 110565.\n\nZhang, C.; Li, Z.; Cheng, Y.; Cai, R.; Chao, H.; and Rui, Y. 2015. MeshStereo: A Global Stereo Model With Mesh Alignment Regularization for View Interpolation. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV).\n\nZhang, Y.; Zhu, J.; and Lin, L. 2023. Multi-View Stereo Representation Revist: Region-Aware MVSNet. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 17376–17385.\n\nZhang, Z.; Peng, R.; Hu, Y.; and Wang, R. 2023. GeoMVS-Net: Learning Multi-View Stereo With Geometry Perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 21508–21518.\n\nZheng, E.; Dunn, E.; Jojic, V.; and Frahm, J. 2014. Patch-Match Based Joint View Selection and Depthmap Estimation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), 1510–1517."
    }
  ],
  "images": [],
  "pdf": "2407.19323v3_err1.pdf"
}