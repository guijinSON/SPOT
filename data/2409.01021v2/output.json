{
  "pages": [
    {
      "page": 1,
      "text": "                          arXiv:2409.01021v1  [cs.CV]  2 Sep 2024\n\n  CONDA: Condensed Deep Association Learning\n                for Co-Salient Object Detection\n\n Long Li1    , Nian Liu3,∗  , Dingwen Zhang1     , Zhongyu Li4, Salman Khan3,5       ,\n           Rao Anwer3     , Hisham Cholakkal3     , Junwei Han1,2,∗   , and\n                              Fahad Shahbaz Khan3,6\n\n                       1  Northwestern Polytechnical University\n  2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n               3 Mohamed bin Zayed University of Artificial Intelligence\n                              4 Xi’an Jiaotong University\n                           5 Australian National University\n                             6  CVL, Linköping University\n\n        Abstract.   Inter-image association modeling is crucial for co-salient ob-\n        ject detection. Despite satisfactory performance, previous methods still\n        have limitations on sufficient inter-image association modeling. Because\n        most of them focus on image feature optimization under the guidance of\n        heuristically calculated raw inter-image associations. They directly rely\n        on raw associations which are not reliable in complex scenarios, and their\n        image feature optimization approach is not explicit for inter-image associ-\n        ation modeling. To alleviate these limitations, this paper proposes a deep\n        association learning strategy that deploys deep networks on raw associa-\n        tions to explicitly transform them into deep association features. Specif-\n        ically, we first create hyperassociations to collect dense pixel-pair-wise\n        raw associations and then deploys deep aggregation networks on them.\n        We design a progressive association generation module for this purpose\n        with additional enhancement of the hyperassociation calculation. More\n        importantly, we propose a correspondence-induced association conden-\n        sation module that introduces a pretext task, i.e. semantic correspon-\n        dence estimation, to condense the hyperassociations for computational\n        burden reduction and noise elimination. We also design an object-aware\n        cycle consistency loss for high-quality correspondence estimations. Ex-\n        perimental results in three benchmark datasets demonstrate the remark-\n        able effectiveness of our proposed method with various training settings.\n        The code is available at: https://github.com/dragonlee258079/CONDA.\n\n        Keywords: Co-salient Object Detection     · Deep Association Learning\n\n1    Introduction\n\nCo-Salient Object Detection (CoSOD) aims to segment salient objects that ap-\npear commonly across a group of related images. Compared to traditional Salient\n\n   ∗ Corresponding authors: {liunian228, junweihan2010}@gmail.com",
      "md": "# CONDA: Condensed Deep Association Learning for Co-Salient Object Detection\n\nLong Li1, Nian Liu3,*, Dingwen Zhang1, Zhongyu Li4, Salman Khan3,5,\nRao Anwer3, Hisham Cholakkal3, Junwei Han1,2,*, and\nFahad Shahbaz Khan3,6\n\n1 Northwestern Polytechnical University\n2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3 Mohamed bin Zayed University of Artificial Intelligence\n4 Xi'an Jiaotong University\n5 Australian National University\n6 CVL, Linköping University\n\n## Abstract\n\nInter-image association modeling is crucial for co-salient object detection. Despite satisfactory performance, previous methods still have limitations on sufficient inter-image association modeling. Because most of them focus on image feature optimization under the guidance of heuristically calculated raw inter-image associations. They directly rely on raw associations which are not reliable in complex scenarios, and their image feature optimization approach is not explicit for inter-image association modeling. To alleviate these limitations, this paper proposes a deep association learning strategy that deploys deep networks on raw associations to explicitly transform them into deep association features. Specifically, we first create hyperassociations to collect dense pixel-pair-wise raw associations and then deploys deep aggregation networks on them. We design a progressive association generation module for this purpose with additional enhancement of the hyperassociation calculation. More importantly, we propose a correspondence-induced association condensation module that introduces a pretext task, i.e. semantic correspondence estimation, to condense the hyperassociations for computational burden reduction and noise elimination. We also design an object-aware cycle consistency loss for high-quality correspondence estimations. Experimental results in three benchmark datasets demonstrate the remarkable effectiveness of our proposed method with various training settings. The code is available at: https://github.com/dragonlee258079/CONDA.\n\n**Keywords:** Co-salient Object Detection · Deep Association Learning\n\n## 1 Introduction\n\nCo-Salient Object Detection (CoSOD) aims to segment salient objects that appear commonly across a group of related images. Compared to traditional Salient\n\n* Corresponding authors: {liunian228, junweihan2010}@gmail.com",
      "images": [
        {
          "name": "page_1.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_1_text_1.jpg",
          "height": 239.44,
          "width": 290.88,
          "x": 162.117,
          "y": 313.519,
          "original_width": 587,
          "original_height": 483,
          "type": "layout_text"
        },
        {
          "name": "page_1_text_2.jpg",
          "height": 22.343,
          "width": 346.327,
          "x": 134.32,
          "y": 623.955,
          "original_width": 699,
          "original_height": 45,
          "type": "layout_text"
        },
        {
          "name": "page_1_sectionHeader_1.jpg",
          "height": 9.738,
          "width": 94.312,
          "x": 134.513,
          "y": 597.726,
          "original_width": 190,
          "original_height": 19,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_1_text_3.jpg",
          "height": 9.528,
          "width": 284.052,
          "x": 162.255,
          "y": 565.304,
          "original_width": 573,
          "original_height": 19,
          "type": "layout_text"
        },
        {
          "name": "page_1_pageHeader_1.jpg",
          "height": 338.334,
          "width": 17.411,
          "x": 17.639,
          "y": 216.274,
          "original_width": 35,
          "original_height": 683,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_1_sectionHeader_2.jpg",
          "height": 32.069,
          "width": 334.217,
          "x": 140.665,
          "y": 114.802,
          "original_width": 674,
          "original_height": 64,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_1_text_4.jpg",
          "height": 33.37,
          "width": 340.679,
          "x": 137.286,
          "y": 169.892,
          "original_width": 688,
          "original_height": 67,
          "type": "layout_text"
        },
        {
          "name": "page_1_listItem_1.jpg",
          "height": 10.222,
          "width": 269.416,
          "x": 144.108,
          "y": 654.997,
          "original_width": 544,
          "original_height": 20,
          "type": "layout_listItem"
        },
        {
          "name": "page_1_text_5.jpg",
          "height": 63.553,
          "width": 331.932,
          "x": 141.444,
          "y": 216.65,
          "original_width": 670,
          "original_height": 128,
          "type": "layout_text"
        },
        {
          "name": "page_1_footnote_1.jpg",
          "height": 10.222,
          "width": 269.416,
          "x": 144.108,
          "y": 654.997,
          "original_width": 544,
          "original_height": 20,
          "type": "layout_footnote"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "heading",
          "lvl": 1,
          "value": "CONDA: Condensed Deep Association Learning for Co-Salient Object Detection",
          "md": "# CONDA: Condensed Deep Association Learning for Co-Salient Object Detection",
          "bBox": {
            "x": 140,
            "y": 1274.65,
            "w": 335,
            "h": 32
          }
        },
        {
          "type": "text",
          "value": "Long Li1, Nian Liu3,*, Dingwen Zhang1, Zhongyu Li4, Salman Khan3,5,\nRao Anwer3, Hisham Cholakkal3, Junwei Han1,2,*, and\nFahad Shahbaz Khan3,6\n\n1 Northwestern Polytechnical University\n2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3 Mohamed bin Zayed University of Artificial Intelligence\n4 Xi'an Jiaotong University\n5 Australian National University\n6 CVL, Linköping University",
          "md": "Long Li1, Nian Liu3,*, Dingwen Zhang1, Zhongyu Li4, Salman Khan3,5,\nRao Anwer3, Hisham Cholakkal3, Junwei Han1,2,*, and\nFahad Shahbaz Khan3,6\n\n1 Northwestern Polytechnical University\n2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3 Mohamed bin Zayed University of Artificial Intelligence\n4 Xi'an Jiaotong University\n5 Australian National University\n6 CVL, Linköping University",
          "bBox": {
            "x": 134,
            "y": 1332.04,
            "w": 343,
            "h": 438.01
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "Abstract",
          "md": "## Abstract",
          "bBox": {
            "x": 0,
            "y": 0,
            "w": 612,
            "h": 792
          }
        },
        {
          "type": "text",
          "value": "Inter-image association modeling is crucial for co-salient object detection. Despite satisfactory performance, previous methods still have limitations on sufficient inter-image association modeling. Because most of them focus on image feature optimization under the guidance of heuristically calculated raw inter-image associations. They directly rely on raw associations which are not reliable in complex scenarios, and their image feature optimization approach is not explicit for inter-image association modeling. To alleviate these limitations, this paper proposes a deep association learning strategy that deploys deep networks on raw associations to explicitly transform them into deep association features. Specifically, we first create hyperassociations to collect dense pixel-pair-wise raw associations and then deploys deep aggregation networks on them. We design a progressive association generation module for this purpose with additional enhancement of the hyperassociation calculation. More importantly, we propose a correspondence-induced association condensation module that introduces a pretext task, i.e. semantic correspondence estimation, to condense the hyperassociations for computational burden reduction and noise elimination. We also design an object-aware cycle consistency loss for high-quality correspondence estimations. Experimental results in three benchmark datasets demonstrate the remarkable effectiveness of our proposed method with various training settings. The code is available at: https://github.com/dragonlee258079/CONDA.\n\n**Keywords:** Co-salient Object Detection · Deep Association Learning",
          "md": "Inter-image association modeling is crucial for co-salient object detection. Despite satisfactory performance, previous methods still have limitations on sufficient inter-image association modeling. Because most of them focus on image feature optimization under the guidance of heuristically calculated raw inter-image associations. They directly rely on raw associations which are not reliable in complex scenarios, and their image feature optimization approach is not explicit for inter-image association modeling. To alleviate these limitations, this paper proposes a deep association learning strategy that deploys deep networks on raw associations to explicitly transform them into deep association features. Specifically, we first create hyperassociations to collect dense pixel-pair-wise raw associations and then deploys deep aggregation networks on them. We design a progressive association generation module for this purpose with additional enhancement of the hyperassociation calculation. More importantly, we propose a correspondence-induced association condensation module that introduces a pretext task, i.e. semantic correspondence estimation, to condense the hyperassociations for computational burden reduction and noise elimination. We also design an object-aware cycle consistency loss for high-quality correspondence estimations. Experimental results in three benchmark datasets demonstrate the remarkable effectiveness of our proposed method with various training settings. The code is available at: https://github.com/dragonlee258079/CONDA.\n\n**Keywords:** Co-salient Object Detection · Deep Association Learning",
          "bBox": {
            "x": 142,
            "y": 1292.65,
            "w": 335,
            "h": 443.38
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "1 Introduction",
          "md": "## 1 Introduction",
          "bBox": {
            "x": 134,
            "y": 1377.02,
            "w": 95,
            "h": 393.02
          }
        },
        {
          "type": "text",
          "value": "Co-Salient Object Detection (CoSOD) aims to segment salient objects that appear commonly across a group of related images. Compared to traditional Salient\n\n* Corresponding authors: {liunian228, junweihan2010}@gmail.com",
          "md": "Co-Salient Object Detection (CoSOD) aims to segment salient objects that appear commonly across a group of related images. Compared to traditional Salient\n\n* Corresponding authors: {liunian228, junweihan2010}@gmail.com",
          "bBox": {
            "x": 134,
            "y": 1332.04,
            "w": 346,
            "h": 475
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [
        {
          "url": "https://orcid.org/0000-0002-1939-5941",
          "text": ", Nian Liu"
        },
        {
          "url": "https://orcid.org/0000-0002-0825-6081",
          "text": ", Dingwen Zhang , Hisham Cholakkal"
        },
        {
          "url": "https://orcid.org/0000-0001-8369-8886",
          "text": ", Zhongyu Li 3"
        },
        {
          "url": "https://orcid.org/0000-0002-9502-1749",
          "text": ","
        },
        {
          "url": "https://orcid.org/0000-0002-9041-2214",
          "text": ", Hisham Cholakkal"
        },
        {
          "url": "https://orcid.org/0000-0002-8230-9065",
          "text": ", Junwei Han Fahad Shahbaz Khan"
        },
        {
          "url": "https://orcid.org/0000-0001-5545-7217",
          "text": ", and"
        },
        {
          "url": "https://orcid.org/0000-0002-4263-3143",
          "text": ""
        },
        {
          "url": "https://github.com/dragonlee258079/CONDA",
          "text": "arXiv:2409.01021v1  [cs.CV]  2 Sep 2024"
        }
      ],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.867,
      "layout": [
        {
          "image": "page_1_text_1.jpg",
          "confidence": 0.981,
          "label": "text",
          "bbox": {
            "x": 0.264,
            "y": 0.393,
            "w": 0.475,
            "h": 0.304
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_text_2.jpg",
          "confidence": 0.965,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.785,
            "w": 0.566,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_sectionHeader_1.jpg",
          "confidence": 0.942,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.754,
            "w": 0.154,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_text_3.jpg",
          "confidence": 0.898,
          "label": "text",
          "bbox": {
            "x": 0.265,
            "y": 0.713,
            "w": 0.464,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_pageHeader_1.jpg",
          "confidence": 0.898,
          "label": "pageHeader",
          "bbox": {
            "x": 0.028,
            "y": 0.273,
            "w": 0.028,
            "h": 0.427
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_sectionHeader_2.jpg",
          "confidence": 0.837,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.228,
            "y": 0.14,
            "w": 0.547,
            "h": 0.044
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_text_4.jpg",
          "confidence": 0.8,
          "label": "text",
          "bbox": {
            "x": 0.223,
            "y": 0.212,
            "w": 0.557,
            "h": 0.044
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_listItem_1.jpg",
          "confidence": 0.753,
          "label": "listItem",
          "bbox": {
            "x": 0.235,
            "y": 0.825,
            "w": 0.44,
            "h": 0.014
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_text_5.jpg",
          "confidence": 0.721,
          "label": "text",
          "bbox": {
            "x": 0.231,
            "y": 0.27,
            "w": 0.542,
            "h": 0.083
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_1_footnote_1.jpg",
          "confidence": 0.662,
          "label": "footnote",
          "bbox": {
            "x": 0.235,
            "y": 0.825,
            "w": 0.44,
            "h": 0.014
          },
          "isLikelyNoise": true
        }
      ]
    },
    {
      "page": 2,
      "text": "2       L Li et al.\n\n (a)                   Raw Inter-Image Associations      (c)     Visual Comparisons\n                       e.g. pixel-, region-, or image-wise similarity ‘Potato’\n                       measurements.\n                                Guidance                                       0\n                           Feature Optimization\n                       e.g. spatial or channel calibration, dynamic\n                       convolution, feature fusion, cross-attention,\n                       and so on. SeeSec. 2.1for details.\n   Images Images Features                   Optimized Features\n (b)\n                         Raw Inter-Image\n                           Associations       Deep\n                       condensed pixel-wise Network\n                        hyperassocitions\n  Images  Images Features                   Deep Association  Image  Raw Asso.  Opt. Fea.  Asso. Fea.\n                                              Features\nFig. 1: Difference of raw-association-based image feature optimization strat-\negy (a) and our proposed deep association learning strategy (b). Our deep\nassociation learning deploys deep learning networks on raw associations to achieve deep\nassociation features. We also present visual samples of our calculated raw associations\n(Raw Asso.), optimized image feature (Opt. Fea.), and our generated deep association\nfeatures (Asso. Fea.) in (c).\n\nObject Detection (SOD) [19–23,26,49,50,52], CoSOD is a more challenging task\nbecause it requires sufficient inter-image association modeling [9].\n           Recently, many advanced works [8,13,16,33,38,41,46,47,51] have emerged and\nachieved impressive performance. These methods first use related image features\nto acquire raw inter-image associations (also called consensus representations),\nand then leverage them as guidance to optimize each image feature, as shown in\nFigure 1 (a). This approach enables the final image feature to implicitly capture\ninter-image cues, thereby achieving the purpose of inter-image association mod-\neling. However, we find this raw-association-based feature optimization strategy\nstill has two limitations: i) they directly rely on raw associations, which are ac-\nquired in heuristic manners, such as pixel-wise [8, 33, 41, 51], region-wise [16, 46],\nor image-wise [13] similarity measurements. Although high-quality raw associ-\nations can be derived from high-level semantic information in image features,\ntheir revelation of common saliency regions still relies on similarity measures,\nwhich are unreliable when encountering complex scenarios, such as significant\ndifferences between co-salient objects or high foreground-background similar-\nity. ii) the primary focus of building their deep models is on optimizing image\nfeatures. Compared to directly modeling association relationship, image feature\noptimization is not an explicit approach for inter-image association modeling\nand will increase the learning difficulty.\n      To alleviate these limitations, we propose a deep association learning strategy\nfor CoSOD, as shown in Figure 1 (b). Instead of directly using raw associations\nto optimize image features, we deploy deep networks on raw associations to learn\ndeep association features. This is a more explicit strategy for inter-image associ-\nation modeling. Moreover, our deep association features can capture high-level\ninter-image association knowledge, making them more robust in complex sce-\nnarios than raw associations, as shown in Figure 1 (c). Technically, we start by\ncollecting all pixel-wise raw associations across the entire image group as hy-\nperassociations. Then, we propose a Progressive Association Generation (PAG)",
      "md": "2       L Li et al.\n\nFigure 1: Difference of raw-association-based image feature optimization strategy (a) and our proposed deep association learning strategy (b)\n\nFig. 1: Difference of raw-association-based image feature optimization strategy (a) and our proposed deep association learning strategy (b). Our deep association learning deploys deep learning networks on raw associations to achieve deep association features. We also present visual samples of our calculated raw associations (Raw Asso.), optimized image feature (Opt. Fea.), and our generated deep association features (Asso. Fea.) in (c).\n\nObject Detection (SOD) [19–23,26,49,50,52], CoSOD is a more challenging task because it requires sufficient inter-image association modeling [9].\n\nRecently, many advanced works [8,13,16,33,38,41,46,47,51] have emerged and achieved impressive performance. These methods first use related image features to acquire raw inter-image associations (also called consensus representations), and then leverage them as guidance to optimize each image feature, as shown in Figure 1 (a). This approach enables the final image feature to implicitly capture inter-image cues, thereby achieving the purpose of inter-image association modeling. However, we find this raw-association-based feature optimization strategy still has two limitations: i) they directly rely on raw associations, which are acquired in heuristic manners, such as pixel-wise [8, 33, 41, 51], region-wise [16, 46], or image-wise [13] similarity measurements. Although high-quality raw associations can be derived from high-level semantic information in image features, their revelation of common saliency regions still relies on similarity measures, which are unreliable when encountering complex scenarios, such as significant differences between co-salient objects or high foreground-background similarity. ii) the primary focus of building their deep models is on optimizing image features. Compared to directly modeling association relationship, image feature optimization is not an explicit approach for inter-image association modeling and will increase the learning difficulty.\n\nTo alleviate these limitations, we propose a deep association learning strategy for CoSOD, as shown in Figure 1 (b). Instead of directly using raw associations to optimize image features, we deploy deep networks on raw associations to learn deep association features. This is a more explicit strategy for inter-image association modeling. Moreover, our deep association features can capture high-level inter-image association knowledge, making them more robust in complex scenarios than raw associations, as shown in Figure 1 (c). Technically, we start by collecting all pixel-wise raw associations across the entire image group as hyperassociations. Then, we propose a Progressive Association Generation (PAG)",
      "images": [
        {
          "name": "img_p1_1.png",
          "height": 170,
          "width": 185,
          "x": 149.18741966168,
          "y": 199.31016682105403,
          "original_width": 185,
          "original_height": 170
        },
        {
          "name": "img_p1_2.png",
          "height": 167,
          "width": 180,
          "x": 145.34069786563398,
          "y": 203.99311282599203,
          "original_width": 180,
          "original_height": 167
        },
        {
          "name": "img_p1_3.png",
          "height": 170,
          "width": 178,
          "x": 141.88422150418398,
          "y": 207.83984883818604,
          "original_width": 178,
          "original_height": 170
        },
        {
          "name": "img_p1_4.png",
          "height": 169,
          "width": 185,
          "x": 149.13167233812197,
          "y": 151.08675431924007,
          "original_width": 185,
          "original_height": 169
        },
        {
          "name": "img_p1_2.png",
          "height": 167,
          "width": 180,
          "x": 145.28495049561798,
          "y": 155.76972866355803,
          "original_width": 180,
          "original_height": 167
        },
        {
          "name": "img_p1_5.png",
          "height": 169,
          "width": 178,
          "x": 141.828474180626,
          "y": 159.61643633637203,
          "original_width": 178,
          "original_height": 169
        },
        {
          "name": "img_p1_6.png",
          "height": 259,
          "width": 241,
          "x": 366.16487038787596,
          "y": 201.81889598711604,
          "original_width": 241,
          "original_height": 259
        },
        {
          "name": "img_p1_7.png",
          "height": 256,
          "width": 239,
          "x": 449.95651240500797,
          "y": 201.81889598711606,
          "original_width": 239,
          "original_height": 256
        },
        {
          "name": "img_p1_8.png",
          "height": 257,
          "width": 241,
          "x": 394.095446097272,
          "y": 201.81889603357405,
          "original_width": 241,
          "original_height": 257
        },
        {
          "name": "img_p1_9.png",
          "height": 258,
          "width": 241,
          "x": 366.16487038787596,
          "y": 171.54684729848006,
          "original_width": 241,
          "original_height": 258
        },
        {
          "name": "img_p1_10.png",
          "height": 260,
          "width": 238,
          "x": 394.26265972856595,
          "y": 171.54687568431805,
          "original_width": 238,
          "original_height": 260
        },
        {
          "name": "img_p1_11.png",
          "height": 259,
          "width": 238,
          "x": 450.068007052124,
          "y": 171.54684729848003,
          "original_width": 238,
          "original_height": 259
        },
        {
          "name": "img_p1_12.png",
          "height": 258,
          "width": 241,
          "x": 366.16487038787596,
          "y": 141.83231449386807,
          "original_width": 241,
          "original_height": 258
        },
        {
          "name": "img_p1_13.png",
          "height": 257,
          "width": 239,
          "x": 450.068007052124,
          "y": 141.83232866355806,
          "original_width": 239,
          "original_height": 257,
          "ocr": [
            {
              "x": 54,
              "y": 40,
              "w": 124,
              "h": 164,
              "confidence": 0.8027157566694463,
              "text": "0"
            }
          ]
        },
        {
          "name": "img_p1_14.png",
          "height": 258,
          "width": 240,
          "x": 394.151165034992,
          "y": 141.83231449386807,
          "original_width": 240,
          "original_height": 258
        },
        {
          "name": "img_p1_15.png",
          "height": 256,
          "width": 239,
          "x": 422.13745972856594,
          "y": 141.72083401644204,
          "original_width": 239,
          "original_height": 256
        },
        {
          "name": "img_p1_16.png",
          "height": 257,
          "width": 237,
          "x": 422.248954375682,
          "y": 201.70738717031006,
          "original_width": 237,
          "original_height": 257
        },
        {
          "name": "img_p1_17.png",
          "height": 259,
          "width": 239,
          "x": 422.13745972856594,
          "y": 171.54684729848003,
          "original_width": 239,
          "original_height": 259
        },
        {
          "name": "page_2.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_2_text_1.jpg",
          "height": 105.75,
          "width": 347.146,
          "x": 133.723,
          "y": 559.481,
          "original_width": 701,
          "original_height": 213,
          "type": "layout_text"
        },
        {
          "name": "page_2_text_2.jpg",
          "height": 213.741,
          "width": 346.95,
          "x": 133.695,
          "y": 343.734,
          "original_width": 700,
          "original_height": 431,
          "type": "layout_text"
        },
        {
          "name": "page_2_caption_1.jpg",
          "height": 64.428,
          "width": 346.9,
          "x": 134.053,
          "y": 245.178,
          "original_width": 700,
          "original_height": 130,
          "type": "layout_caption"
        },
        {
          "name": "page_2_text_3.jpg",
          "height": 22.847,
          "width": 346.223,
          "x": 134.244,
          "y": 318.99,
          "original_width": 699,
          "original_height": 46,
          "type": "layout_text"
        },
        {
          "name": "page_2_picture_1.jpg",
          "height": 124.549,
          "width": 344.665,
          "x": 135.044,
          "y": 116.318,
          "original_width": 696,
          "original_height": 251,
          "type": "layout_picture"
        },
        {
          "name": "page_2_pageHeader_1.jpg",
          "height": 7.74,
          "width": 41.619,
          "x": 167.158,
          "y": 92.818,
          "original_width": 84,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_2_pageHeader_2.jpg",
          "height": 7.234,
          "width": 5.35,
          "x": 134.109,
          "y": 92.852,
          "original_width": 10,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "2       L Li et al.\n\nFigure 1: Difference of raw-association-based image feature optimization strategy (a) and our proposed deep association learning strategy (b)\n\nFig. 1: Difference of raw-association-based image feature optimization strategy (a) and our proposed deep association learning strategy (b). Our deep association learning deploys deep learning networks on raw associations to achieve deep association features. We also present visual samples of our calculated raw associations (Raw Asso.), optimized image feature (Opt. Fea.), and our generated deep association features (Asso. Fea.) in (c).\n\nObject Detection (SOD) [19–23,26,49,50,52], CoSOD is a more challenging task because it requires sufficient inter-image association modeling [9].\n\nRecently, many advanced works [8,13,16,33,38,41,46,47,51] have emerged and achieved impressive performance. These methods first use related image features to acquire raw inter-image associations (also called consensus representations), and then leverage them as guidance to optimize each image feature, as shown in Figure 1 (a). This approach enables the final image feature to implicitly capture inter-image cues, thereby achieving the purpose of inter-image association modeling. However, we find this raw-association-based feature optimization strategy still has two limitations: i) they directly rely on raw associations, which are acquired in heuristic manners, such as pixel-wise [8, 33, 41, 51], region-wise [16, 46], or image-wise [13] similarity measurements. Although high-quality raw associations can be derived from high-level semantic information in image features, their revelation of common saliency regions still relies on similarity measures, which are unreliable when encountering complex scenarios, such as significant differences between co-salient objects or high foreground-background similarity. ii) the primary focus of building their deep models is on optimizing image features. Compared to directly modeling association relationship, image feature optimization is not an explicit approach for inter-image association modeling and will increase the learning difficulty.\n\nTo alleviate these limitations, we propose a deep association learning strategy for CoSOD, as shown in Figure 1 (b). Instead of directly using raw associations to optimize image features, we deploy deep networks on raw associations to learn deep association features. This is a more explicit strategy for inter-image association modeling. Moreover, our deep association features can capture high-level inter-image association knowledge, making them more robust in complex scenarios than raw associations, as shown in Figure 1 (c). Technically, we start by collecting all pixel-wise raw associations across the entire image group as hyperassociations. Then, we propose a Progressive Association Generation (PAG)",
          "md": "2       L Li et al.\n\nFigure 1: Difference of raw-association-based image feature optimization strategy (a) and our proposed deep association learning strategy (b)\n\nFig. 1: Difference of raw-association-based image feature optimization strategy (a) and our proposed deep association learning strategy (b). Our deep association learning deploys deep learning networks on raw associations to achieve deep association features. We also present visual samples of our calculated raw associations (Raw Asso.), optimized image feature (Opt. Fea.), and our generated deep association features (Asso. Fea.) in (c).\n\nObject Detection (SOD) [19–23,26,49,50,52], CoSOD is a more challenging task because it requires sufficient inter-image association modeling [9].\n\nRecently, many advanced works [8,13,16,33,38,41,46,47,51] have emerged and achieved impressive performance. These methods first use related image features to acquire raw inter-image associations (also called consensus representations), and then leverage them as guidance to optimize each image feature, as shown in Figure 1 (a). This approach enables the final image feature to implicitly capture inter-image cues, thereby achieving the purpose of inter-image association modeling. However, we find this raw-association-based feature optimization strategy still has two limitations: i) they directly rely on raw associations, which are acquired in heuristic manners, such as pixel-wise [8, 33, 41, 51], region-wise [16, 46], or image-wise [13] similarity measurements. Although high-quality raw associations can be derived from high-level semantic information in image features, their revelation of common saliency regions still relies on similarity measures, which are unreliable when encountering complex scenarios, such as significant differences between co-salient objects or high foreground-background similarity. ii) the primary focus of building their deep models is on optimizing image features. Compared to directly modeling association relationship, image feature optimization is not an explicit approach for inter-image association modeling and will increase the learning difficulty.\n\nTo alleviate these limitations, we propose a deep association learning strategy for CoSOD, as shown in Figure 1 (b). Instead of directly using raw associations to optimize image features, we deploy deep networks on raw associations to learn deep association features. This is a more explicit strategy for inter-image association modeling. Moreover, our deep association features can capture high-level inter-image association knowledge, making them more robust in complex scenarios than raw associations, as shown in Figure 1 (c). Technically, we start by collecting all pixel-wise raw associations across the entire image group as hyperassociations. Then, we propose a Progressive Association Generation (PAG)",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 572
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.998,
      "layout": [
        {
          "image": "page_2_text_1.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.704,
            "w": 0.567,
            "h": 0.135
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_2_text_2.jpg",
          "confidence": 0.985,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.431,
            "w": 0.566,
            "h": 0.272
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_2_caption_1.jpg",
          "confidence": 0.962,
          "label": "caption",
          "bbox": {
            "x": 0.218,
            "y": 0.308,
            "w": 0.566,
            "h": 0.082
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_2_text_3.jpg",
          "confidence": 0.956,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.401,
            "w": 0.566,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_2_picture_1.jpg",
          "confidence": 0.951,
          "label": "picture",
          "bbox": {
            "x": 0.22,
            "y": 0.146,
            "w": 0.563,
            "h": 0.157
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_2_pageHeader_1.jpg",
          "confidence": 0.794,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.068,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_2_pageHeader_2.jpg",
          "confidence": 0.756,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 3,
      "text": "                                                                   CONDA         3\n\n    module to transform hyperassociations into deep association features. PAG pro-\n   gressively generates association features on varying scales, allowing us to use\n  previous association features to enhance the hyperassociation calculation at the\n    next scale, thereby improving the association quality from the very beginning.\n         Although deep association learning strategy allows more sufficient inter-\n   image association modeling, it significantly increases the computational burden\nand reduces the practicality of this approach. Additionally, this study finds that\nit is not necessary to utilize all pixel associations to generate deep association\n    features. In fact, there are even some noisy pixels that negatively impact the\n    quality of the deep association features. Therefore, we propose a method based\n          on Correspondence-induced Association Condensation (CAC) to condense the\n original full-pixel hyperassociations. This not only alleviates the computational\n    burden but also further enhances the quality of deep association features.\n      Specifically, CAC performs the condensation operation by selectively associ-\n  ating pixels that have semantic correspondence in other images, as well as their\n     surrounding contextual pixels, thereby creating lightweight yet more accurate\n    hyperassociations. Here, we introduce a pretext task, i.e. semantic correspon-\n        dence estimation, into the CoSOD, not only improving the model performance\n  but also delving deeper into the essence of CoSOD. Co-salient objects inherently\n    possess object-level   semantic correspondence. However, in this paper, we aim\n to further explore the finer pixel-level correspondence. Although highly accurate\n  correspondence estimation remains a challenge, we believe it will pave a new way\n       for CoSOD research. We also provide an object-aware cycle consistency (OCC)\n    loss to aid in learning correspondences within co-salient pixels.\n        In summary, the contributions of this paper are as follows:\n     –  For the first time, we introduce a deep association learning  approach for\n         CoSOD, applying deep networks to transform raw associations into deep as-\n sociation features for sufficient inter-image association modeling. Specifically,\n        we develop a CONdensed Deep Association (CONDA) learning model.\n     –     We propose a PAG module to progressively generate deep association fea-\n       tures. It enhances image features with previous association features to im-\n        prove hyperassociation calculation.\n     –      We introduce semantic correspondence estimation into the CoSOD task to\n     condense the original hyperassociation for alleviating the computational bur-\n            den and further improving the performance. We also propose an OCC loss\n        for effective correspondence estimation.\n     –  Experimental results demonstrate that our model achieves significantly im-\n       proved state-of-the-art performance on three benchmark datasets across dif-\n        ferent training settings.\n\n    2    Related Work\n\n    2.1   Co-Salient Object Detection\n   Recently, there has been a surge of excellent methods for CoSOD [8,10,13,16,33,\n  38,41–43,45–47,51]. These methods initially acquire raw inter-image associations",
      "md": "module to transform hyperassociations into deep association features. PAG pro-\ngressively generates association features on varying scales, allowing us to use\nprevious association features to enhance the hyperassociation calculation at the\nnext scale, thereby improving the association quality from the very beginning.\n\nAlthough deep association learning strategy allows more sufficient inter-\nimage association modeling, it significantly increases the computational burden\nand reduces the practicality of this approach. Additionally, this study finds that\nit is not necessary to utilize all pixel associations to generate deep association\nfeatures. In fact, there are even some noisy pixels that negatively impact the\nquality of the deep association features. Therefore, we propose a method based\non Correspondence-induced Association Condensation (CAC) to condense the\noriginal full-pixel hyperassociations. This not only alleviates the computational\nburden but also further enhances the quality of deep association features.\n\nSpecifically, CAC performs the condensation operation by selectively associ-\nating pixels that have semantic correspondence in other images, as well as their\nsurrounding contextual pixels, thereby creating lightweight yet more accurate\nhyperassociations. Here, we introduce a pretext task, i.e. semantic correspon-\ndence estimation, into the CoSOD, not only improving the model performance\nbut also delving deeper into the essence of CoSOD. Co-salient objects inherently\npossess object-level semantic correspondence. However, in this paper, we aim\nto further explore the finer pixel-level correspondence. Although highly accurate\ncorrespondence estimation remains a challenge, we believe it will pave a new way\nfor CoSOD research. We also provide an object-aware cycle consistency (OCC)\nloss to aid in learning correspondences within co-salient pixels.\n\nIn summary, the contributions of this paper are as follows:\n\n- For the first time, we introduce a deep association learning approach for\nCoSOD, applying deep networks to transform raw associations into deep as-\nsociation features for sufficient inter-image association modeling. Specifically,\nwe develop a CONdensed Deep Association (CONDA) learning model.\n- We propose a PAG module to progressively generate deep association fea-\ntures. It enhances image features with previous association features to im-\nprove hyperassociation calculation.\n- We introduce semantic correspondence estimation into the CoSOD task to\ncondense the original hyperassociation for alleviating the computational bur-\nden and further improving the performance. We also propose an OCC loss\nfor effective correspondence estimation.\n- Experimental results demonstrate that our model achieves significantly im-\nproved state-of-the-art performance on three benchmark datasets across dif-\nferent training settings.\n\n## 2 Related Work\n\n### 2.1 Co-Salient Object Detection\n\nRecently, there has been a surge of excellent methods for CoSOD [8,10,13,16,33,\n38,41–43,45–47,51]. These methods initially acquire raw inter-image associations",
      "images": [
        {
          "name": "page_3.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_3_text_1.jpg",
          "height": 129.294,
          "width": 347.171,
          "x": 133.858,
          "y": 273.505,
          "original_width": 701,
          "original_height": 261,
          "type": "layout_text"
        },
        {
          "name": "page_3_listItem_1.jpg",
          "height": 45.776,
          "width": 340.186,
          "x": 140.583,
          "y": 501.374,
          "original_width": 687,
          "original_height": 92,
          "type": "layout_listItem"
        },
        {
          "name": "page_3_listItem_2.jpg",
          "height": 33.755,
          "width": 339.958,
          "x": 140.573,
          "y": 549.061,
          "original_width": 686,
          "original_height": 68,
          "type": "layout_listItem"
        },
        {
          "name": "page_3_listItem_3.jpg",
          "height": 46.379,
          "width": 340.526,
          "x": 140.069,
          "y": 418.245,
          "original_width": 687,
          "original_height": 93,
          "type": "layout_listItem"
        },
        {
          "name": "page_3_text_2.jpg",
          "height": 46.419,
          "width": 346.77,
          "x": 133.982,
          "y": 117.84,
          "original_width": 700,
          "original_height": 93,
          "type": "layout_text"
        },
        {
          "name": "page_3_text_3.jpg",
          "height": 106.442,
          "width": 347.289,
          "x": 133.624,
          "y": 165.469,
          "original_width": 701,
          "original_height": 215,
          "type": "layout_text"
        },
        {
          "name": "page_3_listItem_4.jpg",
          "height": 34.038,
          "width": 340.35,
          "x": 140.356,
          "y": 465.945,
          "original_width": 687,
          "original_height": 68,
          "type": "layout_listItem"
        },
        {
          "name": "page_3_text_4.jpg",
          "height": 22.357,
          "width": 346.317,
          "x": 134.128,
          "y": 642.9,
          "original_width": 699,
          "original_height": 45,
          "type": "layout_text"
        },
        {
          "name": "page_3_sectionHeader_1.jpg",
          "height": 9.445,
          "width": 102.646,
          "x": 134.362,
          "y": 601.528,
          "original_width": 207,
          "original_height": 19,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_3_sectionHeader_2.jpg",
          "height": 10.161,
          "width": 168.309,
          "x": 134.169,
          "y": 625.148,
          "original_width": 339,
          "original_height": 20,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_3_pageHeader_1.jpg",
          "height": 8.108,
          "width": 34.585,
          "x": 413.057,
          "y": 92.586,
          "original_width": 69,
          "original_height": 16,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_3_pageHeader_2.jpg",
          "height": 7.706,
          "width": 5.111,
          "x": 475.626,
          "y": 92.973,
          "original_width": 10,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_3_text_5.jpg",
          "height": 10.441,
          "width": 256.287,
          "x": 148.46,
          "y": 404.867,
          "original_width": 517,
          "original_height": 21,
          "type": "layout_text"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "module to transform hyperassociations into deep association features. PAG pro-\ngressively generates association features on varying scales, allowing us to use\nprevious association features to enhance the hyperassociation calculation at the\nnext scale, thereby improving the association quality from the very beginning.\n\nAlthough deep association learning strategy allows more sufficient inter-\nimage association modeling, it significantly increases the computational burden\nand reduces the practicality of this approach. Additionally, this study finds that\nit is not necessary to utilize all pixel associations to generate deep association\nfeatures. In fact, there are even some noisy pixels that negatively impact the\nquality of the deep association features. Therefore, we propose a method based\non Correspondence-induced Association Condensation (CAC) to condense the\noriginal full-pixel hyperassociations. This not only alleviates the computational\nburden but also further enhances the quality of deep association features.\n\nSpecifically, CAC performs the condensation operation by selectively associ-\nating pixels that have semantic correspondence in other images, as well as their\nsurrounding contextual pixels, thereby creating lightweight yet more accurate\nhyperassociations. Here, we introduce a pretext task, i.e. semantic correspon-\ndence estimation, into the CoSOD, not only improving the model performance\nbut also delving deeper into the essence of CoSOD. Co-salient objects inherently\npossess object-level semantic correspondence. However, in this paper, we aim\nto further explore the finer pixel-level correspondence. Although highly accurate\ncorrespondence estimation remains a challenge, we believe it will pave a new way\nfor CoSOD research. We also provide an object-aware cycle consistency (OCC)\nloss to aid in learning correspondences within co-salient pixels.\n\nIn summary, the contributions of this paper are as follows:\n\n- For the first time, we introduce a deep association learning approach for\nCoSOD, applying deep networks to transform raw associations into deep as-\nsociation features for sufficient inter-image association modeling. Specifically,\nwe develop a CONdensed Deep Association (CONDA) learning model.\n- We propose a PAG module to progressively generate deep association fea-\ntures. It enhances image features with previous association features to im-\nprove hyperassociation calculation.\n- We introduce semantic correspondence estimation into the CoSOD task to\ncondense the original hyperassociation for alleviating the computational bur-\nden and further improving the performance. We also propose an OCC loss\nfor effective correspondence estimation.\n- Experimental results demonstrate that our model achieves significantly im-\nproved state-of-the-art performance on three benchmark datasets across dif-\nferent training settings.",
          "md": "module to transform hyperassociations into deep association features. PAG pro-\ngressively generates association features on varying scales, allowing us to use\nprevious association features to enhance the hyperassociation calculation at the\nnext scale, thereby improving the association quality from the very beginning.\n\nAlthough deep association learning strategy allows more sufficient inter-\nimage association modeling, it significantly increases the computational burden\nand reduces the practicality of this approach. Additionally, this study finds that\nit is not necessary to utilize all pixel associations to generate deep association\nfeatures. In fact, there are even some noisy pixels that negatively impact the\nquality of the deep association features. Therefore, we propose a method based\non Correspondence-induced Association Condensation (CAC) to condense the\noriginal full-pixel hyperassociations. This not only alleviates the computational\nburden but also further enhances the quality of deep association features.\n\nSpecifically, CAC performs the condensation operation by selectively associ-\nating pixels that have semantic correspondence in other images, as well as their\nsurrounding contextual pixels, thereby creating lightweight yet more accurate\nhyperassociations. Here, we introduce a pretext task, i.e. semantic correspon-\ndence estimation, into the CoSOD, not only improving the model performance\nbut also delving deeper into the essence of CoSOD. Co-salient objects inherently\npossess object-level semantic correspondence. However, in this paper, we aim\nto further explore the finer pixel-level correspondence. Although highly accurate\ncorrespondence estimation remains a challenge, we believe it will pave a new way\nfor CoSOD research. We also provide an object-aware cycle consistency (OCC)\nloss to aid in learning correspondences within co-salient pixels.\n\nIn summary, the contributions of this paper are as follows:\n\n- For the first time, we introduce a deep association learning approach for\nCoSOD, applying deep networks to transform raw associations into deep as-\nsociation features for sufficient inter-image association modeling. Specifically,\nwe develop a CONdensed Deep Association (CONDA) learning model.\n- We propose a PAG module to progressively generate deep association fea-\ntures. It enhances image features with previous association features to im-\nprove hyperassociation calculation.\n- We introduce semantic correspondence estimation into the CoSOD task to\ncondense the original hyperassociation for alleviating the computational bur-\nden and further improving the performance. We also propose an OCC loss\nfor effective correspondence estimation.\n- Experimental results demonstrate that our model achieves significantly im-\nproved state-of-the-art performance on three benchmark datasets across dif-\nferent training settings.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 490
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "2 Related Work",
          "md": "## 2 Related Work",
          "bBox": {
            "x": 134,
            "y": 599.04,
            "w": 102,
            "h": 12
          }
        },
        {
          "type": "heading",
          "lvl": 3,
          "value": "2.1 Co-Salient Object Detection",
          "md": "### 2.1 Co-Salient Object Detection",
          "bBox": {
            "x": 134,
            "y": 599.04,
            "w": 168,
            "h": 33.99
          }
        },
        {
          "type": "text",
          "value": "Recently, there has been a surge of excellent methods for CoSOD [8,10,13,16,33,\n38,41–43,45–47,51]. These methods initially acquire raw inter-image associations",
          "md": "Recently, there has been a surge of excellent methods for CoSOD [8,10,13,16,33,\n38,41–43,45–47,51]. These methods initially acquire raw inter-image associations",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 572
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 1,
      "layout": [
        {
          "image": "page_3_text_1.jpg",
          "confidence": 0.981,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.343,
            "w": 0.567,
            "h": 0.165
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_listItem_1.jpg",
          "confidence": 0.978,
          "label": "listItem",
          "bbox": {
            "x": 0.228,
            "y": 0.631,
            "w": 0.556,
            "h": 0.059
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_listItem_2.jpg",
          "confidence": 0.977,
          "label": "listItem",
          "bbox": {
            "x": 0.228,
            "y": 0.69,
            "w": 0.556,
            "h": 0.045
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_listItem_3.jpg",
          "confidence": 0.975,
          "label": "listItem",
          "bbox": {
            "x": 0.228,
            "y": 0.526,
            "w": 0.556,
            "h": 0.06
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_text_2.jpg",
          "confidence": 0.975,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.146,
            "w": 0.566,
            "h": 0.06
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_text_3.jpg",
          "confidence": 0.974,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.207,
            "w": 0.567,
            "h": 0.136
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_listItem_4.jpg",
          "confidence": 0.971,
          "label": "listItem",
          "bbox": {
            "x": 0.228,
            "y": 0.585,
            "w": 0.556,
            "h": 0.045
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_text_4.jpg",
          "confidence": 0.963,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.809,
            "w": 0.566,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_sectionHeader_1.jpg",
          "confidence": 0.942,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.759,
            "w": 0.167,
            "h": 0.011
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_sectionHeader_2.jpg",
          "confidence": 0.942,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.789,
            "w": 0.275,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_pageHeader_1.jpg",
          "confidence": 0.932,
          "label": "pageHeader",
          "bbox": {
            "x": 0.674,
            "y": 0.116,
            "w": 0.056,
            "h": 0.01
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_pageHeader_2.jpg",
          "confidence": 0.878,
          "label": "pageHeader",
          "bbox": {
            "x": 0.777,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_3_text_5.jpg",
          "confidence": 0.814,
          "label": "text",
          "bbox": {
            "x": 0.242,
            "y": 0.511,
            "w": 0.418,
            "h": 0.013
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 4,
      "text": "4       L Li et al.\n\nwith related image features and then utilize them to optimize each image feature.\nMost methods generate pixel-wise [8, 33, 41, 51], region-wise [16, 46], and image-\nwise [13] raw associations through similarity-based manners, e.g. inner product\ncalculations between image features. Even for transformer-based methods [16,\n33], they also rely on inner product calculation to produce attention maps [36]\nas raw associations. The image feature optimization manners include spatial or\nchannel calibration [8,13,51], dynamic convolution [41,46], feature fusion [16,41],\nand cross-attention [16, 33], etc. However, they lack the learning of high-level\nassociation knowledge and heavily focus on optimized image features. Unlike\nthem, this paper proposes a new research direction that deploys deep networks\non associations to achieve deep association features for CoSOD.\n\n2.2   Inter-image Relation Modeling\nApart from CoSOD, other tasks necessitating the consideration of inter-image re-\nlationships, such as few-shot segmentation [25,27,28,39], stereo matching [2,40],\nvideo semantic segmentation [34], etc. These tasks have made significant ad-\nvancements recently by effectively modeling inter-image relations. Most of these\nmethods [2, 4, 12, 27, 39, 40] initially create cost volumes to capture dense inter-\nimage pixel-wise similarities and subsequently use various modules to convert\nthese hyper volumes into task-specific features. Our approach distinguishes it-\nself from prior methods in three aspects. Firstly, most of them create 4D cost\nvolumes between two images while we create 6D hyperassociations between all\nrelated images. Secondly, they calculate hyper volumes based on original image\nfeatures, whereas We propose PAG to progressively enhance image features for\nbetter hyperassociation calculation. Last and most importantly, they rely on the\nfull-pixel cost volume. We consider the condensation of hyperassociations using\nsemantic correspondences to eliminate noisy pixel associations.\n\n2.3   Semantic Correspondence Estimation\nSemantic correspondence estimation [24] aims to establish reliable pixel corre-\nspondences between different instances of the same object category. Most works\nperformed this task using fully supervised training [14, 24]. Some recent works\nhave utilized unsupervised learning with photometric, forward-backward consis-\ntency, and warp-supervision losses [30, 35, 48]. However, they implement these\nlosses on the entire image, where background pixels may affect the performance.\nIn this paper, we introduce this task to condense hyperassociations for Co-SOD\nand tailor the cycle consistency loss by only applying it to co-salient pixels, hence\neffectively avoiding the influence of background and extraneous objects.\n\n3    Proposed Method\n\nAs shown in Figure 2, CONDA integrates the deep association learning process\ninto an FPN framework. Specifically, given a group of related images {Ii}N , we\n                                                                       i=1",
      "md": "4       L Li et al.\n\nwith related image features and then utilize them to optimize each image feature. Most methods generate pixel-wise [8,33,41,51], region-wise [16,46], and image-wise [13] raw associations through similarity-based manners, e.g. inner product calculations between image features. Even for transformer-based methods [16, 33], they also rely on inner product calculation to produce attention maps [36] as raw associations. The image feature optimization manners include spatial or channel calibration [8,13,51], dynamic convolution [41,46], feature fusion [16,41], and cross-attention [16,33], etc. However, they lack the learning of high-level association knowledge and heavily focus on optimized image features. Unlike them, this paper proposes a new research direction that deploys deep networks on associations to achieve deep association features for CoSOD.\n\n## 2.2 Inter-image Relation Modeling\n\nApart from CoSOD, other tasks necessitating the consideration of inter-image relationships, such as few-shot segmentation [25,27,28,39], stereo matching [2,40], video semantic segmentation [34], etc. These tasks have made significant advancements recently by effectively modeling inter-image relations. Most of these methods [2,4,12,27,39,40] initially create cost volumes to capture dense inter-image pixel-wise similarities and subsequently use various modules to convert these hyper volumes into task-specific features. Our approach distinguishes itself from prior methods in three aspects. Firstly, most of them create 4D cost volumes between two images while we create 6D hyperassociations between all related images. Secondly, they calculate hyper volumes based on original image features, whereas We propose PAG to progressively enhance image features for better hyperassociation calculation. Last and most importantly, they rely on the full-pixel cost volume. We consider the condensation of hyperassociations using semantic correspondences to eliminate noisy pixel associations.\n\n## 2.3 Semantic Correspondence Estimation\n\nSemantic correspondence estimation [24] aims to establish reliable pixel correspondences between different instances of the same object category. Most works performed this task using fully supervised training [14,24]. Some recent works have utilized unsupervised learning with photometric, forward-backward consistency, and warp-supervision losses [30,35,48]. However, they implement these losses on the entire image, where background pixels may affect the performance. In this paper, we introduce this task to condense hyperassociations for Co-SOD and tailor the cycle consistency loss by only applying it to co-salient pixels, hence effectively avoiding the influence of background and extraneous objects.\n\n## 3 Proposed Method\n\nAs shown in Figure 2, CONDA integrates the deep association learning process into an FPN framework. Specifically, given a group of related images $${I_i}_{i=1}^N$$, we",
      "images": [
        {
          "name": "page_4.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_4_text_1.jpg",
          "height": 165.592,
          "width": 347.268,
          "x": 133.7,
          "y": 286.988,
          "original_width": 701,
          "original_height": 334,
          "type": "layout_text"
        },
        {
          "name": "page_4_text_2.jpg",
          "height": 106.066,
          "width": 346.752,
          "x": 133.984,
          "y": 491.855,
          "original_width": 700,
          "original_height": 214,
          "type": "layout_text"
        },
        {
          "name": "page_4_text_3.jpg",
          "height": 129.856,
          "width": 347.013,
          "x": 133.673,
          "y": 117.892,
          "original_width": 700,
          "original_height": 262,
          "type": "layout_text"
        },
        {
          "name": "page_4_text_4.jpg",
          "height": 22.357,
          "width": 346.894,
          "x": 133.983,
          "y": 643.109,
          "original_width": 700,
          "original_height": 45,
          "type": "layout_text"
        },
        {
          "name": "page_4_sectionHeader_1.jpg",
          "height": 11.777,
          "width": 125.804,
          "x": 134.475,
          "y": 617.961,
          "original_width": 254,
          "original_height": 23,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_4_sectionHeader_2.jpg",
          "height": 9.822,
          "width": 181.301,
          "x": 134.449,
          "y": 267.332,
          "original_width": 366,
          "original_height": 19,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_4_sectionHeader_3.jpg",
          "height": 10.384,
          "width": 214.093,
          "x": 134.287,
          "y": 472.071,
          "original_width": 432,
          "original_height": 20,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_4_pageHeader_1.jpg",
          "height": 7.765,
          "width": 41.652,
          "x": 167.082,
          "y": 92.735,
          "original_width": 84,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_4_pageHeader_2.jpg",
          "height": 7.31,
          "width": 5.047,
          "x": 134.392,
          "y": 92.884,
          "original_width": 10,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "4       L Li et al.\n\nwith related image features and then utilize them to optimize each image feature. Most methods generate pixel-wise [8,33,41,51], region-wise [16,46], and image-wise [13] raw associations through similarity-based manners, e.g. inner product calculations between image features. Even for transformer-based methods [16, 33], they also rely on inner product calculation to produce attention maps [36] as raw associations. The image feature optimization manners include spatial or channel calibration [8,13,51], dynamic convolution [41,46], feature fusion [16,41], and cross-attention [16,33], etc. However, they lack the learning of high-level association knowledge and heavily focus on optimized image features. Unlike them, this paper proposes a new research direction that deploys deep networks on associations to achieve deep association features for CoSOD.",
          "md": "4       L Li et al.\n\nwith related image features and then utilize them to optimize each image feature. Most methods generate pixel-wise [8,33,41,51], region-wise [16,46], and image-wise [13] raw associations through similarity-based manners, e.g. inner product calculations between image features. Even for transformer-based methods [16, 33], they also rely on inner product calculation to produce attention maps [36] as raw associations. The image feature optimization manners include spatial or channel calibration [8,13,51], dynamic convolution [41,46], feature fusion [16,41], and cross-attention [16,33], etc. However, they lack the learning of high-level association knowledge and heavily focus on optimized image features. Unlike them, this paper proposes a new research direction that deploys deep networks on associations to achieve deep association features for CoSOD.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 537.01
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "2.2 Inter-image Relation Modeling",
          "md": "## 2.2 Inter-image Relation Modeling",
          "bBox": {
            "x": 134,
            "y": 266.04,
            "w": 181,
            "h": 10
          }
        },
        {
          "type": "text",
          "value": "Apart from CoSOD, other tasks necessitating the consideration of inter-image relationships, such as few-shot segmentation [25,27,28,39], stereo matching [2,40], video semantic segmentation [34], etc. These tasks have made significant advancements recently by effectively modeling inter-image relations. Most of these methods [2,4,12,27,39,40] initially create cost volumes to capture dense inter-image pixel-wise similarities and subsequently use various modules to convert these hyper volumes into task-specific features. Our approach distinguishes itself from prior methods in three aspects. Firstly, most of them create 4D cost volumes between two images while we create 6D hyperassociations between all related images. Secondly, they calculate hyper volumes based on original image features, whereas We propose PAG to progressively enhance image features for better hyperassociation calculation. Last and most importantly, they rely on the full-pixel cost volume. We consider the condensation of hyperassociations using semantic correspondences to eliminate noisy pixel associations.",
          "md": "Apart from CoSOD, other tasks necessitating the consideration of inter-image relationships, such as few-shot segmentation [25,27,28,39], stereo matching [2,40], video semantic segmentation [34], etc. These tasks have made significant advancements recently by effectively modeling inter-image relations. Most of these methods [2,4,12,27,39,40] initially create cost volumes to capture dense inter-image pixel-wise similarities and subsequently use various modules to convert these hyper volumes into task-specific features. Our approach distinguishes itself from prior methods in three aspects. Firstly, most of them create 4D cost volumes between two images while we create 6D hyperassociations between all related images. Secondly, they calculate hyper volumes based on original image features, whereas We propose PAG to progressively enhance image features for better hyperassociation calculation. Last and most importantly, they rely on the full-pixel cost volume. We consider the condensation of hyperassociations using semantic correspondences to eliminate noisy pixel associations.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 537.01
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "2.3 Semantic Correspondence Estimation",
          "md": "## 2.3 Semantic Correspondence Estimation",
          "bBox": {
            "x": 134,
            "y": 471.04,
            "w": 214,
            "h": 157.01
          }
        },
        {
          "type": "text",
          "value": "Semantic correspondence estimation [24] aims to establish reliable pixel correspondences between different instances of the same object category. Most works performed this task using fully supervised training [14,24]. Some recent works have utilized unsupervised learning with photometric, forward-backward consistency, and warp-supervision losses [30,35,48]. However, they implement these losses on the entire image, where background pixels may affect the performance. In this paper, we introduce this task to condense hyperassociations for Co-SOD and tailor the cycle consistency loss by only applying it to co-salient pixels, hence effectively avoiding the influence of background and extraneous objects.",
          "md": "Semantic correspondence estimation [24] aims to establish reliable pixel correspondences between different instances of the same object category. Most works performed this task using fully supervised training [14,24]. Some recent works have utilized unsupervised learning with photometric, forward-backward consistency, and warp-supervision losses [30,35,48]. However, they implement these losses on the entire image, where background pixels may affect the performance. In this paper, we introduce this task to condense hyperassociations for Co-SOD and tailor the cycle consistency loss by only applying it to co-salient pixels, hence effectively avoiding the influence of background and extraneous objects.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 572
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "3 Proposed Method",
          "md": "## 3 Proposed Method",
          "bBox": {
            "x": 134,
            "y": 616.04,
            "w": 126,
            "h": 12
          }
        },
        {
          "type": "text",
          "value": "As shown in Figure 2, CONDA integrates the deep association learning process into an FPN framework. Specifically, given a group of related images $${I_i}_{i=1}^N$$, we",
          "md": "As shown in Figure 2, CONDA integrates the deep association learning process into an FPN framework. Specifically, given a group of related images $${I_i}_{i=1}^N$$, we",
          "bBox": {
            "x": 134,
            "y": 641.04,
            "w": 346,
            "h": 24.99
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.991,
      "layout": [
        {
          "image": "page_4_text_1.jpg",
          "confidence": 0.988,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.359,
            "w": 0.567,
            "h": 0.211
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_text_2.jpg",
          "confidence": 0.988,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.618,
            "w": 0.566,
            "h": 0.136
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_text_3.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.146,
            "w": 0.567,
            "h": 0.166
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_text_4.jpg",
          "confidence": 0.974,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.809,
            "w": 0.566,
            "h": 0.031
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_sectionHeader_1.jpg",
          "confidence": 0.957,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.78,
            "w": 0.205,
            "h": 0.014
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_sectionHeader_2.jpg",
          "confidence": 0.957,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.337,
            "w": 0.296,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_sectionHeader_3.jpg",
          "confidence": 0.956,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.218,
            "y": 0.594,
            "w": 0.35,
            "h": 0.014
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_pageHeader_1.jpg",
          "confidence": 0.866,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.068,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_4_pageHeader_2.jpg",
          "confidence": 0.798,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 5,
      "text": "                                                                                                                                            CONDA                          5\n\n                                                                                                                                      PAG\n                                       𝐿𝐿                    HAC                        CAC                        Agg\n                                𝑭𝑭   5                              𝑨𝑨                       �                              𝐴𝐴\n                                 5,                                                              𝑨𝑨                         𝑭𝑭\n                                 𝑙𝑙  𝑙𝑙=1                          5                          5                             5\n\n                                              𝐿𝐿5\n                                       Feature Extraction                   (3 ×𝐻𝐻₅×𝑊𝑊×         (3 ×𝐻𝐻×𝑊𝑊×\n                                          1                             3 ×𝐻𝐻 ×𝑊𝑊 5                  5    5               (3 ×𝐻𝐻×𝑊𝑊×𝐶𝐶) FPN Decoder\n                                                                           5   5×𝐿𝐿)                3 ×𝐾𝐾×𝐾𝐾×𝐿𝐿)\n                                                                                  5                              5                 5  5  5\n\n                                𝑭𝑭₄  𝐿𝐿4                   HAC\n                                ,𝑙𝑙  𝑙𝑙=1                         𝑨𝑨                    CAC�                   Agg        𝐴𝐴\n                                                                       4                         𝑨𝑨₄                        𝑭𝑭₄\n\n                                          1  𝐿𝐿4   E                      (3 ×𝐻𝐻₄×𝑊𝑊×         (3 ×𝐻𝐻  ×𝑊𝑊×\n                                                                       3 ×𝐻𝐻  ×𝑊𝑊 4                  4    4               (3 ×𝐻𝐻  ×𝑊𝑊×𝐶𝐶)\n                                                                           4  4×𝐿𝐿)                 3 ×𝐾𝐾×𝐾𝐾×𝐿𝐿)\n                                                                                      4                          4                 4  4  4\n                                       𝐿𝐿\n                               𝑭𝑭3,          3               HAC                        CAC                        Agg\n                                 𝑙𝑙  𝑙𝑙=1                        𝑨𝑨                        �                              𝐴𝐴\n                                                                       3                         𝑨𝑨₃                        𝑭𝑭₃\n                                              𝐿𝐿3   E\n                                           1\n                                                                            (3 ×𝐻𝐻₃×𝑊𝑊×       (3 ×  𝐻𝐻×𝑊𝑊×\n                                                                                      3                  3  3                 (3 ×𝐻𝐻₃×𝑊𝑊×𝐶𝐶)\n                                                                           3 ×𝐻𝐻₃×𝑊𝑊×𝐿𝐿)        3 ×𝐾𝐾×𝐾𝐾×𝐿𝐿)              3  3\n                                                                              3   3                        3\n\n  CAC                             ⋯            Initial Condensation        Agg               𝓞𝓞  Offsets              HAC     Hyperassociation Calculation\n\n                         𝐾𝐾  𝐾𝐾                          ⋯                                                            E     Association-induced Feature Enhancement\n                                                      �′                                   𝑨𝑨′                        Agg     Aggregation Network\n                                                      𝒂𝒂 ∈ ℝ3×𝐾𝐾×                    𝑭𝑭0  ∆      ∈ ℝ\n                                                                𝐾𝐾×𝐿𝐿                                 3×𝐾𝐾×𝐾𝐾×\n                Max-Similarity              ⨁          3          3                        3     𝟑𝟑            2\n    𝑨𝑨          Selection                                                                            �                CAC     Correspondence-induced Association\n      3                                                                         Final Condensation     𝑨𝑨₃                    Condensation\n\n                                  ⋯                     855     858                         ⋯                                   Associations between each pixel and\n                                                                 ⋯                    𝒂𝒂                                      other pixels\n                                                                                      �    ∈ ℝ\n                                                                                            3×\n                                                                                              𝐾𝐾×𝐾𝐾×\n\n                          𝒂𝒂  1  ⋯    𝐿𝐿₃                                          3         𝐿𝐿3                   ⨁      Addition operation\n                               ∈ ℝ\n                                 3×\n                               3   𝐻𝐻3×𝑊𝑊3×𝐿𝐿3\n\nFig. 2: Overall flowchart of our CONDA model. Specifically, CONDA first uti-\nlizes the image features to calculate hyperassociations. Then, the full-pixel hyperasso-\nciations are condensed by CAC and fed into the aggregation networks to achieve deep\nassociation features. These features are then used in the FPN decoder process for the\nfinal prediction. To be concise, only three related images are shown.\n\nfirst input them into a VGG-16 [32] backbone to collect its intermediate features\nfor PAG and FPN decoding. In detail, we collect all features of the last three\nstages for PAG and the last feature of each stage for the FPN decoder as follows:\n\n                                       FP =  Fs,l | s ∈ {3, · · ·                        ,5}, l ∈ {1,· · · , Ls} ,                                                        (1)\n\n                                                       FD =  F                        |s ∈ {1,· · · 5}\n                                                                            s,Ls                                     ,\n\nwhere FP and FD are the feature collections for PAG and FPN decoder, respec-\ntively. F                              ∈ RN×H ×W ×C                        represents the VGG feature of the                                                    l-th layer in\n                    s,l                       s        s       s\nthe s-th stage. There are in total L                                                  layers in the s-th stage. H                                , W          and C\n                                                                                s                                                              s         s                  s\nrepresent the height, width, and channel of the s-th stage, respectively.\n        Then, we input FP                                                                                                into PAG to calculate hyperassociations and genarate\ndeep association features {FA ∈ RN×H ×W ×C }5                                                                                                    . Finally, these association\n                                                               s                       s        s      s    s=3\nfeatures will be fused with FD for the FPN decoder process, formulated as:\n\n                                                                           F s,L = Fs,L + ϕ(F A),     s = 3,· · ·          ,5,\n                                                      s F                s                 s             5                                                                (2)\n                                                               = Decoder({F s,L }s=1),\n                                                                                                     s\n\nwhere ϕ is a convolution layer. F ∈ RN×H×W ×C                                                                                              is the final feature for the final\nco-saliency prediction. We adopt BCE and IoU losses for supervision.\n                                                                                                 The rest of this section will introduce PAG with full-pixel hyperassociation\nand the condensation of hyperassociations by plugging the Correspondence-\ninduced Association Condensation (CAC) module into PAG.",
      "md": "CONDA 5\n\n```mermaid\ngraph TD\nA[Feature Extraction] --> B1[HAC 5]\nA --> B2[HAC 4]\nA --> B3[HAC 3]\nB1 --> C1[CAC 5]\nB2 --> C2[CAC 4]\nB3 --> C3[CAC 3]\nC1 --> D1[Agg 5]\nC2 --> D2[Agg 4]\nC3 --> D3[Agg 3]\nD1 --> E[FPN Decoder]\nD2 --> E\nD3 --> E\n\nsubgraph CAC\nF[Initial Condensation] --> G[Agg]\nG --> H[Final Condensation]\nI[Max-Similarity Selection] --> F\nend\n```\n\nFig. 2: Overall flowchart of our CONDA model. Specifically, CONDA first utilizes the image features to calculate hyperassociations. Then, the full-pixel hyperassociations are condensed by CAC and fed into the aggregation networks to achieve deep association features. These features are then used in the FPN decoder process for the final prediction. To be concise, only three related images are shown.\n\nFirst input them into a VGG-16 [32] backbone to collect its intermediate features for PAG and FPN decoding. In detail, we collect all features of the last three stages for PAG and the last feature of each stage for the FPN decoder as follows:\n\n$$F^P = \\{F_{s,l} | s \\in \\{3, \\cdots, 5\\}, l \\in \\{1, \\cdots, L_s\\}\\},$$\n$$F^D = \\{F_{s,L_s} | s \\in \\{1, \\cdots 5\\}\\},$$\n\nwhere $F^P$ and $F^D$ are the feature collections for PAG and FPN decoder, respectively. $F_{s,l} \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C_s}$ represents the VGG feature of the $l$-th layer in the $s$-th stage. There are in total $L_s$ layers in the $s$-th stage. $H_s$, $W_s$ and $C_s$ represent the height, width, and channel of the $s$-th stage, respectively.\n\nThen, we input $F^P$ into PAG to calculate hyperassociations and generate deep association features $\\{F^A_s \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C_s}\\}_{s=3}^5$. Finally, these association features will be fused with $F^D$ for the FPN decoder process, formulated as:\n\n$$F_{s,L_s} = F_{s,L_s} + \\phi(F^A_s), \\quad s = 3, \\cdots, 5,$$\n$$F = \\text{Decoder}(\\{F_{s,L_s}\\}_{s=1}^5),$$\n\nwhere $\\phi$ is a convolution layer. $F \\in \\mathbb{R}^{N \\times H \\times W \\times C}$ is the final feature for the final co-saliency prediction. We adopt BCE and IoU losses for supervision.\n\nThe rest of this section will introduce PAG with full-pixel hyperassociation and the condensation of hyperassociations by plugging the Correspondence-induced Association Condensation (CAC) module into PAG.",
      "images": [
        {
          "name": "img_p4_6.png",
          "height": 195,
          "width": 203,
          "x": 449.46772102683497,
          "y": 132.70815261640502,
          "original_width": 203,
          "original_height": 195
        },
        {
          "name": "img_p4_1.png",
          "height": 193,
          "width": 203,
          "x": 138.086974074155,
          "y": 133.15511773462498,
          "original_width": 203,
          "original_height": 193
        },
        {
          "name": "img_p4_5.png",
          "height": 192,
          "width": 204,
          "x": 448.87175074736,
          "y": 169.50770020337998,
          "original_width": 204,
          "original_height": 192
        },
        {
          "name": "img_p4_4.png",
          "height": 192,
          "width": 203,
          "x": 448.87177599220996,
          "y": 206.00924996611502,
          "original_width": 203,
          "original_height": 192
        },
        {
          "name": "img_p4_3.png",
          "height": 193,
          "width": 196,
          "x": 137.54069219830998,
          "y": 206.10859523800502,
          "original_width": 196,
          "original_height": 193
        },
        {
          "name": "img_p4_2.png",
          "height": 190,
          "width": 199,
          "x": 137.14339342551497,
          "y": 170.20297780470003,
          "original_width": 199,
          "original_height": 190
        },
        {
          "name": "img_p4_24.png",
          "height": 261,
          "width": 143,
          "x": 242.52615084529998,
          "y": 283.03502260206005,
          "original_width": 143,
          "original_height": 261,
          "ocr": [
            {
              "x": 22,
              "y": 70,
              "w": 102,
              "h": 106,
              "confidence": 0.1950910365335528,
              "text": "855"
            }
          ]
        },
        {
          "name": "img_p4_10.png",
          "height": 262,
          "width": 142,
          "x": 184.47128600573,
          "y": 243.05712269999998,
          "original_width": 142,
          "original_height": 262
        },
        {
          "name": "img_p4_30.png",
          "height": 262,
          "width": 142,
          "x": 205.47830339765,
          "y": 282.29009765103,
          "original_width": 142,
          "original_height": 262
        },
        {
          "name": "img_p4_31.png",
          "height": 262,
          "width": 142,
          "x": 202.20061214257998,
          "y": 282.29009765103,
          "original_width": 142,
          "original_height": 262
        },
        {
          "name": "img_p4_30.png",
          "height": 262,
          "width": 142,
          "x": 186.80539342551498,
          "y": 282.19078085202,
          "original_width": 142,
          "original_height": 262
        },
        {
          "name": "img_p4_31.png",
          "height": 262,
          "width": 142,
          "x": 183.52769587992498,
          "y": 282.141117734625,
          "original_width": 142,
          "original_height": 262
        },
        {
          "name": "img_p4_7.png",
          "height": 261,
          "width": 142,
          "x": 202.79655084529998,
          "y": 243.00746901838502,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_8.png",
          "height": 261,
          "width": 142,
          "x": 199.56851960374996,
          "y": 243.10678900404,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_9.png",
          "height": 261,
          "width": 142,
          "x": 196.39016095675998,
          "y": 243.10678267213498,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_8.png",
          "height": 261,
          "width": 142,
          "x": 181.24324210036997,
          "y": 243.156442685655,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_9.png",
          "height": 261,
          "width": 142,
          "x": 178.11453709360998,
          "y": 243.156442685655,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_20.png",
          "height": 261,
          "width": 142,
          "x": 266.41356071949497,
          "y": 282.985362629925,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_21.png",
          "height": 261,
          "width": 142,
          "x": 263.235195781985,
          "y": 283.03502260206,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_22.png",
          "height": 261,
          "width": 142,
          "x": 260.15616336978496,
          "y": 283.03502260206,
          "original_width": 142,
          "original_height": 261,
          "ocr": [
            {
              "x": 22,
              "y": 66,
              "w": 104,
              "h": 120,
              "confidence": 0.19072086268218058,
              "text": "858"
            }
          ]
        },
        {
          "name": "img_p4_23.png",
          "height": 261,
          "width": 142,
          "x": 248.83322083095499,
          "y": 282.737062727865,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_21.png",
          "height": 261,
          "width": 142,
          "x": 245.65485585206,
          "y": 283.08469519662003,
          "original_width": 142,
          "original_height": 261
        },
        {
          "name": "img_p4_33.png",
          "height": 262,
          "width": 141,
          "x": 180.498329771765,
          "y": 282.19078085202,
          "original_width": 141,
          "original_height": 262
        },
        {
          "name": "img_p4_32.png",
          "height": 261,
          "width": 141,
          "x": 199.17123345337998,
          "y": 282.33975762316504,
          "original_width": 141,
          "original_height": 261
        },
        {
          "name": "img_p4_19.png",
          "height": 161,
          "width": 105,
          "x": 328.391740775225,
          "y": 252.44323134699
        },
        {
          "name": "img_p4_17.png",
          "height": 160,
          "width": 105,
          "x": 334.89745064941997,
          "y": 252.64189656315003
        },
        {
          "name": "img_p4_18.png",
          "height": 160,
          "width": 105,
          "x": 331.619790971105,
          "y": 252.44323767889497
        },
        {
          "name": "img_p4_15.png",
          "height": 110,
          "width": 61,
          "x": 261.49705860028996,
          "y": 255.07531126339498
        },
        {
          "name": "img_p4_16.png",
          "height": 110,
          "width": 61,
          "x": 258.120041069045,
          "y": 254.97599765102999
        },
        {
          "name": "img_p4_25.png",
          "height": 110,
          "width": 61,
          "x": 312.2516286425,
          "y": 290.43466258854
        },
        {
          "name": "page_5.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_5_text_1.jpg",
          "height": 34.845,
          "width": 346.634,
          "x": 134.071,
          "y": 386.345,
          "original_width": 700,
          "original_height": 70,
          "type": "layout_text"
        },
        {
          "name": "page_5_picture_1.jpg",
          "height": 203.231,
          "width": 344.339,
          "x": 134.94,
          "y": 115.177,
          "original_width": 695,
          "original_height": 410,
          "type": "layout_picture"
        },
        {
          "name": "page_5_text_2.jpg",
          "height": 47.883,
          "width": 346.492,
          "x": 133.953,
          "y": 469.677,
          "original_width": 699,
          "original_height": 96,
          "type": "layout_text"
        },
        {
          "name": "page_5_text_3.jpg",
          "height": 34.087,
          "width": 346.403,
          "x": 134.175,
          "y": 631.183,
          "original_width": 699,
          "original_height": 68,
          "type": "layout_text"
        },
        {
          "name": "page_5_caption_1.jpg",
          "height": 53.605,
          "width": 346.625,
          "x": 134.072,
          "y": 323.144,
          "original_width": 700,
          "original_height": 108,
          "type": "layout_caption"
        },
        {
          "name": "page_5_formula_1.jpg",
          "height": 29.714,
          "width": 257.965,
          "x": 222.091,
          "y": 564.183,
          "original_width": 520,
          "original_height": 60,
          "type": "layout_formula"
        },
        {
          "name": "page_5_formula_2.jpg",
          "height": 29.662,
          "width": 270.359,
          "x": 209.974,
          "y": 432.434,
          "original_width": 546,
          "original_height": 59,
          "type": "layout_formula"
        },
        {
          "name": "page_5_text_4.jpg",
          "height": 33.909,
          "width": 347.004,
          "x": 134.07,
          "y": 519.269,
          "original_width": 700,
          "original_height": 68,
          "type": "layout_text"
        },
        {
          "name": "page_5_text_5.jpg",
          "height": 23.244,
          "width": 346.392,
          "x": 134.058,
          "y": 605.481,
          "original_width": 699,
          "original_height": 46,
          "type": "layout_text"
        },
        {
          "name": "page_5_pageHeader_1.jpg",
          "height": 7.282,
          "width": 5.083,
          "x": 475.592,
          "y": 93.245,
          "original_width": 10,
          "original_height": 14,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_5_pageHeader_2.jpg",
          "height": 7.698,
          "width": 34.464,
          "x": 413.182,
          "y": 92.721,
          "original_width": 69,
          "original_height": 15,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "CONDA 5\n\n```mermaid\ngraph TD\nA[Feature Extraction] --> B1[HAC 5]\nA --> B2[HAC 4]\nA --> B3[HAC 3]\nB1 --> C1[CAC 5]\nB2 --> C2[CAC 4]\nB3 --> C3[CAC 3]\nC1 --> D1[Agg 5]\nC2 --> D2[Agg 4]\nC3 --> D3[Agg 3]\nD1 --> E[FPN Decoder]\nD2 --> E\nD3 --> E\n\nsubgraph CAC\nF[Initial Condensation] --> G[Agg]\nG --> H[Final Condensation]\nI[Max-Similarity Selection] --> F\nend\n```\n\nFig. 2: Overall flowchart of our CONDA model. Specifically, CONDA first utilizes the image features to calculate hyperassociations. Then, the full-pixel hyperassociations are condensed by CAC and fed into the aggregation networks to achieve deep association features. These features are then used in the FPN decoder process for the final prediction. To be concise, only three related images are shown.\n\nFirst input them into a VGG-16 [32] backbone to collect its intermediate features for PAG and FPN decoding. In detail, we collect all features of the last three stages for PAG and the last feature of each stage for the FPN decoder as follows:\n\n$$F^P = \\{F_{s,l} | s \\in \\{3, \\cdots, 5\\}, l \\in \\{1, \\cdots, L_s\\}\\},$$\n$$F^D = \\{F_{s,L_s} | s \\in \\{1, \\cdots 5\\}\\},$$\n\nwhere $F^P$ and $F^D$ are the feature collections for PAG and FPN decoder, respectively. $F_{s,l} \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C_s}$ represents the VGG feature of the $l$-th layer in the $s$-th stage. There are in total $L_s$ layers in the $s$-th stage. $H_s$, $W_s$ and $C_s$ represent the height, width, and channel of the $s$-th stage, respectively.\n\nThen, we input $F^P$ into PAG to calculate hyperassociations and generate deep association features $\\{F^A_s \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C_s}\\}_{s=3}^5$. Finally, these association features will be fused with $F^D$ for the FPN decoder process, formulated as:\n\n$$F_{s,L_s} = F_{s,L_s} + \\phi(F^A_s), \\quad s = 3, \\cdots, 5,$$\n$$F = \\text{Decoder}(\\{F_{s,L_s}\\}_{s=1}^5),$$\n\nwhere $\\phi$ is a convolution layer. $F \\in \\mathbb{R}^{N \\times H \\times W \\times C}$ is the final feature for the final co-saliency prediction. We adopt BCE and IoU losses for supervision.\n\nThe rest of this section will introduce PAG with full-pixel hyperassociation and the condensation of hyperassociations by plugging the Correspondence-induced Association Condensation (CAC) module into PAG.",
          "md": "CONDA 5\n\n```mermaid\ngraph TD\nA[Feature Extraction] --> B1[HAC 5]\nA --> B2[HAC 4]\nA --> B3[HAC 3]\nB1 --> C1[CAC 5]\nB2 --> C2[CAC 4]\nB3 --> C3[CAC 3]\nC1 --> D1[Agg 5]\nC2 --> D2[Agg 4]\nC3 --> D3[Agg 3]\nD1 --> E[FPN Decoder]\nD2 --> E\nD3 --> E\n\nsubgraph CAC\nF[Initial Condensation] --> G[Agg]\nG --> H[Final Condensation]\nI[Max-Similarity Selection] --> F\nend\n```\n\nFig. 2: Overall flowchart of our CONDA model. Specifically, CONDA first utilizes the image features to calculate hyperassociations. Then, the full-pixel hyperassociations are condensed by CAC and fed into the aggregation networks to achieve deep association features. These features are then used in the FPN decoder process for the final prediction. To be concise, only three related images are shown.\n\nFirst input them into a VGG-16 [32] backbone to collect its intermediate features for PAG and FPN decoding. In detail, we collect all features of the last three stages for PAG and the last feature of each stage for the FPN decoder as follows:\n\n$$F^P = \\{F_{s,l} | s \\in \\{3, \\cdots, 5\\}, l \\in \\{1, \\cdots, L_s\\}\\},$$\n$$F^D = \\{F_{s,L_s} | s \\in \\{1, \\cdots 5\\}\\},$$\n\nwhere $F^P$ and $F^D$ are the feature collections for PAG and FPN decoder, respectively. $F_{s,l} \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C_s}$ represents the VGG feature of the $l$-th layer in the $s$-th stage. There are in total $L_s$ layers in the $s$-th stage. $H_s$, $W_s$ and $C_s$ represent the height, width, and channel of the $s$-th stage, respectively.\n\nThen, we input $F^P$ into PAG to calculate hyperassociations and generate deep association features $\\{F^A_s \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C_s}\\}_{s=3}^5$. Finally, these association features will be fused with $F^D$ for the FPN decoder process, formulated as:\n\n$$F_{s,L_s} = F_{s,L_s} + \\phi(F^A_s), \\quad s = 3, \\cdots, 5,$$\n$$F = \\text{Decoder}(\\{F_{s,L_s}\\}_{s=1}^5),$$\n\nwhere $\\phi$ is a convolution layer. $F \\in \\mathbb{R}^{N \\times H \\times W \\times C}$ is the final feature for the final co-saliency prediction. We adopt BCE and IoU losses for supervision.\n\nThe rest of this section will introduce PAG with full-pixel hyperassociation and the condensation of hyperassociations by plugging the Correspondence-induced Association Condensation (CAC) module into PAG.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 572
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.848,
      "layout": [
        {
          "image": "page_5_text_1.jpg",
          "confidence": 0.982,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.486,
            "w": 0.566,
            "h": 0.045
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_picture_1.jpg",
          "confidence": 0.979,
          "label": "picture",
          "bbox": {
            "x": 0.22,
            "y": 0.145,
            "w": 0.562,
            "h": 0.256
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_text_2.jpg",
          "confidence": 0.977,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.592,
            "w": 0.566,
            "h": 0.061
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_text_3.jpg",
          "confidence": 0.973,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.795,
            "w": 0.566,
            "h": 0.044
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_caption_1.jpg",
          "confidence": 0.973,
          "label": "caption",
          "bbox": {
            "x": 0.218,
            "y": 0.406,
            "w": 0.566,
            "h": 0.069
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_formula_1.jpg",
          "confidence": 0.97,
          "label": "formula",
          "bbox": {
            "x": 0.362,
            "y": 0.71,
            "w": 0.421,
            "h": 0.038
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_formula_2.jpg",
          "confidence": 0.968,
          "label": "formula",
          "bbox": {
            "x": 0.343,
            "y": 0.544,
            "w": 0.441,
            "h": 0.039
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_text_4.jpg",
          "confidence": 0.967,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.652,
            "w": 0.567,
            "h": 0.045
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_text_5.jpg",
          "confidence": 0.965,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.762,
            "w": 0.566,
            "h": 0.031
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_pageHeader_1.jpg",
          "confidence": 0.854,
          "label": "pageHeader",
          "bbox": {
            "x": 0.777,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_5_pageHeader_2.jpg",
          "confidence": 0.843,
          "label": "pageHeader",
          "bbox": {
            "x": 0.675,
            "y": 0.117,
            "w": 0.056,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 6,
      "text": "6     L Li et al.\n\n3.1  Progressive Association Generation\n\nOur deep association learning involves two steps: 1) acquiring the raw hyper-\nassociations {As}5       from three stages; 2) employing aggregation networks on\n{A }5          s=3                    A 5\n   s s=3 to obtain association features {F s }s=3.\n    Earlier methods [4, 11, 31] directly utilize the original backbone features,\ni.e. FP , to calculate inter-image interactions, such as the so-called cost volume.\nWe argue that hyperassociations derived straight from backbone features might\nobstruct further improvement of deep association learning, given that the current\nbackbone is pre-trained without any consideration for inter-image associations.\n    To alleviate this problem, we propose the PAG module to progressively gener-\nate pyramid association features so that we can utilize the high-level association\nfeature, e.g. FA     from the (s + 1)-th stage, to enhance the neighbouring low-\n             s+1     P\nlevel VGG features in F , e.g. Fs,l, for attaining association-enhanced features\nˆ\nF s,l, based on which we can calculate high-quality hyperassociations As in the\ns-th stage. After that, we execute the subsequent aggregation networks on As\nto achieve association features FA, which will continue to enhance the VGG\n                             s\nfeatures of the next stage and carry out progressive association generation. The\nwhole process of our PAG can be formulated as follows:\n\n                        A   = HAC     ˆ  L\n                          s        {F s,l}l s    ,\n                            FA = Agg A  =1\n                  ˆ     L      s          s ,  L    A                        (3)\n                 {Fs−1,l}l s−¹ = Enh  {F s−1,l}  s−1;F  ,\n                         =1                    l=1  s\n\nwhere s ranges from 5 to 3 and ˆ  L             L\n                            {F 5,l} 5 = {F 5,l}        5 . The HAC, Agg, and Enh\n                                  l=1           l=1\nrepresent the hyperassociation calculation, aggregation network, and association-\ninduced feature enhancement, respectively. Next, we explain them in detail.\nHyperassociation Calculation. For each stage, we first compute the raw asso-\nciations at each layer using the inner product between l-2 normalized association-\nenhanced features of N related images. After that, we stack the raw associations\nof all layers to form the final hyperassociation of this stage. The hyperassociation\nfor the s-th stage,                   i.e. As ∈ RN×Hₛ×Wₛ×N×Hₛ×Wₛ×Lₛ , can be calculated via:\n\n                A  = HAC    ˆ   L\n                  s     {F s,l}l s    ,\n                                 =1\n                                      ˆ   ˆ⊤\n                   = Stack    ReLU    F s,l · Fs,l   Lₛ  ,                   (4)\n                                      ˆ   ˆ ⊤    l=1\n                                      ∥Fs,l∥∥F s,l∥\n\nwhere ˆ    ˆ⊤     N×H ×W ×N×H ×W\n      F s,l · Fs,l ∈ R  s s    s         s. The ⊤ indicates transposing the last\ndimension and the first three dimensions. ∥·∥ represents the l-2 norm. We employ\nReLU to suppress noisy association values.\nAggregation Network. The raw hyperassociation As ∈ RN×Hˢ×Wˢ×N×Hˢ×Wˢ\n×Ls   is a hypercube with a nested structure, where each pixel position is char-\nacterized by a 4D tensor (N × Hs × Ws × Ls). Each 4D tensor documents the\nassociations of the respective pixel with all other pixels in N related images. For",
      "md": "6     L Li et al.\n\n## 3.1 Progressive Association Generation\n\nOur deep association learning involves two steps: 1) acquiring the raw hyper-associations {As}5s=3 from three stages; 2) employing aggregation networks on {As}5s=3 to obtain association features {FAs}5s=3.\n\nEarlier methods [4, 11, 31] directly utilize the original backbone features, i.e. FP, to calculate inter-image interactions, such as the so-called cost volume. We argue that hyperassociations derived straight from backbone features might obstruct further improvement of deep association learning, given that the current backbone is pre-trained without any consideration for inter-image associations.\n\nTo alleviate this problem, we propose the PAG module to progressively generate pyramid association features so that we can utilize the high-level association feature, e.g. FAs+1 from the (s + 1)-th stage, to enhance the neighbouring low-level VGG features in FP, e.g. Fs,l, for attaining association-enhanced features F̂s,l, based on which we can calculate high-quality hyperassociations As in the s-th stage. After that, we execute the subsequent aggregation networks on As to achieve association features FAs, which will continue to enhance the VGG features of the next stage and carry out progressive association generation. The whole process of our PAG can be formulated as follows:\n\nAs = HAC({F̂s,l}Lsl=1),\n\nFAs = Agg(As),                        (3)\n\n{F̂s-1,l}Ls-1l=1 = Enh({Fs-1,l}Ls-1l=1; FAs),\n\nwhere s ranges from 5 to 3 and {F̂5,l}L5l=1 = {F5,l}L5l=1. The HAC, Agg, and Enh represent the hyperassociation calculation, aggregation network, and association-induced feature enhancement, respectively. Next, we explain them in detail.\n\n**Hyperassociation Calculation.** For each stage, we first compute the raw associations at each layer using the inner product between l-2 normalized association-enhanced features of N related images. After that, we stack the raw associations of all layers to form the final hyperassociation of this stage. The hyperassociation for the s-th stage, i.e. As ∈ RN×Hs×Ws×N×Hs×Ws×Ls, can be calculated via:\n\nAs = HAC({F̂s,l}Lsl=1),\n\n= Stack({ReLU(F̂s,l · F̂⊤s,l / ∥F̂s,l∥∥F̂⊤s,l∥)}Lsl=1),                   (4)\n\nwhere F̂s,l · F̂⊤s,l ∈ RN×Hs×Ws×N×Hs×Ws. The ⊤ indicates transposing the last dimension and the first three dimensions. ∥·∥ represents the l-2 norm. We employ ReLU to suppress noisy association values.\n\n**Aggregation Network.** The raw hyperassociation As ∈ RN×Hs×Ws×N×Hs×Ws×Ls is a hypercube with a nested structure, where each pixel position is characterized by a 4D tensor (N × Hs × Ws × Ls). Each 4D tensor documents the associations of the respective pixel with all other pixels in N related images. For",
      "images": [
        {
          "name": "page_6.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_6_text_1.jpg",
          "height": 107.579,
          "width": 347.061,
          "x": 133.768,
          "y": 234.034,
          "original_width": 700,
          "original_height": 217,
          "type": "layout_text"
        },
        {
          "name": "page_6_text_2.jpg",
          "height": 35.107,
          "width": 346.594,
          "x": 134.211,
          "y": 138.315,
          "original_width": 699,
          "original_height": 70,
          "type": "layout_text"
        },
        {
          "name": "page_6_text_3.jpg",
          "height": 46.991,
          "width": 346.523,
          "x": 134.195,
          "y": 618.231,
          "original_width": 699,
          "original_height": 94,
          "type": "layout_text"
        },
        {
          "name": "page_6_sectionHeader_1.jpg",
          "height": 10.112,
          "width": 203.905,
          "x": 134.395,
          "y": 117.959,
          "original_width": 411,
          "original_height": 20,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_6_text_4.jpg",
          "height": 58.797,
          "width": 347.024,
          "x": 133.82,
          "y": 447.791,
          "original_width": 700,
          "original_height": 118,
          "type": "layout_text"
        },
        {
          "name": "page_6_formula_1.jpg",
          "height": 48.838,
          "width": 259.755,
          "x": 220.322,
          "y": 351.091,
          "original_width": 524,
          "original_height": 98,
          "type": "layout_formula"
        },
        {
          "name": "page_6_text_5.jpg",
          "height": 58.397,
          "width": 346.46,
          "x": 133.843,
          "y": 174.173,
          "original_width": 699,
          "original_height": 117,
          "type": "layout_text"
        },
        {
          "name": "page_6_text_6.jpg",
          "height": 38.942,
          "width": 346.602,
          "x": 133.938,
          "y": 578.4,
          "original_width": 700,
          "original_height": 78,
          "type": "layout_text"
        },
        {
          "name": "page_6_text_7.jpg",
          "height": 35.307,
          "width": 346.801,
          "x": 133.939,
          "y": 410.766,
          "original_width": 700,
          "original_height": 71,
          "type": "layout_text"
        },
        {
          "name": "page_6_formula_2.jpg",
          "height": 51.758,
          "width": 265.873,
          "x": 213.893,
          "y": 516.285,
          "original_width": 536,
          "original_height": 104,
          "type": "layout_formula"
        },
        {
          "name": "page_6_pageHeader_1.jpg",
          "height": 7.812,
          "width": 41.701,
          "x": 167,
          "y": 92.71,
          "original_width": 84,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_6_pageHeader_2.jpg",
          "height": 7.603,
          "width": 5.347,
          "x": 134.189,
          "y": 92.726,
          "original_width": 10,
          "original_height": 15,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "6     L Li et al.",
          "md": "6     L Li et al.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 259,
            "h": 436
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "3.1 Progressive Association Generation",
          "md": "## 3.1 Progressive Association Generation",
          "bBox": {
            "x": 134,
            "y": 116.04,
            "w": 259,
            "h": 472.98
          }
        },
        {
          "type": "text",
          "value": "Our deep association learning involves two steps: 1) acquiring the raw hyper-associations {As}5s=3 from three stages; 2) employing aggregation networks on {As}5s=3 to obtain association features {FAs}5s=3.\n\nEarlier methods [4, 11, 31] directly utilize the original backbone features, i.e. FP, to calculate inter-image interactions, such as the so-called cost volume. We argue that hyperassociations derived straight from backbone features might obstruct further improvement of deep association learning, given that the current backbone is pre-trained without any consideration for inter-image associations.\n\nTo alleviate this problem, we propose the PAG module to progressively generate pyramid association features so that we can utilize the high-level association feature, e.g. FAs+1 from the (s + 1)-th stage, to enhance the neighbouring low-level VGG features in FP, e.g. Fs,l, for attaining association-enhanced features F̂s,l, based on which we can calculate high-quality hyperassociations As in the s-th stage. After that, we execute the subsequent aggregation networks on As to achieve association features FAs, which will continue to enhance the VGG features of the next stage and carry out progressive association generation. The whole process of our PAG can be formulated as follows:\n\nAs = HAC({F̂s,l}Lsl=1),\n\nFAs = Agg(As),                        (3)\n\n{F̂s-1,l}Ls-1l=1 = Enh({Fs-1,l}Ls-1l=1; FAs),\n\nwhere s ranges from 5 to 3 and {F̂5,l}L5l=1 = {F5,l}L5l=1. The HAC, Agg, and Enh represent the hyperassociation calculation, aggregation network, and association-induced feature enhancement, respectively. Next, we explain them in detail.\n\n**Hyperassociation Calculation.** For each stage, we first compute the raw associations at each layer using the inner product between l-2 normalized association-enhanced features of N related images. After that, we stack the raw associations of all layers to form the final hyperassociation of this stage. The hyperassociation for the s-th stage, i.e. As ∈ RN×Hs×Ws×N×Hs×Ws×Ls, can be calculated via:\n\nAs = HAC({F̂s,l}Lsl=1),\n\n= Stack({ReLU(F̂s,l · F̂⊤s,l / ∥F̂s,l∥∥F̂⊤s,l∥)}Lsl=1),                   (4)\n\nwhere F̂s,l · F̂⊤s,l ∈ RN×Hs×Ws×N×Hs×Ws. The ⊤ indicates transposing the last dimension and the first three dimensions. ∥·∥ represents the l-2 norm. We employ ReLU to suppress noisy association values.\n\n**Aggregation Network.** The raw hyperassociation As ∈ RN×Hs×Ws×N×Hs×Ws×Ls is a hypercube with a nested structure, where each pixel position is characterized by a 4D tensor (N × Hs × Ws × Ls). Each 4D tensor documents the associations of the respective pixel with all other pixels in N related images. For",
          "md": "Our deep association learning involves two steps: 1) acquiring the raw hyper-associations {As}5s=3 from three stages; 2) employing aggregation networks on {As}5s=3 to obtain association features {FAs}5s=3.\n\nEarlier methods [4, 11, 31] directly utilize the original backbone features, i.e. FP, to calculate inter-image interactions, such as the so-called cost volume. We argue that hyperassociations derived straight from backbone features might obstruct further improvement of deep association learning, given that the current backbone is pre-trained without any consideration for inter-image associations.\n\nTo alleviate this problem, we propose the PAG module to progressively generate pyramid association features so that we can utilize the high-level association feature, e.g. FAs+1 from the (s + 1)-th stage, to enhance the neighbouring low-level VGG features in FP, e.g. Fs,l, for attaining association-enhanced features F̂s,l, based on which we can calculate high-quality hyperassociations As in the s-th stage. After that, we execute the subsequent aggregation networks on As to achieve association features FAs, which will continue to enhance the VGG features of the next stage and carry out progressive association generation. The whole process of our PAG can be formulated as follows:\n\nAs = HAC({F̂s,l}Lsl=1),\n\nFAs = Agg(As),                        (3)\n\n{F̂s-1,l}Ls-1l=1 = Enh({Fs-1,l}Ls-1l=1; FAs),\n\nwhere s ranges from 5 to 3 and {F̂5,l}L5l=1 = {F5,l}L5l=1. The HAC, Agg, and Enh represent the hyperassociation calculation, aggregation network, and association-induced feature enhancement, respectively. Next, we explain them in detail.\n\n**Hyperassociation Calculation.** For each stage, we first compute the raw associations at each layer using the inner product between l-2 normalized association-enhanced features of N related images. After that, we stack the raw associations of all layers to form the final hyperassociation of this stage. The hyperassociation for the s-th stage, i.e. As ∈ RN×Hs×Ws×N×Hs×Ws×Ls, can be calculated via:\n\nAs = HAC({F̂s,l}Lsl=1),\n\n= Stack({ReLU(F̂s,l · F̂⊤s,l / ∥F̂s,l∥∥F̂⊤s,l∥)}Lsl=1),                   (4)\n\nwhere F̂s,l · F̂⊤s,l ∈ RN×Hs×Ws×N×Hs×Ws. The ⊤ indicates transposing the last dimension and the first three dimensions. ∥·∥ represents the l-2 norm. We employ ReLU to suppress noisy association values.\n\n**Aggregation Network.** The raw hyperassociation As ∈ RN×Hs×Ws×N×Hs×Ws×Ls is a hypercube with a nested structure, where each pixel position is characterized by a 4D tensor (N × Hs × Ws × Ls). Each 4D tensor documents the associations of the respective pixel with all other pixels in N related images. For",
          "bBox": {
            "x": 134,
            "y": 116.04,
            "w": 346,
            "h": 547
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.729,
      "layout": [
        {
          "image": "page_6_text_1.jpg",
          "confidence": 0.976,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.294,
            "w": 0.567,
            "h": 0.137
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_text_2.jpg",
          "confidence": 0.956,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.173,
            "w": 0.566,
            "h": 0.045
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_text_3.jpg",
          "confidence": 0.951,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.779,
            "w": 0.566,
            "h": 0.06
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_sectionHeader_1.jpg",
          "confidence": 0.95,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.148,
            "w": 0.333,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_text_4.jpg",
          "confidence": 0.95,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.564,
            "w": 0.567,
            "h": 0.075
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_formula_1.jpg",
          "confidence": 0.948,
          "label": "formula",
          "bbox": {
            "x": 0.357,
            "y": 0.441,
            "w": 0.426,
            "h": 0.062
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_text_5.jpg",
          "confidence": 0.946,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.218,
            "w": 0.566,
            "h": 0.075
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_text_6.jpg",
          "confidence": 0.938,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.729,
            "w": 0.566,
            "h": 0.049
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_text_7.jpg",
          "confidence": 0.928,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.515,
            "w": 0.566,
            "h": 0.048
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_formula_2.jpg",
          "confidence": 0.883,
          "label": "formula",
          "bbox": {
            "x": 0.349,
            "y": 0.649,
            "w": 0.434,
            "h": 0.068
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_pageHeader_1.jpg",
          "confidence": 0.878,
          "label": "pageHeader",
          "bbox": {
            "x": 0.272,
            "y": 0.117,
            "w": 0.068,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_6_pageHeader_2.jpg",
          "confidence": 0.834,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 7,
      "text": "                                                                     CONDA                                      7\n\n                                   clarity, we designate the first and second N × Hs × Ws dimensions in As as the\n                                source and target dimensions, respectively. Although these 4D tensors are crucial\n                              for exploring the consensus information for co-saliency detection, they essentially\n                                  comprise pixel-to-pixel similarity values, seen in (4), which may be suboptimal\n                                   and unreliable in complex scenarios. Therefore, we propose using deep networks\n                              to transform these pixel-wise similarities into deep association features with con-\n                                    textual and high-order association knowledge. This has never been explored in\n                                       previous CoSOD methods. This is implemented via context aggregations on As\nto squeeze these 4D tensors as Cs-dimensional vectors, formulated as:\n\n                                                         N ×Hs×Ws×N ×Hs×Ws×Ls → N ×Hs×Ws×Cs.                  (5)\n\n                                       In detail, we first deploy several association aggregation layers on As to\n                                   progressively aggregate context information, enlarging Ls as Cs, and eliminate\n                                   the target Hs × Ws dimension in 4D tensors. Each aggregation layer consists of\n                                    2D convolution layers and a downsampling operation. Specifically, focusing on\n                                the first aggregation layer for technical explanation, we first aggregate context\n                                   information in the target Hs × Ws dimension by applying a 2D convolution layer\non all 4D tensors. The operations on the 4D tensor at pixel position (hi, wi)                                  in\nimage  Ii can be formulated as:\n\n         A1(i, hi, wi, j, :, :, :) = C1(As(i, hi, wi, j, :,:,:)),  j = 1, · · · , N ,                         (6)\n           s\nwhere  C1 is a 3 × 3 2D convolution layer. Here,                                  j is the index of other related\nimages in the 4D tensor, and As(i, hi, wi, j, :,:, :) ∈ RHₛ×Wₛ×Lₛ      illustrates the\nassociations between the pixel     (hi, wi) in Ii  and all pixels in image                               Ij. This\ninterpretation applies to other similar symbols.\n                                     Then, a downsampling operation D, i.e. bilinear interpolation, is applied on\nA1 to reduce the spatial dimension of the 4D tensor by a scaling factor:\n  s\n                  A2(i, hi, wi, j, :,:, :) = D(A1(i, hi, wi, j, :,:, :)),                                     (7)\n                    s                          s\n\nwhere A2(i, h , w  , j, :, :, :) ∈ RH′ ×W′×C′, H′  and W                                 ′ are downsampled height\n         s    ′i  i                s   s   s    s        s\nand width. Cs is the channel number after convolution C1.\n                                      Finally, we also aggregate context information in the source Hs × Ws dimen-\n                                 sion. Specifically, we extract the values along the source dimension and channel\n                                     dimension in A2 to form 4D tensors and apply a 2D convolution layer on them.\nFor instance,   s2                       N ×H ×W ×C′\n(h , w )       As(:,:,:, j, hj, wj,:) ∈ R     s    s   s                               is such a 4D tensor, where\n  j   j  is a pixel position in the target dimension. This can be formulated as:\n\n         A3(i, :,:, j, hj , wj, :) = C2 A2(i, :,:, j, hj, wj ,:) ,  i = 1, · · · , N,                         (8)\n           s                           s\nwhere C2  is a 3 × 3 2D convolution layer.   i is the related image index.\n                                        After several association aggregation layers, as shown in (6)-(8), we can\nachieve the aggregated association features with the target Hs × Ws                                        dimen-\nsion eliminated, denoted as FA′                                            ∈ RN×Hˢ×Wˢ×N×C. Subsequently, we aver-\nage F A′  on its second N         s\n       s                                                      dimension and obtain the final association features",
      "md": "clarity, we designate the first and second N × Hs × Ws dimensions in As as the\nsource and target dimensions, respectively. Although these 4D tensors are crucial\nfor exploring the consensus information for co-saliency detection, they essentially\ncomprise pixel-to-pixel similarity values, seen in (4), which may be suboptimal\nand unreliable in complex scenarios. Therefore, we propose using deep networks\nto transform these pixel-wise similarities into deep association features with con-\ntextual and high-order association knowledge. This has never been explored in\nprevious CoSOD methods. This is implemented via context aggregations on As\nto squeeze these 4D tensors as Cs-dimensional vectors, formulated as:\n\n$$N \\times H_s \\times W_s \\times N \\times H_s \\times W_s \\times L_s \\rightarrow N \\times H_s \\times W_s \\times C_s.$$\n\n(5)\n\nIn detail, we first deploy several association aggregation layers on As to\nprogressively aggregate context information, enlarging Ls as Cs, and eliminate\nthe target Hs × Ws dimension in 4D tensors. Each aggregation layer consists of\n2D convolution layers and a downsampling operation. Specifically, focusing on\nthe first aggregation layer for technical explanation, we first aggregate context\ninformation in the target Hs × Ws dimension by applying a 2D convolution layer\non all 4D tensors. The operations on the 4D tensor at pixel position (hi, wi) in\nimage Ii can be formulated as:\n\n$$A_s^1(i, h_i, w_i, j, :, :, :) = C_1(A_s(i, h_i, w_i, j, :, :, :)), \\quad j = 1, \\cdots , N,$$\n\n(6)\n\nwhere C1 is a 3 × 3 2D convolution layer. Here, j is the index of other related\nimages in the 4D tensor, and As(i, hi, wi, j, :, :, :) ∈ R^(Hs×Ws×Ls) illustrates the\nassociations between the pixel (hi, wi) in Ii and all pixels in image Ij. This\ninterpretation applies to other similar symbols.\n\nThen, a downsampling operation D, i.e. bilinear interpolation, is applied on\nA^1_s to reduce the spatial dimension of the 4D tensor by a scaling factor:\n\n$$A_s^2(i, h_i, w_i, j, :, :, :) = D(A_s^1(i, h_i, w_i, j, :, :, :)),$$\n\n(7)\n\nwhere A^2_s(i, hi, wi, j, :, :, :) ∈ R^(H's×W's×C's), H's and W's are downsampled height\nand width. C's is the channel number after convolution C1.\n\nFinally, we also aggregate context information in the source Hs × Ws dimen-\nsion. Specifically, we extract the values along the source dimension and channel\ndimension in A^2_s to form 4D tensors and apply a 2D convolution layer on them.\nFor instance, A^2_s(:, :, :, j, hj, wj, :) ∈ R^(N×Hs×Ws×C's) is such a 4D tensor, where\n(hj, wj) is a pixel position in the target dimension. This can be formulated as:\n\n$$A_s^3(i, :, :, j, h_j, w_j, :) = C_2(A_s^2(i, :, :, j, h_j, w_j, :)), \\quad i = 1, \\cdots , N,$$\n\n(8)\n\nwhere C2 is a 3 × 3 2D convolution layer. i is the related image index.\n\nAfter several association aggregation layers, as shown in (6)-(8), we can\nachieve the aggregated association features with the target Hs × Ws dimen-\nsion eliminated, denoted as F^A's ∈ R^(N×Hs×Ws×N×C). Subsequently, we aver-\nage F^A's on its second N dimension and obtain the final association features",
      "images": [
        {
          "name": "page_7.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_7_text_1.jpg",
          "height": 93.978,
          "width": 347.246,
          "x": 133.812,
          "y": 257.159,
          "original_width": 701,
          "original_height": 189,
          "type": "layout_text"
        },
        {
          "name": "page_7_text_2.jpg",
          "height": 106.203,
          "width": 347.326,
          "x": 133.736,
          "y": 117.694,
          "original_width": 701,
          "original_height": 214,
          "type": "layout_text"
        },
        {
          "name": "page_7_text_3.jpg",
          "height": 46.425,
          "width": 346.927,
          "x": 133.834,
          "y": 383.807,
          "original_width": 700,
          "original_height": 93,
          "type": "layout_text"
        },
        {
          "name": "page_7_text_4.jpg",
          "height": 59.452,
          "width": 346.64,
          "x": 134.045,
          "y": 513.038,
          "original_width": 700,
          "original_height": 120,
          "type": "layout_text"
        },
        {
          "name": "page_7_text_5.jpg",
          "height": 49.601,
          "width": 346.657,
          "x": 133.866,
          "y": 616.396,
          "original_width": 700,
          "original_height": 100,
          "type": "layout_text"
        },
        {
          "name": "page_7_text_6.jpg",
          "height": 22.84,
          "width": 346.556,
          "x": 134.356,
          "y": 431.811,
          "original_width": 699,
          "original_height": 46,
          "type": "layout_text"
        },
        {
          "name": "page_7_formula_1.jpg",
          "height": 13.422,
          "width": 307.619,
          "x": 172.762,
          "y": 360.997,
          "original_width": 621,
          "original_height": 27,
          "type": "layout_formula"
        },
        {
          "name": "page_7_formula_2.jpg",
          "height": 13.171,
          "width": 271.359,
          "x": 208.844,
          "y": 463.652,
          "original_width": 548,
          "original_height": 26,
          "type": "layout_formula"
        },
        {
          "name": "page_7_text_7.jpg",
          "height": 24.244,
          "width": 346.385,
          "x": 134.106,
          "y": 487.432,
          "original_width": 699,
          "original_height": 48,
          "type": "layout_text"
        },
        {
          "name": "page_7_formula_3.jpg",
          "height": 11.016,
          "width": 286.869,
          "x": 193.678,
          "y": 234.717,
          "original_width": 579,
          "original_height": 22,
          "type": "layout_formula"
        },
        {
          "name": "page_7_formula_4.jpg",
          "height": 13.33,
          "width": 309.296,
          "x": 170.991,
          "y": 581.327,
          "original_width": 624,
          "original_height": 26,
          "type": "layout_formula"
        },
        {
          "name": "page_7_pageHeader_1.jpg",
          "height": 8.094,
          "width": 34.572,
          "x": 413.079,
          "y": 92.564,
          "original_width": 69,
          "original_height": 16,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_7_text_8.jpg",
          "height": 10.446,
          "width": 304.242,
          "x": 133.654,
          "y": 604.276,
          "original_width": 614,
          "original_height": 21,
          "type": "layout_text"
        },
        {
          "name": "page_7_pageHeader_2.jpg",
          "height": 7.362,
          "width": 5.085,
          "x": 475.698,
          "y": 92.973,
          "original_width": 10,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "clarity, we designate the first and second N × Hs × Ws dimensions in As as the\nsource and target dimensions, respectively. Although these 4D tensors are crucial\nfor exploring the consensus information for co-saliency detection, they essentially\ncomprise pixel-to-pixel similarity values, seen in (4), which may be suboptimal\nand unreliable in complex scenarios. Therefore, we propose using deep networks\nto transform these pixel-wise similarities into deep association features with con-\ntextual and high-order association knowledge. This has never been explored in\nprevious CoSOD methods. This is implemented via context aggregations on As\nto squeeze these 4D tensors as Cs-dimensional vectors, formulated as:\n\n$$N \\times H_s \\times W_s \\times N \\times H_s \\times W_s \\times L_s \\rightarrow N \\times H_s \\times W_s \\times C_s.$$\n\n(5)\n\nIn detail, we first deploy several association aggregation layers on As to\nprogressively aggregate context information, enlarging Ls as Cs, and eliminate\nthe target Hs × Ws dimension in 4D tensors. Each aggregation layer consists of\n2D convolution layers and a downsampling operation. Specifically, focusing on\nthe first aggregation layer for technical explanation, we first aggregate context\ninformation in the target Hs × Ws dimension by applying a 2D convolution layer\non all 4D tensors. The operations on the 4D tensor at pixel position (hi, wi) in\nimage Ii can be formulated as:\n\n$$A_s^1(i, h_i, w_i, j, :, :, :) = C_1(A_s(i, h_i, w_i, j, :, :, :)), \\quad j = 1, \\cdots , N,$$\n\n(6)\n\nwhere C1 is a 3 × 3 2D convolution layer. Here, j is the index of other related\nimages in the 4D tensor, and As(i, hi, wi, j, :, :, :) ∈ R^(Hs×Ws×Ls) illustrates the\nassociations between the pixel (hi, wi) in Ii and all pixels in image Ij. This\ninterpretation applies to other similar symbols.\n\nThen, a downsampling operation D, i.e. bilinear interpolation, is applied on\nA^1_s to reduce the spatial dimension of the 4D tensor by a scaling factor:\n\n$$A_s^2(i, h_i, w_i, j, :, :, :) = D(A_s^1(i, h_i, w_i, j, :, :, :)),$$\n\n(7)\n\nwhere A^2_s(i, hi, wi, j, :, :, :) ∈ R^(H's×W's×C's), H's and W's are downsampled height\nand width. C's is the channel number after convolution C1.\n\nFinally, we also aggregate context information in the source Hs × Ws dimen-\nsion. Specifically, we extract the values along the source dimension and channel\ndimension in A^2_s to form 4D tensors and apply a 2D convolution layer on them.\nFor instance, A^2_s(:, :, :, j, hj, wj, :) ∈ R^(N×Hs×Ws×C's) is such a 4D tensor, where\n(hj, wj) is a pixel position in the target dimension. This can be formulated as:\n\n$$A_s^3(i, :, :, j, h_j, w_j, :) = C_2(A_s^2(i, :, :, j, h_j, w_j, :)), \\quad i = 1, \\cdots , N,$$\n\n(8)\n\nwhere C2 is a 3 × 3 2D convolution layer. i is the related image index.\n\nAfter several association aggregation layers, as shown in (6)-(8), we can\nachieve the aggregated association features with the target Hs × Ws dimen-\nsion eliminated, denoted as F^A's ∈ R^(N×Hs×Ws×N×C). Subsequently, we aver-\nage F^A's on its second N dimension and obtain the final association features",
          "md": "clarity, we designate the first and second N × Hs × Ws dimensions in As as the\nsource and target dimensions, respectively. Although these 4D tensors are crucial\nfor exploring the consensus information for co-saliency detection, they essentially\ncomprise pixel-to-pixel similarity values, seen in (4), which may be suboptimal\nand unreliable in complex scenarios. Therefore, we propose using deep networks\nto transform these pixel-wise similarities into deep association features with con-\ntextual and high-order association knowledge. This has never been explored in\nprevious CoSOD methods. This is implemented via context aggregations on As\nto squeeze these 4D tensors as Cs-dimensional vectors, formulated as:\n\n$$N \\times H_s \\times W_s \\times N \\times H_s \\times W_s \\times L_s \\rightarrow N \\times H_s \\times W_s \\times C_s.$$\n\n(5)\n\nIn detail, we first deploy several association aggregation layers on As to\nprogressively aggregate context information, enlarging Ls as Cs, and eliminate\nthe target Hs × Ws dimension in 4D tensors. Each aggregation layer consists of\n2D convolution layers and a downsampling operation. Specifically, focusing on\nthe first aggregation layer for technical explanation, we first aggregate context\ninformation in the target Hs × Ws dimension by applying a 2D convolution layer\non all 4D tensors. The operations on the 4D tensor at pixel position (hi, wi) in\nimage Ii can be formulated as:\n\n$$A_s^1(i, h_i, w_i, j, :, :, :) = C_1(A_s(i, h_i, w_i, j, :, :, :)), \\quad j = 1, \\cdots , N,$$\n\n(6)\n\nwhere C1 is a 3 × 3 2D convolution layer. Here, j is the index of other related\nimages in the 4D tensor, and As(i, hi, wi, j, :, :, :) ∈ R^(Hs×Ws×Ls) illustrates the\nassociations between the pixel (hi, wi) in Ii and all pixels in image Ij. This\ninterpretation applies to other similar symbols.\n\nThen, a downsampling operation D, i.e. bilinear interpolation, is applied on\nA^1_s to reduce the spatial dimension of the 4D tensor by a scaling factor:\n\n$$A_s^2(i, h_i, w_i, j, :, :, :) = D(A_s^1(i, h_i, w_i, j, :, :, :)),$$\n\n(7)\n\nwhere A^2_s(i, hi, wi, j, :, :, :) ∈ R^(H's×W's×C's), H's and W's are downsampled height\nand width. C's is the channel number after convolution C1.\n\nFinally, we also aggregate context information in the source Hs × Ws dimen-\nsion. Specifically, we extract the values along the source dimension and channel\ndimension in A^2_s to form 4D tensors and apply a 2D convolution layer on them.\nFor instance, A^2_s(:, :, :, j, hj, wj, :) ∈ R^(N×Hs×Ws×C's) is such a 4D tensor, where\n(hj, wj) is a pixel position in the target dimension. This can be formulated as:\n\n$$A_s^3(i, :, :, j, h_j, w_j, :) = C_2(A_s^2(i, :, :, j, h_j, w_j, :)), \\quad i = 1, \\cdots , N,$$\n\n(8)\n\nwhere C2 is a 3 × 3 2D convolution layer. i is the related image index.\n\nAfter several association aggregation layers, as shown in (6)-(8), we can\nachieve the aggregated association features with the target Hs × Ws dimen-\nsion eliminated, denoted as F^A's ∈ R^(N×Hs×Ws×N×C). Subsequently, we aver-\nage F^A's on its second N dimension and obtain the final association features",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 347,
            "h": 574.99
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.958,
      "layout": [
        {
          "image": "page_7_text_1.jpg",
          "confidence": 0.988,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.322,
            "w": 0.567,
            "h": 0.121
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_2.jpg",
          "confidence": 0.988,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.146,
            "w": 0.567,
            "h": 0.136
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_3.jpg",
          "confidence": 0.979,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.482,
            "w": 0.566,
            "h": 0.06
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_4.jpg",
          "confidence": 0.977,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.646,
            "w": 0.566,
            "h": 0.076
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_5.jpg",
          "confidence": 0.968,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.776,
            "w": 0.566,
            "h": 0.064
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_6.jpg",
          "confidence": 0.958,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.542,
            "w": 0.566,
            "h": 0.031
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_formula_1.jpg",
          "confidence": 0.951,
          "label": "formula",
          "bbox": {
            "x": 0.282,
            "y": 0.455,
            "w": 0.502,
            "h": 0.016
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_formula_2.jpg",
          "confidence": 0.945,
          "label": "formula",
          "bbox": {
            "x": 0.339,
            "y": 0.584,
            "w": 0.444,
            "h": 0.017
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_7.jpg",
          "confidence": 0.943,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.613,
            "w": 0.566,
            "h": 0.032
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_formula_3.jpg",
          "confidence": 0.941,
          "label": "formula",
          "bbox": {
            "x": 0.316,
            "y": 0.295,
            "w": 0.468,
            "h": 0.014
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_formula_4.jpg",
          "confidence": 0.934,
          "label": "formula",
          "bbox": {
            "x": 0.279,
            "y": 0.732,
            "w": 0.505,
            "h": 0.018
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_pageHeader_1.jpg",
          "confidence": 0.928,
          "label": "pageHeader",
          "bbox": {
            "x": 0.674,
            "y": 0.116,
            "w": 0.056,
            "h": 0.01
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_text_8.jpg",
          "confidence": 0.911,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.762,
            "w": 0.497,
            "h": 0.013
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_7_pageHeader_2.jpg",
          "confidence": 0.872,
          "label": "pageHeader",
          "bbox": {
            "x": 0.777,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 8,
      "text": "8       L Li et al.\n\n           𝐼𝐼𝑖𝑖       𝐼𝐼𝑗𝑗         𝐼𝐼𝑖𝑖       𝐼𝐼𝑗𝑗       𝐼𝐼𝑗𝑗\n\n                                                       ℎ𝑗𝑗0,𝑤𝑤𝑗𝑗0\n         ℎ𝑖𝑖, 𝑤𝑤𝑖𝑖                  ℎ𝑖𝑖, 𝑤𝑤𝑖𝑖                   ℎ𝑗𝑗,𝑤𝑤𝑗𝑗\n\n                  (a)                                     (b)\nFig. 3: Difference between full-pixel hyperassociation (a) and condensed hy-\nperassociation (b). We provide an example of collecting the pixel associations from\nimage Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel\nassociations in   Ij, while our condensed hyperassociation only collects the associations\nof its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We\nfirst heuristically find an initial pixel (h⁰, w⁰) with a fixed surrounding window and\n                                         j   j\nthen learn coordinate offsets to locate the optimized correspondence and surrounding\npixels.\n\nF A ∈ RN×Hₛ×Wₛ×C, formulated as:\n  s\n\n                          FA =    1  N      FA′(:,:,:, j, :).                         (9)\n                            s     N     j=1   s\n\nAssociation-induced Feature Enhancement. Once we have obtained asso-\nciation feature F A of the s-th stage, we will use it to enhance the VGG feature\nof the (s−        s                    L                                   A\n           1)-th stage,  i.e. {Fs−1,l}l s−¹. Specifically, we upsample F         to align\nthe spatial size of features in the (s−=1                                  s      L\n                                       1)-th stage, and then add it to {Fs−1,l}l s−¹\nfollowed by a convolution layer, formulated as:                                   =1\n\n                           ˆ                           A\n                          Fs−1,l = C3 F s−1,l + U(F s )    ,                         (10)\n\nwhere  C3 and U              represent a 2D convolution layer and the bilinear upsampling\noperation, respectively.\n\n3.2   Correspondence-induced Association Condensation\n\nAlthough PAG based on full-pixel hyperassociations can deliver satisfactory per-\nformance in CoSOD, it also introduces substantial computational overhead. Ad-\nditionally, we argue that for each pixel in an image, it is unnecessary to gather\nits associations with all pixels of other related images to form hyperassociations.\nSome pixel associations may even impair the final performance, such as those\nbetween ambiguous regions. To this end, this subsection try to condense the orig-\ninal full-pixel hyperassociations to retain only informative pixel associations.\n              This subsection will focus on explaining the condensation of pixel associa-\ntions of a pixel (e.g. (hi, wi) in Ii) to ones of the other images (e.g. image       Ij),\ni.e. As(i, hi, wi, j, :, :,:) ∈ RHˢ×Wˢ×Lˢ , as shown in Figure 3. We will simplify\nthe symbol As(i, hi, wi, j, :,:,:) as aj in the subsequent text for convenience.\n                                       s\n                Specifically, CAC opts to select K×K (K<Hs, K<Ws) informative pixel asso-\nciations from aj to form its condensed representation, i.e. ˜j       K×K×L\n                s                                             as ∈ R             s. Thus,\nthe entire condensed hyperassocaiton can be symbolized as ˜            N×H ×W ×N×\n                                                               As ∈ R       s   s",
      "md": "8       L Li et al.\n\n![Figure 3: Difference between full-pixel hyperassociation (a) and condensed hyperassociation (b). We provide an example of collecting the pixel associations from image Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel associations in Ij, while our condensed hyperassociation only collects the associations of its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We first heuristically find an initial pixel (h⁰j, w⁰j) with a fixed surrounding window and then learn coordinate offsets to locate the optimized correspondence and surrounding pixels.]\n\nFig. 3: Difference between full-pixel hyperassociation (a) and condensed hyperassociation (b). We provide an example of collecting the pixel associations from image Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel associations in Ij, while our condensed hyperassociation only collects the associations of its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We first heuristically find an initial pixel (h⁰j, w⁰j) with a fixed surrounding window and then learn coordinate offsets to locate the optimized correspondence and surrounding pixels.\n\n$$F_s^A \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C}$$, formulated as:\n\n$$F_s^A = \\frac{1}{N} \\sum_{j=1}^N F_s^{A'}(:,:,:,j,:).$$                         (9)\n\nAssociation-induced Feature Enhancement. Once we have obtained association feature $F_s^A$ of the s-th stage, we will use it to enhance the VGG feature of the (s−1)-th stage, i.e. ${F_{s-1,l}}_{l=1}^{L_{s-1}}$. Specifically, we upsample $F_s^A$ to align the spatial size of features in the (s−1)-th stage, and then add it to ${F_{s-1,l}}_{l=1}^{L_{s-1}}$ followed by a convolution layer, formulated as:\n\n$$\\hat{F}_{s-1,l} = C_3(F_{s-1,l} + U(F_s^A)),$$                         (10)\n\nwhere $C_3$ and $U$ represent a 2D convolution layer and the bilinear upsampling operation, respectively.\n\n### 3.2 Correspondence-induced Association Condensation\n\nAlthough PAG based on full-pixel hyperassociations can deliver satisfactory performance in CoSOD, it also introduces substantial computational overhead. Additionally, we argue that for each pixel in an image, it is unnecessary to gather its associations with all pixels of other related images to form hyperassociations. Some pixel associations may even impair the final performance, such as those between ambiguous regions. To this end, this subsection try to condense the original full-pixel hyperassociations to retain only informative pixel associations.\n\nThis subsection will focus on explaining the condensation of pixel associations of a pixel (e.g. (hi, wi) in Ii) to ones of the other images (e.g. image Ij), i.e. $A_s(i, h_i, w_i, j, :, :,:) \\in \\mathbb{R}^{H_s \\times W_s \\times L_s}$, as shown in Figure 3. We will simplify the symbol $A_s(i, h_i, w_i, j, :,:,:)$ as $a_s^j$ in the subsequent text for convenience.\n\nSpecifically, CAC opts to select K×K (K<Hs, K<Ws) informative pixel associations from $a_s^j$ to form its condensed representation, i.e. $\\tilde{a}_s^j \\in \\mathbb{R}^{K \\times K \\times L_s}$. Thus, the entire condensed hyperassocaiton can be symbolized as $\\tilde{A}_s \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times N \\times K \\times K \\times L_s}$.",
      "images": [
        {
          "name": "img_p7_1.png",
          "height": 375,
          "width": 500,
          "x": 211.684378055516,
          "y": 124.73899724599599,
          "original_width": 500,
          "original_height": 375
        },
        {
          "name": "img_p7_2.png",
          "height": 375,
          "width": 500,
          "x": 151.049017157888,
          "y": 124.739009847376,
          "original_width": 500,
          "original_height": 375
        },
        {
          "name": "img_p7_2.png",
          "height": 375,
          "width": 500,
          "x": 280.698615365996,
          "y": 124.838176841104,
          "original_width": 500,
          "original_height": 375
        },
        {
          "name": "img_p7_1.png",
          "height": 375,
          "width": 500,
          "x": 341.681021284892,
          "y": 124.639842894964,
          "original_width": 500,
          "original_height": 375
        },
        {
          "name": "img_p7_1.png",
          "height": 375,
          "width": 500,
          "x": 402.76260679889594,
          "y": 124.739009847376,
          "original_width": 500,
          "original_height": 375
        },
        {
          "name": "page_8.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_8_text_1.jpg",
          "height": 62.048,
          "width": 346.958,
          "x": 133.772,
          "y": 342.334,
          "original_width": 700,
          "original_height": 125,
          "type": "layout_text"
        },
        {
          "name": "page_8_text_2.jpg",
          "height": 22.33,
          "width": 346.919,
          "x": 133.92,
          "y": 437.722,
          "original_width": 700,
          "original_height": 45,
          "type": "layout_text"
        },
        {
          "name": "page_8_text_3.jpg",
          "height": 82.142,
          "width": 346.856,
          "x": 133.834,
          "y": 499.684,
          "original_width": 700,
          "original_height": 165,
          "type": "layout_text"
        },
        {
          "name": "page_8_formula_1.jpg",
          "height": 22.193,
          "width": 239.298,
          "x": 240.866,
          "y": 310.888,
          "original_width": 483,
          "original_height": 44,
          "type": "layout_formula"
        },
        {
          "name": "page_8_caption_1.jpg",
          "height": 85.191,
          "width": 347.472,
          "x": 133.666,
          "y": 196.064,
          "original_width": 701,
          "original_height": 172,
          "type": "layout_caption"
        },
        {
          "name": "page_8_sectionHeader_1.jpg",
          "height": 9.993,
          "width": 282.97,
          "x": 134.244,
          "y": 479.589,
          "original_width": 571,
          "original_height": 20,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_8_text_4.jpg",
          "height": 33.912,
          "width": 346.503,
          "x": 134.103,
          "y": 631.28,
          "original_width": 699,
          "original_height": 68,
          "type": "layout_text"
        },
        {
          "name": "page_8_formula_2.jpg",
          "height": 13.299,
          "width": 239.487,
          "x": 240.812,
          "y": 414.014,
          "original_width": 483,
          "original_height": 26,
          "type": "layout_formula"
        },
        {
          "name": "page_8_text_5.jpg",
          "height": 46.725,
          "width": 346.276,
          "x": 134.115,
          "y": 583.165,
          "original_width": 699,
          "original_height": 94,
          "type": "layout_text"
        },
        {
          "name": "page_8_pageHeader_1.jpg",
          "height": 7.481,
          "width": 5.376,
          "x": 134.187,
          "y": 92.866,
          "original_width": 10,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_8_text_6.jpg",
          "height": 12.733,
          "width": 156.944,
          "x": 134.301,
          "y": 288.651,
          "original_width": 316,
          "original_height": 25,
          "type": "layout_text"
        },
        {
          "name": "page_8_picture_1.jpg",
          "height": 68.865,
          "width": 59.621,
          "x": 402.523,
          "y": 116.309,
          "original_width": 120,
          "original_height": 139,
          "type": "layout_picture"
        },
        {
          "name": "page_8_picture_2.jpg",
          "height": 77.833,
          "width": 59.795,
          "x": 341.384,
          "y": 115.81,
          "original_width": 120,
          "original_height": 157,
          "type": "layout_picture"
        },
        {
          "name": "page_8_pageHeader_2.jpg",
          "height": 7.786,
          "width": 41.792,
          "x": 167.105,
          "y": 92.772,
          "original_width": 84,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_8_picture_3.jpg",
          "height": 62.759,
          "width": 62.205,
          "x": 277.944,
          "y": 122.332,
          "original_width": 125,
          "original_height": 126,
          "type": "layout_picture"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "8       L Li et al.\n\n![Figure 3: Difference between full-pixel hyperassociation (a) and condensed hyperassociation (b). We provide an example of collecting the pixel associations from image Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel associations in Ij, while our condensed hyperassociation only collects the associations of its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We first heuristically find an initial pixel (h⁰j, w⁰j) with a fixed surrounding window and then learn coordinate offsets to locate the optimized correspondence and surrounding pixels.]\n\nFig. 3: Difference between full-pixel hyperassociation (a) and condensed hyperassociation (b). We provide an example of collecting the pixel associations from image Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel associations in Ij, while our condensed hyperassociation only collects the associations of its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We first heuristically find an initial pixel (h⁰j, w⁰j) with a fixed surrounding window and then learn coordinate offsets to locate the optimized correspondence and surrounding pixels.\n\n$$F_s^A \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C}$$, formulated as:\n\n$$F_s^A = \\frac{1}{N} \\sum_{j=1}^N F_s^{A'}(:,:,:,j,:).$$                         (9)\n\nAssociation-induced Feature Enhancement. Once we have obtained association feature $F_s^A$ of the s-th stage, we will use it to enhance the VGG feature of the (s−1)-th stage, i.e. ${F_{s-1,l}}_{l=1}^{L_{s-1}}$. Specifically, we upsample $F_s^A$ to align the spatial size of features in the (s−1)-th stage, and then add it to ${F_{s-1,l}}_{l=1}^{L_{s-1}}$ followed by a convolution layer, formulated as:\n\n$$\\hat{F}_{s-1,l} = C_3(F_{s-1,l} + U(F_s^A)),$$                         (10)\n\nwhere $C_3$ and $U$ represent a 2D convolution layer and the bilinear upsampling operation, respectively.",
          "md": "8       L Li et al.\n\n![Figure 3: Difference between full-pixel hyperassociation (a) and condensed hyperassociation (b). We provide an example of collecting the pixel associations from image Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel associations in Ij, while our condensed hyperassociation only collects the associations of its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We first heuristically find an initial pixel (h⁰j, w⁰j) with a fixed surrounding window and then learn coordinate offsets to locate the optimized correspondence and surrounding pixels.]\n\nFig. 3: Difference between full-pixel hyperassociation (a) and condensed hyperassociation (b). We provide an example of collecting the pixel associations from image Ij for a pixel (hi, wi) in image Ii. Full-pixel hyperassociation collects all pixel associations in Ij, while our condensed hyperassociation only collects the associations of its correspondence pixel (hj, wj) (red dot) and surrounding pixels (green dots). We first heuristically find an initial pixel (h⁰j, w⁰j) with a fixed surrounding window and then learn coordinate offsets to locate the optimized correspondence and surrounding pixels.\n\n$$F_s^A \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times C}$$, formulated as:\n\n$$F_s^A = \\frac{1}{N} \\sum_{j=1}^N F_s^{A'}(:,:,:,j,:).$$                         (9)\n\nAssociation-induced Feature Enhancement. Once we have obtained association feature $F_s^A$ of the s-th stage, we will use it to enhance the VGG feature of the (s−1)-th stage, i.e. ${F_{s-1,l}}_{l=1}^{L_{s-1}}$. Specifically, we upsample $F_s^A$ to align the spatial size of features in the (s−1)-th stage, and then add it to ${F_{s-1,l}}_{l=1}^{L_{s-1}}$ followed by a convolution layer, formulated as:\n\n$$\\hat{F}_{s-1,l} = C_3(F_{s-1,l} + U(F_s^A)),$$                         (10)\n\nwhere $C_3$ and $U$ represent a 2D convolution layer and the bilinear upsampling operation, respectively.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 569.99
          }
        },
        {
          "type": "heading",
          "lvl": 3,
          "value": "3.2 Correspondence-induced Association Condensation",
          "md": "### 3.2 Correspondence-induced Association Condensation",
          "bBox": {
            "x": 134,
            "y": 295.03,
            "w": 325,
            "h": 365.99
          }
        },
        {
          "type": "text",
          "value": "Although PAG based on full-pixel hyperassociations can deliver satisfactory performance in CoSOD, it also introduces substantial computational overhead. Additionally, we argue that for each pixel in an image, it is unnecessary to gather its associations with all pixels of other related images to form hyperassociations. Some pixel associations may even impair the final performance, such as those between ambiguous regions. To this end, this subsection try to condense the original full-pixel hyperassociations to retain only informative pixel associations.\n\nThis subsection will focus on explaining the condensation of pixel associations of a pixel (e.g. (hi, wi) in Ii) to ones of the other images (e.g. image Ij), i.e. $A_s(i, h_i, w_i, j, :, :,:) \\in \\mathbb{R}^{H_s \\times W_s \\times L_s}$, as shown in Figure 3. We will simplify the symbol $A_s(i, h_i, w_i, j, :,:,:)$ as $a_s^j$ in the subsequent text for convenience.\n\nSpecifically, CAC opts to select K×K (K<Hs, K<Ws) informative pixel associations from $a_s^j$ to form its condensed representation, i.e. $\\tilde{a}_s^j \\in \\mathbb{R}^{K \\times K \\times L_s}$. Thus, the entire condensed hyperassocaiton can be symbolized as $\\tilde{A}_s \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times N \\times K \\times K \\times L_s}$.",
          "md": "Although PAG based on full-pixel hyperassociations can deliver satisfactory performance in CoSOD, it also introduces substantial computational overhead. Additionally, we argue that for each pixel in an image, it is unnecessary to gather its associations with all pixels of other related images to form hyperassociations. Some pixel associations may even impair the final performance, such as those between ambiguous regions. To this end, this subsection try to condense the original full-pixel hyperassociations to retain only informative pixel associations.\n\nThis subsection will focus on explaining the condensation of pixel associations of a pixel (e.g. (hi, wi) in Ii) to ones of the other images (e.g. image Ij), i.e. $A_s(i, h_i, w_i, j, :, :,:) \\in \\mathbb{R}^{H_s \\times W_s \\times L_s}$, as shown in Figure 3. We will simplify the symbol $A_s(i, h_i, w_i, j, :,:,:)$ as $a_s^j$ in the subsequent text for convenience.\n\nSpecifically, CAC opts to select K×K (K<Hs, K<Ws) informative pixel associations from $a_s^j$ to form its condensed representation, i.e. $\\tilde{a}_s^j \\in \\mathbb{R}^{K \\times K \\times L_s}$. Thus, the entire condensed hyperassocaiton can be symbolized as $\\tilde{A}_s \\in \\mathbb{R}^{N \\times H_s \\times W_s \\times N \\times K \\times K \\times L_s}$.",
          "bBox": {
            "x": 134,
            "y": 216.03,
            "w": 346,
            "h": 444.99
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.921,
      "layout": [
        {
          "image": "page_8_text_1.jpg",
          "confidence": 0.985,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.43,
            "w": 0.566,
            "h": 0.079
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_text_2.jpg",
          "confidence": 0.977,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.55,
            "w": 0.566,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_text_3.jpg",
          "confidence": 0.974,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.628,
            "w": 0.566,
            "h": 0.105
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_formula_1.jpg",
          "confidence": 0.961,
          "label": "formula",
          "bbox": {
            "x": 0.393,
            "y": 0.39,
            "w": 0.391,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_caption_1.jpg",
          "confidence": 0.956,
          "label": "caption",
          "bbox": {
            "x": 0.218,
            "y": 0.244,
            "w": 0.567,
            "h": 0.11
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_sectionHeader_1.jpg",
          "confidence": 0.951,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.605,
            "w": 0.462,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_text_4.jpg",
          "confidence": 0.944,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.795,
            "w": 0.566,
            "h": 0.044
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_formula_2.jpg",
          "confidence": 0.934,
          "label": "formula",
          "bbox": {
            "x": 0.393,
            "y": 0.521,
            "w": 0.391,
            "h": 0.018
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_text_5.jpg",
          "confidence": 0.928,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.734,
            "w": 0.566,
            "h": 0.06
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_pageHeader_1.jpg",
          "confidence": 0.796,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_text_6.jpg",
          "confidence": 0.792,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.363,
            "w": 0.256,
            "h": 0.017
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_picture_1.jpg",
          "confidence": 0.742,
          "label": "picture",
          "bbox": {
            "x": 0.657,
            "y": 0.145,
            "w": 0.097,
            "h": 0.088
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_picture_2.jpg",
          "confidence": 0.698,
          "label": "picture",
          "bbox": {
            "x": 0.557,
            "y": 0.145,
            "w": 0.097,
            "h": 0.098
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_pageHeader_2.jpg",
          "confidence": 0.668,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.068,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_8_picture_3.jpg",
          "confidence": 0.656,
          "label": "picture",
          "bbox": {
            "x": 0.454,
            "y": 0.154,
            "w": 0.101,
            "h": 0.079
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 9,
      "text": "                                                                     CONDA              9\n\nK×K×Ls. To ensure the proper selection of K ×K                pixels, we introduce a pre-\ntext task, i.e. semantic correspondence estimation [14,24]. This allows us to first\nlocate the corresponding pixel of     (hi, wi) in image  Ij,      i.e. (hj, wj), and then\ncombine   (hj, wj) with its surrounding pixels as the K ×K           pixel set. We design\nthis approach based on the observation that the co-salient objects across N           re-\nlated images belong to the same semantic category, and the pixels within them\nshould have semantic correspondences to each other, as shown in Figure 4. There-\nfore, the introduction of semantic correspondence in CAC not only improves the\nCoSOD performance but also delves deeper into the core nature of the CoSOD\ntask. As far as we know, this is the first work to use semantic correspondence in\nthe CoSOD task.\nCorrespondence Estimation. To estimate the correspondence pixel (hj, wj)\nin Ij for (hi, wi), we first identify an initial pixel (h0, w0) via a heuristic approach.\n                                                     j   j  0   0\nSubsequently, we produce a spatial offset to refine       (hj, wj)  into    (hj , wj). To\nachieve this purpose, all initial correspondence pixels should be utilized to form\nthe initial condensed hyperassociations, with which the initial deep association\nfeatures can be generated for spatial offset prediction.\n         Specifically, we pick out (h0, w0) that has the largest feature similarity value\nwith (h , w                     j   j                                  j        0   0\n        i     i). Since we have calculated the feature similarities in as, the (hj , wj )\ncan be obtained as follows:\n\n                                 ¯j     Lₛ      j\n                              0  as =      l=1 as(:,:, l),                           (11)\n                            (h , w0) = argmax(¯j\n                              j   j       x,y   as(x, y)),\n\nwhere aj       H ×W\n       ¯s ∈ R   s    s        is a similarity matrix obtained by eliminating the last di-\nmension of aj ∈ RHˢ×Wˢ×Lˢ through summation. argmax returns the coordinate\n             s\nof the maximum value.\n   Next, we select K × K              pixels within the square region centered around the\ninitial pixels,        e.g. (h0, w0), to construct the initial condensed hyperassociation\n˜ ′                   j   j\nAs. Then, we can feed it into the aggregation network, described in Sec 3.1,\nand achieve the initial aggregated association features FA′ ∈ RN×Hˢ×Wˢ×N×Cˢ.\n                                                             s\nIt can be regarded as the association features of each pixel to N           other related\nimages. We utilize the feature of (hi, wi) to   Ij, i.e. F A′(i, hi, wi, j, :) ∈ RCₛ , to\npredict the offsets for (h  , w )                          s\n                          j   j  via a linear layer, formulated as:\n\n                            ∆j = O(FA′ (i, hi, wi, j, :)),                           (12)\n                              s         s\n\nwhere O is a linear layer for offset generation. ∆j ∈ RK×K×2 consists of K ×\nK  offsets, besides the center offset ∆j(k   , k ,:)  s\ni.e. refining (h0, w0) as (h , w )       s  c   c          for correspondence estimation,\n                j   j       j   j        , we also generate other offsets for surrounding\npixel selection. Thus, the corresponding pixel (hj , wj) can be obtained by adding\nthe offset to the initial pixel (h0, w0), formulated as:\n                                 j    j\n\n                        (hj, wj) = (h0, w0) + ∆j(kc, kc, :),                         (13)\n                                       j   j      s",
      "md": "K×K×Ls. To ensure the proper selection of K×K pixels, we introduce a pre-text task, i.e. semantic correspondence estimation [14,24]. This allows us to first locate the corresponding pixel of (hi, wi) in image Ij, i.e. (hj, wj), and then combine (hj, wj) with its surrounding pixels as the K×K pixel set. We design this approach based on the observation that the co-salient objects across N related images belong to the same semantic category, and the pixels within them should have semantic correspondences to each other, as shown in Figure 4. Therefore, the introduction of semantic correspondence in CAC not only improves the CoSOD performance but also delves deeper into the core nature of the CoSOD task. As far as we know, this is the first work to use semantic correspondence in the CoSOD task.\n\nCorrespondence Estimation. To estimate the correspondence pixel (hj, wj) in Ij for (hi, wi), we first identify an initial pixel (h⁰j, w⁰j) via a heuristic approach. Subsequently, we produce a spatial offset to refine (h⁰j, w⁰j) into (hj, wj). To achieve this purpose, all initial correspondence pixels should be utilized to form the initial condensed hyperassociations, with which the initial deep association features can be generated for spatial offset prediction.\n\nSpecifically, we pick out (h⁰j, w⁰j) that has the largest feature similarity value with (hi, wi). Since we have calculated the feature similarities in a^j_s, the (h⁰j, w⁰j) can be obtained as follows:\n\n$$\n\\bar{a}^j_s = \\sum_{l=1}^{L_s} a^j_s(:,:,l),\n$$\n\n$$\n(h^0_j, w^0_j) = \\text{argmax}_{x,y}(\\bar{a}^j_s(x,y)),\n$$\n\nwhere $\\bar{a}^j_s \\in \\mathbb{R}^{H_s \\times W_s}$ is a similarity matrix obtained by eliminating the last dimension of $a^j_s \\in \\mathbb{R}^{H_s \\times W_s \\times L_s}$ through summation. argmax returns the coordinate of the maximum value.\n\nNext, we select K × K pixels within the square region centered around the initial pixels, e.g. (h⁰j, w⁰j), to construct the initial condensed hyperassociation Ã′s. Then, we can feed it into the aggregation network, described in Sec 3.1, and achieve the initial aggregated association features F^A′_s ∈ R^(N×Hs×Ws×N×Cs). It can be regarded as the association features of each pixel to N other related images. We utilize the feature of (hi, wi) to Ij, i.e. F^A′_s(i, hi, wi, j, :) ∈ R^Cs, to predict the offsets for (hj, wj) via a linear layer, formulated as:\n\n$$\n\\Delta^j_s = O(F^{A'}_s(i, h_i, w_i, j, :)),\n$$\n\nwhere O is a linear layer for offset generation. $\\Delta^j_s \\in \\mathbb{R}^{K \\times K \\times 2}$ consists of K × K offsets, besides the center offset $\\Delta^j_s(k_c, k_c, :)$ for correspondence estimation, i.e. refining (h⁰j, w⁰j) as (hj, wj), we also generate other offsets for surrounding pixel selection. Thus, the corresponding pixel (hj, wj) can be obtained by adding the offset to the initial pixel (h⁰j, w⁰j), formulated as:\n\n$$\n(h_j, w_j) = (h^0_j, w^0_j) + \\Delta^j_s(k_c, k_c, :),\n$$",
      "images": [
        {
          "name": "page_9.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_9_text_1.jpg",
          "height": 61.004,
          "width": 346.771,
          "x": 133.965,
          "y": 581.835,
          "original_width": 700,
          "original_height": 123,
          "type": "layout_text"
        },
        {
          "name": "page_9_text_2.jpg",
          "height": 71.832,
          "width": 346.814,
          "x": 134,
          "y": 249.298,
          "original_width": 700,
          "original_height": 145,
          "type": "layout_text"
        },
        {
          "name": "page_9_text_3.jpg",
          "height": 130.159,
          "width": 347.256,
          "x": 133.666,
          "y": 117.106,
          "original_width": 701,
          "original_height": 262,
          "type": "layout_text"
        },
        {
          "name": "page_9_text_4.jpg",
          "height": 35.655,
          "width": 346.763,
          "x": 133.899,
          "y": 418.798,
          "original_width": 700,
          "original_height": 72,
          "type": "layout_text"
        },
        {
          "name": "page_9_formula_1.jpg",
          "height": 42.199,
          "width": 234.555,
          "x": 245.692,
          "y": 366.017,
          "original_width": 473,
          "original_height": 85,
          "type": "layout_formula"
        },
        {
          "name": "page_9_text_5.jpg",
          "height": 34.111,
          "width": 346.79,
          "x": 133.812,
          "y": 322.524,
          "original_width": 700,
          "original_height": 68,
          "type": "layout_text"
        },
        {
          "name": "page_9_formula_2.jpg",
          "height": 14.895,
          "width": 231.633,
          "x": 248.345,
          "y": 555.88,
          "original_width": 467,
          "original_height": 30,
          "type": "layout_formula"
        },
        {
          "name": "page_9_formula_3.jpg",
          "height": 13.926,
          "width": 246.193,
          "x": 233.853,
          "y": 653.145,
          "original_width": 497,
          "original_height": 28,
          "type": "layout_formula"
        },
        {
          "name": "page_9_pageHeader_1.jpg",
          "height": 8.179,
          "width": 34.552,
          "x": 413.043,
          "y": 92.525,
          "original_width": 69,
          "original_height": 16,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_9_text_6.jpg",
          "height": 89.3,
          "width": 346.982,
          "x": 133.739,
          "y": 456.315,
          "original_width": 700,
          "original_height": 180,
          "type": "layout_text"
        },
        {
          "name": "page_9_pageHeader_2.jpg",
          "height": 7.836,
          "width": 5.361,
          "x": 475.479,
          "y": 92.713,
          "original_width": 10,
          "original_height": 15,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "K×K×Ls. To ensure the proper selection of K×K pixels, we introduce a pre-text task, i.e. semantic correspondence estimation [14,24]. This allows us to first locate the corresponding pixel of (hi, wi) in image Ij, i.e. (hj, wj), and then combine (hj, wj) with its surrounding pixels as the K×K pixel set. We design this approach based on the observation that the co-salient objects across N related images belong to the same semantic category, and the pixels within them should have semantic correspondences to each other, as shown in Figure 4. Therefore, the introduction of semantic correspondence in CAC not only improves the CoSOD performance but also delves deeper into the core nature of the CoSOD task. As far as we know, this is the first work to use semantic correspondence in the CoSOD task.\n\nCorrespondence Estimation. To estimate the correspondence pixel (hj, wj) in Ij for (hi, wi), we first identify an initial pixel (h⁰j, w⁰j) via a heuristic approach. Subsequently, we produce a spatial offset to refine (h⁰j, w⁰j) into (hj, wj). To achieve this purpose, all initial correspondence pixels should be utilized to form the initial condensed hyperassociations, with which the initial deep association features can be generated for spatial offset prediction.\n\nSpecifically, we pick out (h⁰j, w⁰j) that has the largest feature similarity value with (hi, wi). Since we have calculated the feature similarities in a^j_s, the (h⁰j, w⁰j) can be obtained as follows:\n\n$$\n\\bar{a}^j_s = \\sum_{l=1}^{L_s} a^j_s(:,:,l),\n$$\n\n$$\n(h^0_j, w^0_j) = \\text{argmax}_{x,y}(\\bar{a}^j_s(x,y)),\n$$\n\nwhere $\\bar{a}^j_s \\in \\mathbb{R}^{H_s \\times W_s}$ is a similarity matrix obtained by eliminating the last dimension of $a^j_s \\in \\mathbb{R}^{H_s \\times W_s \\times L_s}$ through summation. argmax returns the coordinate of the maximum value.\n\nNext, we select K × K pixels within the square region centered around the initial pixels, e.g. (h⁰j, w⁰j), to construct the initial condensed hyperassociation Ã′s. Then, we can feed it into the aggregation network, described in Sec 3.1, and achieve the initial aggregated association features F^A′_s ∈ R^(N×Hs×Ws×N×Cs). It can be regarded as the association features of each pixel to N other related images. We utilize the feature of (hi, wi) to Ij, i.e. F^A′_s(i, hi, wi, j, :) ∈ R^Cs, to predict the offsets for (hj, wj) via a linear layer, formulated as:\n\n$$\n\\Delta^j_s = O(F^{A'}_s(i, h_i, w_i, j, :)),\n$$\n\nwhere O is a linear layer for offset generation. $\\Delta^j_s \\in \\mathbb{R}^{K \\times K \\times 2}$ consists of K × K offsets, besides the center offset $\\Delta^j_s(k_c, k_c, :)$ for correspondence estimation, i.e. refining (h⁰j, w⁰j) as (hj, wj), we also generate other offsets for surrounding pixel selection. Thus, the corresponding pixel (hj, wj) can be obtained by adding the offset to the initial pixel (h⁰j, w⁰j), formulated as:\n\n$$\n(h_j, w_j) = (h^0_j, w^0_j) + \\Delta^j_s(k_c, k_c, :),\n$$",
          "md": "K×K×Ls. To ensure the proper selection of K×K pixels, we introduce a pre-text task, i.e. semantic correspondence estimation [14,24]. This allows us to first locate the corresponding pixel of (hi, wi) in image Ij, i.e. (hj, wj), and then combine (hj, wj) with its surrounding pixels as the K×K pixel set. We design this approach based on the observation that the co-salient objects across N related images belong to the same semantic category, and the pixels within them should have semantic correspondences to each other, as shown in Figure 4. Therefore, the introduction of semantic correspondence in CAC not only improves the CoSOD performance but also delves deeper into the core nature of the CoSOD task. As far as we know, this is the first work to use semantic correspondence in the CoSOD task.\n\nCorrespondence Estimation. To estimate the correspondence pixel (hj, wj) in Ij for (hi, wi), we first identify an initial pixel (h⁰j, w⁰j) via a heuristic approach. Subsequently, we produce a spatial offset to refine (h⁰j, w⁰j) into (hj, wj). To achieve this purpose, all initial correspondence pixels should be utilized to form the initial condensed hyperassociations, with which the initial deep association features can be generated for spatial offset prediction.\n\nSpecifically, we pick out (h⁰j, w⁰j) that has the largest feature similarity value with (hi, wi). Since we have calculated the feature similarities in a^j_s, the (h⁰j, w⁰j) can be obtained as follows:\n\n$$\n\\bar{a}^j_s = \\sum_{l=1}^{L_s} a^j_s(:,:,l),\n$$\n\n$$\n(h^0_j, w^0_j) = \\text{argmax}_{x,y}(\\bar{a}^j_s(x,y)),\n$$\n\nwhere $\\bar{a}^j_s \\in \\mathbb{R}^{H_s \\times W_s}$ is a similarity matrix obtained by eliminating the last dimension of $a^j_s \\in \\mathbb{R}^{H_s \\times W_s \\times L_s}$ through summation. argmax returns the coordinate of the maximum value.\n\nNext, we select K × K pixels within the square region centered around the initial pixels, e.g. (h⁰j, w⁰j), to construct the initial condensed hyperassociation Ã′s. Then, we can feed it into the aggregation network, described in Sec 3.1, and achieve the initial aggregated association features F^A′_s ∈ R^(N×Hs×Ws×N×Cs). It can be regarded as the association features of each pixel to N other related images. We utilize the feature of (hi, wi) to Ij, i.e. F^A′_s(i, hi, wi, j, :) ∈ R^Cs, to predict the offsets for (hj, wj) via a linear layer, formulated as:\n\n$$\n\\Delta^j_s = O(F^{A'}_s(i, h_i, w_i, j, :)),\n$$\n\nwhere O is a linear layer for offset generation. $\\Delta^j_s \\in \\mathbb{R}^{K \\times K \\times 2}$ consists of K × K offsets, besides the center offset $\\Delta^j_s(k_c, k_c, :)$ for correspondence estimation, i.e. refining (h⁰j, w⁰j) as (hj, wj), we also generate other offsets for surrounding pixel selection. Thus, the corresponding pixel (hj, wj) can be obtained by adding the offset to the initial pixel (h⁰j, w⁰j), formulated as:\n\n$$\n(h_j, w_j) = (h^0_j, w^0_j) + \\Delta^j_s(k_c, k_c, :),\n$$",
          "bBox": {
            "x": 134,
            "y": 116.03,
            "w": 347,
            "h": 550
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.922,
      "layout": [
        {
          "image": "page_9_text_1.jpg",
          "confidence": 0.984,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.732,
            "w": 0.566,
            "h": 0.079
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_text_2.jpg",
          "confidence": 0.98,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.313,
            "w": 0.566,
            "h": 0.092
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_text_3.jpg",
          "confidence": 0.978,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.146,
            "w": 0.567,
            "h": 0.165
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_text_4.jpg",
          "confidence": 0.975,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.527,
            "w": 0.566,
            "h": 0.045
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_formula_1.jpg",
          "confidence": 0.971,
          "label": "formula",
          "bbox": {
            "x": 0.4,
            "y": 0.46,
            "w": 0.384,
            "h": 0.054
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_text_5.jpg",
          "confidence": 0.968,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.404,
            "w": 0.566,
            "h": 0.046
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_formula_2.jpg",
          "confidence": 0.948,
          "label": "formula",
          "bbox": {
            "x": 0.405,
            "y": 0.7,
            "w": 0.379,
            "h": 0.019
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_formula_3.jpg",
          "confidence": 0.942,
          "label": "formula",
          "bbox": {
            "x": 0.38,
            "y": 0.823,
            "w": 0.403,
            "h": 0.018
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_pageHeader_1.jpg",
          "confidence": 0.924,
          "label": "pageHeader",
          "bbox": {
            "x": 0.674,
            "y": 0.116,
            "w": 0.056,
            "h": 0.01
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_text_6.jpg",
          "confidence": 0.889,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.574,
            "w": 0.566,
            "h": 0.114
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_9_pageHeader_2.jpg",
          "confidence": 0.876,
          "label": "pageHeader",
          "bbox": {
            "x": 0.776,
            "y": 0.117,
            "w": 0.008,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 10,
      "text": "10      L Li et al.\n\nwhere (kc, kc) is the center position of K ×K     square.\nCondensation Operation. Given the estimated correspondence pixel (hj, wj)\nand other offsets in ∆j , we can obtain the surrounding pixels and combine\nthem with   (h  , w )    s\nnj ∈ RK×K×2j       j  to form K ×K          pixel set. Their coordinates are stored in\n s             . This process can be formulated as:\n\n      nj(x, y, :) = (hj, wj) + ∆j(x, y, :),  x, y ∈ {1, · · · , K}; x, y = kc,\n       s                         s                                                (14)\n      nj(kc, kc,:) = (hj, wj),\n       s\nwhere (kc, kc) is the center position of K ×K     square.\n            Finally, we can perform the condensation operation via the index selection\non aj , formulated as:\n     s                              aj     j   j\n                                    ˜s = as(ns),                                  (15)\nwhere ˜j                                         j\n         as is the condensed representation of as, i.e. pixel associations of (hi, wi)\nto image       Ij. Furthermore, we also illustrate the condensation for pixel associa-\ntions of (hi, wi) to all images, i.e. as ∈ RN×Hₛ×Wₛ×Lₛ, in Figure 2. By applying\nsuch a condensation process to all pixel associations, we can obtain the final\ncondensed hyperassociation     ˜\n                              As.\n\n3.3   Object-aware Cycle Consistency Loss\nTo achieve accurate correspondence estimations, applying effective supervisions\non them is necessary. As explicit semantic correspondence annotations are not\navailable, we can only rely on unsupervised losses by imposing correspondence-\nrelated constraints on estimated correspondences. Previous unsupervised ap-\nproaches apply constraints to all pixels [30, 35, 48], including those on the back-\nground and extraneous objects that don’t have mutual correspondences, hence\nharming the model effectiveness. To avoid this problem, we propose an object-\naware constraint to only access losses on the co-salient pixels.\n                We propose an Object-aware Cycle Consistency (OCC) loss for the super-\nvision of correspondence estimations in CoSOD. The cycle consistency can be\nexplained as: if a co-salient pixel (hi, wi) of image Ii corresponds to the pixel\n(hj, wj) in image Ij, then the pixel (hj, wj) should also semantically correspond\nto pixel (hi, wi).\n             Based on this cycle consistency constraint, we adopt image warping opera-\ntions to conduct the OCC loss. Specifically, we first warp the image Is (a resized\nI to align the scales in the s-th stage) as I s   using the Is → Is       i\n i                              s            i→j    s        i     sj   correspondence\nestimations. Next, we warp    Ii→j  backward to Ii→j→i using        Ij → Is correspon-\ndence estimations. Finally, we can utilize the SSIM loss between       Isi and Is\n                                                                        i   s    i→j→i\nto measure the cycle consistency for mutual correspondence pixels in Ii and Is.\n                                                                                    j\nMoreover, to ensure the constraints are only conducted on co-salient objects, we\nmask the images with their ground truth masks, formulated as:\n                              N   N\n                  LC =    1     LSSIM(Is         · Gs, Is     · Gs),              (16)\n                    s    N2 i=1  j=1           i    i  i→j→i     i",
      "md": "where $(k_c, k_c)$ is the center position of $K \\times K$ square.\n\n**Condensation Operation.** Given the estimated correspondence pixel $(h_j, w_j)$ and other offsets in $\\Delta_s^j$, we can obtain the surrounding pixels and combine them with $(h_j, w_j)$ to form $K \\times K$ pixel set. Their coordinates are stored in $n_s^j \\in \\mathbb{R}^{K\\times K\\times2}$. This process can be formulated as:\n\n$$\n\\begin{aligned}\nn_s^j(x, y, :) &= (h_j, w_j) + \\Delta_s^j(x, y, :), \\quad x, y \\in \\{1, \\cdots, K\\}; x, y \\neq k_c, \\\\\nn_s^j(k_c, k_c,:) &= (h_j, w_j),\n\\end{aligned}\n\\tag{14}\n$$\n\nwhere $(k_c, k_c)$ is the center position of $K \\times K$ square.\n\nFinally, we can perform the condensation operation via the index selection on $a_s^j$, formulated as:\n\n$$\n\\tilde{a}_s^j = a_s^j(n_s^j),\n\\tag{15}\n$$\n\nwhere $\\tilde{a}_s^j$ is the condensed representation of $a_s^j$, i.e. pixel associations of $(h_i, w_i)$ to image $I_j$. Furthermore, we also illustrate the condensation for pixel associations of $(h_i, w_i)$ to all images, i.e. $a_s \\in \\mathbb{R}^{N\\times H_s\\times W_s\\times L_s}$, in Figure 2. By applying such a condensation process to all pixel associations, we can obtain the final condensed hyperassociation $\\tilde{A}_s$.\n\n### 3.3 Object-aware Cycle Consistency Loss\n\nTo achieve accurate correspondence estimations, applying effective supervisions on them is necessary. As explicit semantic correspondence annotations are not available, we can only rely on unsupervised losses by imposing correspondence-related constraints on estimated correspondences. Previous unsupervised approaches apply constraints to all pixels [30, 35, 48], including those on the background and extraneous objects that don't have mutual correspondences, hence harming the model effectiveness. To avoid this problem, we propose an object-aware constraint to only access losses on the co-salient pixels.\n\nWe propose an Object-aware Cycle Consistency (OCC) loss for the supervision of correspondence estimations in CoSOD. The cycle consistency can be explained as: if a co-salient pixel $(h_i, w_i)$ of image $I_i$ corresponds to the pixel $(h_j, w_j)$ in image $I_j$, then the pixel $(h_j, w_j)$ should also semantically correspond to pixel $(h_i, w_i)$.\n\nBased on this cycle consistency constraint, we adopt image warping operations to conduct the OCC loss. Specifically, we first warp the image $I_i^s$ (a resized $I_i$ to align the scales in the s-th stage) as $I_{i\\rightarrow j}^s$ using the $I_i^s \\rightarrow I_j^s$ correspondence estimations. Next, we warp $I_{i\\rightarrow j}^s$ backward to $I_{i\\rightarrow j\\rightarrow i}^s$ using $I_j^s \\rightarrow I_i^s$ correspondence estimations. Finally, we can utilize the SSIM loss between $I_i^s$ and $I_{i\\rightarrow j\\rightarrow i}^s$ to measure the cycle consistency for mutual correspondence pixels in $I_i^s$ and $I_j^s$. Moreover, to ensure the constraints are only conducted on co-salient objects, we mask the images with their ground truth masks, formulated as:\n\n$$\n\\mathcal{L}_s^C = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\mathcal{L}_{SSIM}(I_i^s \\cdot G_i^s, I_{i\\rightarrow j\\rightarrow i}^s \\cdot G_i^s),\n\\tag{16}\n$$",
      "images": [
        {
          "name": "page_10.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_10_text_1.jpg",
          "height": 94.074,
          "width": 346.736,
          "x": 133.877,
          "y": 381.67,
          "original_width": 700,
          "original_height": 190,
          "type": "layout_text"
        },
        {
          "name": "page_10_text_2.jpg",
          "height": 58.783,
          "width": 347.121,
          "x": 133.74,
          "y": 282.497,
          "original_width": 701,
          "original_height": 118,
          "type": "layout_text"
        },
        {
          "name": "page_10_text_3.jpg",
          "height": 93.797,
          "width": 347.036,
          "x": 133.803,
          "y": 537.381,
          "original_width": 700,
          "original_height": 189,
          "type": "layout_text"
        },
        {
          "name": "page_10_text_4.jpg",
          "height": 58.539,
          "width": 346.962,
          "x": 133.913,
          "y": 477.22,
          "original_width": 700,
          "original_height": 118,
          "type": "layout_text"
        },
        {
          "name": "page_10_formula_1.jpg",
          "height": 32.756,
          "width": 272.559,
          "x": 207.375,
          "y": 635.575,
          "original_width": 550,
          "original_height": 66,
          "type": "layout_formula"
        },
        {
          "name": "page_10_text_5.jpg",
          "height": 47.004,
          "width": 346.962,
          "x": 134.021,
          "y": 129.717,
          "original_width": 700,
          "original_height": 94,
          "type": "layout_text"
        },
        {
          "name": "page_10_formula_2.jpg",
          "height": 29.557,
          "width": 322.955,
          "x": 157.373,
          "y": 186.924,
          "original_width": 652,
          "original_height": 59,
          "type": "layout_formula"
        },
        {
          "name": "page_10_sectionHeader_1.jpg",
          "height": 10.21,
          "width": 213.669,
          "x": 134.343,
          "y": 361.529,
          "original_width": 431,
          "original_height": 20,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_10_text_6.jpg",
          "height": 22.202,
          "width": 346.43,
          "x": 134.071,
          "y": 239.341,
          "original_width": 699,
          "original_height": 44,
          "type": "layout_text"
        },
        {
          "name": "page_10_formula_3.jpg",
          "height": 12.287,
          "width": 201.066,
          "x": 279.058,
          "y": 262.072,
          "original_width": 406,
          "original_height": 24,
          "type": "layout_formula"
        },
        {
          "name": "page_10_pageHeader_1.jpg",
          "height": 7.915,
          "width": 41.644,
          "x": 167.126,
          "y": 92.672,
          "original_width": 84,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_10_pageHeader_2.jpg",
          "height": 7.454,
          "width": 9.796,
          "x": 134.414,
          "y": 92.997,
          "original_width": 19,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_10_text_7.jpg",
          "height": 10.407,
          "width": 230.151,
          "x": 133.963,
          "y": 226.953,
          "original_width": 464,
          "original_height": 21,
          "type": "layout_text"
        },
        {
          "name": "page_10_text_8.jpg",
          "height": 10.248,
          "width": 229.966,
          "x": 134.084,
          "y": 117.847,
          "original_width": 464,
          "original_height": 20,
          "type": "layout_text"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "where $(k_c, k_c)$ is the center position of $K \\times K$ square.\n\n**Condensation Operation.** Given the estimated correspondence pixel $(h_j, w_j)$ and other offsets in $\\Delta_s^j$, we can obtain the surrounding pixels and combine them with $(h_j, w_j)$ to form $K \\times K$ pixel set. Their coordinates are stored in $n_s^j \\in \\mathbb{R}^{K\\times K\\times2}$. This process can be formulated as:\n\n$$\n\\begin{aligned}\nn_s^j(x, y, :) &= (h_j, w_j) + \\Delta_s^j(x, y, :), \\quad x, y \\in \\{1, \\cdots, K\\}; x, y \\neq k_c, \\\\\nn_s^j(k_c, k_c,:) &= (h_j, w_j),\n\\end{aligned}\n\\tag{14}\n$$\n\nwhere $(k_c, k_c)$ is the center position of $K \\times K$ square.\n\nFinally, we can perform the condensation operation via the index selection on $a_s^j$, formulated as:\n\n$$\n\\tilde{a}_s^j = a_s^j(n_s^j),\n\\tag{15}\n$$\n\nwhere $\\tilde{a}_s^j$ is the condensed representation of $a_s^j$, i.e. pixel associations of $(h_i, w_i)$ to image $I_j$. Furthermore, we also illustrate the condensation for pixel associations of $(h_i, w_i)$ to all images, i.e. $a_s \\in \\mathbb{R}^{N\\times H_s\\times W_s\\times L_s}$, in Figure 2. By applying such a condensation process to all pixel associations, we can obtain the final condensed hyperassociation $\\tilde{A}_s$.",
          "md": "where $(k_c, k_c)$ is the center position of $K \\times K$ square.\n\n**Condensation Operation.** Given the estimated correspondence pixel $(h_j, w_j)$ and other offsets in $\\Delta_s^j$, we can obtain the surrounding pixels and combine them with $(h_j, w_j)$ to form $K \\times K$ pixel set. Their coordinates are stored in $n_s^j \\in \\mathbb{R}^{K\\times K\\times2}$. This process can be formulated as:\n\n$$\n\\begin{aligned}\nn_s^j(x, y, :) &= (h_j, w_j) + \\Delta_s^j(x, y, :), \\quad x, y \\in \\{1, \\cdots, K\\}; x, y \\neq k_c, \\\\\nn_s^j(k_c, k_c,:) &= (h_j, w_j),\n\\end{aligned}\n\\tag{14}\n$$\n\nwhere $(k_c, k_c)$ is the center position of $K \\times K$ square.\n\nFinally, we can perform the condensation operation via the index selection on $a_s^j$, formulated as:\n\n$$\n\\tilde{a}_s^j = a_s^j(n_s^j),\n\\tag{15}\n$$\n\nwhere $\\tilde{a}_s^j$ is the condensed representation of $a_s^j$, i.e. pixel associations of $(h_i, w_i)$ to image $I_j$. Furthermore, we also illustrate the condensation for pixel associations of $(h_i, w_i)$ to all images, i.e. $a_s \\in \\mathbb{R}^{N\\times H_s\\times W_s\\times L_s}$, in Figure 2. By applying such a condensation process to all pixel associations, we can obtain the final condensed hyperassociation $\\tilde{A}_s$.",
          "bBox": {
            "x": 134,
            "y": 116.04,
            "w": 346,
            "h": 540.99
          }
        },
        {
          "type": "heading",
          "lvl": 3,
          "value": "3.3 Object-aware Cycle Consistency Loss",
          "md": "### 3.3 Object-aware Cycle Consistency Loss",
          "bBox": {
            "x": 134,
            "y": 146.03,
            "w": 341,
            "h": 511
          }
        },
        {
          "type": "text",
          "value": "To achieve accurate correspondence estimations, applying effective supervisions on them is necessary. As explicit semantic correspondence annotations are not available, we can only rely on unsupervised losses by imposing correspondence-related constraints on estimated correspondences. Previous unsupervised approaches apply constraints to all pixels [30, 35, 48], including those on the background and extraneous objects that don't have mutual correspondences, hence harming the model effectiveness. To avoid this problem, we propose an object-aware constraint to only access losses on the co-salient pixels.\n\nWe propose an Object-aware Cycle Consistency (OCC) loss for the supervision of correspondence estimations in CoSOD. The cycle consistency can be explained as: if a co-salient pixel $(h_i, w_i)$ of image $I_i$ corresponds to the pixel $(h_j, w_j)$ in image $I_j$, then the pixel $(h_j, w_j)$ should also semantically correspond to pixel $(h_i, w_i)$.\n\nBased on this cycle consistency constraint, we adopt image warping operations to conduct the OCC loss. Specifically, we first warp the image $I_i^s$ (a resized $I_i$ to align the scales in the s-th stage) as $I_{i\\rightarrow j}^s$ using the $I_i^s \\rightarrow I_j^s$ correspondence estimations. Next, we warp $I_{i\\rightarrow j}^s$ backward to $I_{i\\rightarrow j\\rightarrow i}^s$ using $I_j^s \\rightarrow I_i^s$ correspondence estimations. Finally, we can utilize the SSIM loss between $I_i^s$ and $I_{i\\rightarrow j\\rightarrow i}^s$ to measure the cycle consistency for mutual correspondence pixels in $I_i^s$ and $I_j^s$. Moreover, to ensure the constraints are only conducted on co-salient objects, we mask the images with their ground truth masks, formulated as:\n\n$$\n\\mathcal{L}_s^C = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\mathcal{L}_{SSIM}(I_i^s \\cdot G_i^s, I_{i\\rightarrow j\\rightarrow i}^s \\cdot G_i^s),\n\\tag{16}\n$$",
          "md": "To achieve accurate correspondence estimations, applying effective supervisions on them is necessary. As explicit semantic correspondence annotations are not available, we can only rely on unsupervised losses by imposing correspondence-related constraints on estimated correspondences. Previous unsupervised approaches apply constraints to all pixels [30, 35, 48], including those on the background and extraneous objects that don't have mutual correspondences, hence harming the model effectiveness. To avoid this problem, we propose an object-aware constraint to only access losses on the co-salient pixels.\n\nWe propose an Object-aware Cycle Consistency (OCC) loss for the supervision of correspondence estimations in CoSOD. The cycle consistency can be explained as: if a co-salient pixel $(h_i, w_i)$ of image $I_i$ corresponds to the pixel $(h_j, w_j)$ in image $I_j$, then the pixel $(h_j, w_j)$ should also semantically correspond to pixel $(h_i, w_i)$.\n\nBased on this cycle consistency constraint, we adopt image warping operations to conduct the OCC loss. Specifically, we first warp the image $I_i^s$ (a resized $I_i$ to align the scales in the s-th stage) as $I_{i\\rightarrow j}^s$ using the $I_i^s \\rightarrow I_j^s$ correspondence estimations. Next, we warp $I_{i\\rightarrow j}^s$ backward to $I_{i\\rightarrow j\\rightarrow i}^s$ using $I_j^s \\rightarrow I_i^s$ correspondence estimations. Finally, we can utilize the SSIM loss between $I_i^s$ and $I_{i\\rightarrow j\\rightarrow i}^s$ to measure the cycle consistency for mutual correspondence pixels in $I_i^s$ and $I_j^s$. Moreover, to ensure the constraints are only conducted on co-salient objects, we mask the images with their ground truth masks, formulated as:\n\n$$\n\\mathcal{L}_s^C = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\mathcal{L}_{SSIM}(I_i^s \\cdot G_i^s, I_{i\\rightarrow j\\rightarrow i}^s \\cdot G_i^s),\n\\tag{16}\n$$",
          "bBox": {
            "x": 134,
            "y": 146.03,
            "w": 346,
            "h": 521
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.845,
      "layout": [
        {
          "image": "page_10_text_1.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.479,
            "w": 0.566,
            "h": 0.12
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_2.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.353,
            "w": 0.567,
            "h": 0.077
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_3.jpg",
          "confidence": 0.985,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.676,
            "w": 0.567,
            "h": 0.12
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_4.jpg",
          "confidence": 0.981,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.601,
            "w": 0.566,
            "h": 0.075
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_formula_1.jpg",
          "confidence": 0.971,
          "label": "formula",
          "bbox": {
            "x": 0.338,
            "y": 0.802,
            "w": 0.445,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_5.jpg",
          "confidence": 0.965,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.161,
            "w": 0.566,
            "h": 0.061
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_formula_2.jpg",
          "confidence": 0.959,
          "label": "formula",
          "bbox": {
            "x": 0.257,
            "y": 0.233,
            "w": 0.527,
            "h": 0.039
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_sectionHeader_1.jpg",
          "confidence": 0.959,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.456,
            "w": 0.349,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_6.jpg",
          "confidence": 0.955,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.302,
            "w": 0.566,
            "h": 0.028
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_formula_3.jpg",
          "confidence": 0.944,
          "label": "formula",
          "bbox": {
            "x": 0.455,
            "y": 0.329,
            "w": 0.328,
            "h": 0.016
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_pageHeader_1.jpg",
          "confidence": 0.885,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.068,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_pageHeader_2.jpg",
          "confidence": 0.873,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.016,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_7.jpg",
          "confidence": 0.871,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.286,
            "w": 0.376,
            "h": 0.013
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_10_text_8.jpg",
          "confidence": 0.843,
          "label": "text",
          "bbox": {
            "x": 0.219,
            "y": 0.148,
            "w": 0.375,
            "h": 0.012
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 11,
      "text": "                                                                CONDA        11\n\nTable 1: Ablation Study of our proposed modules. SAG, SAC, and FCC are\nablation modules for PAG, CAC, and OCC, respectively.\n\n        ID            Modules                    CoCA [47]\n           SAG  PAG  SAC CAC   FCC OCC   Sm ↑    Eξ ↑    Fβ ↑   M ↓\n        1                               0.6936  0.7642  0.5729  0.1206\n        2   ✓                           0.7236  0.8029  0.6357  0.1106\n        3        ✓                      0.7308  0.8122  0.6459  0.1075\n        4        ✓     ✓                0.7304  0.8123  0.6500  0.1085\n        5        ✓         ✓            0.7473  0.8155  0.6591  0.0956\n        6        ✓         ✓    ✓       0.7398  0.8138  0.6506  0.1013\n        7        ✓         ✓        ✓    0.7570 0.8248  0.6751  0.0924\n\nwhere Gs is the resized ground truth of image Ii. The total OCC loss LC is the\n        i                           C     C     C    C\nsum of three stages, formulated as: L    = L3 + L4 + L5 . More details can be\nfound in the supplementary materials.\n\n4     Experiments\n\n4.1   Evaluation Datasets and Metrics\nWe follow [8] to evaluate our model on three benchmark datasets, i.e. CoCA [47]\n(1295 images of 80 groups), CoSal2015 [44] (2015 images of 50 groups), and\nCoSOD3k [7] (3316 images of 160 groups). We adopt four widely-used met-\nrics for the quantitative evaluation,  i.e. Structure-measure (Sm) [5], Maximum\nenhanced-alignment measure (Eξ) [6], Maximum F-measure (Fβ) [1], and Mean\nAbsolute Error (M) [3].\n\n4.2   Implementation Details\nTo construct the training data, we follow [51] to use different combinations of\nthree commonly used training datasets,     i.e. DUTS class [47] (8250 images of\n291 groups), COCO-9k [18] (9213 images of 65 groups), and COCO-SEG [37]\n(200,000 images of 78 groups), for a fair comparison with other state-of-the-art\n(SOTA) works. We also implement the synthesis strategy for the DUTS class\ndataset following [46].\n   For training specifics, we employ the data augmentation strategy in [21] and\nuse 256 × 256 as the input size for the network. We employ the Adam optimizer\n[15] with β1 = 0.9 and β2 = 0.99 to optimize the network. We train our CONDA\nmodel for 300 epochs, starting with an initial learning rate of 1e − 4, which is\ndivided by 10 at the 60, 000th iteration. Our experiments are implemented based\non PyTorch [29] on a single Tesla A40 GPU, with the batchsize set to N     = 6.\nThe hyperparameter K   in CAC is set to 9.\n\n4.3   Ablation Study\nWe conduct ablation studies on the most challenging CoCA [47] dataset. To\nconstruct the baseline, we use the FPN [17] (with VGG-16 as encoder) as the",
      "md": "CONDA        11\n\nTable 1: Ablation Study of our proposed modules. SAG, SAC, and FCC are ablation modules for PAG, CAC, and OCC, respectively.\n\n| ID | Modules<br/>SAG | Modules<br/>PAG | Modules<br/>SAC | Modules<br/>CAC | Modules<br/>FCC | Modules<br/>OCC | CoCA \\[47]<br/>Sm ↑ | CoCA \\[47]<br/>Eξ ↑ | CoCA \\[47]<br/>Fβ ↑ | CoCA \\[47]<br/>M ↓ |\n| -- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- | ------------------- | ------------------- | ------------------- | ------------------ |\n| 1  |                 |                 |                 |                 |                 |                 | 0.6936              | 0.7642              | 0.5729              | 0.1206             |\n| 2  | ✓               |                 |                 |                 |                 |                 | 0.7236              | 0.8029              | 0.6357              | 0.1106             |\n| 3  |                 | ✓               |                 |                 |                 |                 | 0.7308              | 0.8122              | 0.6459              | 0.1075             |\n| 4  |                 | ✓               | ✓               |                 |                 |                 | 0.7304              | 0.8123              | 0.6500              | 0.1085             |\n| 5  |                 | ✓               |                 | ✓               |                 |                 | 0.7473              | 0.8155              | 0.6591              | 0.0956             |\n| 6  |                 | ✓               |                 | ✓               | ✓               |                 | 0.7398              | 0.8138              | 0.6506              | 0.1013             |\n| 7  |                 | ✓               |                 | ✓               |                 | ✓               | 0.7570              | 0.8248              | 0.6751              | 0.0924             |\n\nwhere $G_i^s$ is the resized ground truth of image $I_i$. The total OCC loss $L^C$ is the sum of three stages, formulated as: $L^C = L_3^C + L_4^C + L_5^C$. More details can be found in the supplementary materials.\n\n## 4 Experiments\n\n### 4.1 Evaluation Datasets and Metrics\n\nWe follow [8] to evaluate our model on three benchmark datasets, i.e. CoCA [47] (1295 images of 80 groups), CoSal2015 [44] (2015 images of 50 groups), and CoSOD3k [7] (3316 images of 160 groups). We adopt four widely-used metrics for the quantitative evaluation, i.e. Structure-measure ($S_m$) [5], Maximum enhanced-alignment measure ($E_ξ$) [6], Maximum F-measure ($F_β$) [1], and Mean Absolute Error ($M$) [3].\n\n### 4.2 Implementation Details\n\nTo construct the training data, we follow [51] to use different combinations of three commonly used training datasets, i.e. DUTS class [47] (8250 images of 291 groups), COCO-9k [18] (9213 images of 65 groups), and COCO-SEG [37] (200,000 images of 78 groups), for a fair comparison with other state-of-the-art (SOTA) works. We also implement the synthesis strategy for the DUTS class dataset following [46].\n\nFor training specifics, we employ the data augmentation strategy in [21] and use 256 × 256 as the input size for the network. We employ the Adam optimizer [15] with $β_1 = 0.9$ and $β_2 = 0.99$ to optimize the network. We train our CONDA model for 300 epochs, starting with an initial learning rate of $1e − 4$, which is divided by 10 at the 60,000th iteration. Our experiments are implemented based on PyTorch [29] on a single Tesla A40 GPU, with the batchsize set to $N = 6$. The hyperparameter $K$ in CAC is set to 9.\n\n### 4.3 Ablation Study\n\nWe conduct ablation studies on the most challenging CoCA [47] dataset. To construct the baseline, we use the FPN [17] (with VGG-16 as encoder) as the",
      "images": [
        {
          "name": "page_11.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_11_table_1.jpg",
          "height": 96.026,
          "width": 273.913,
          "x": 167.927,
          "y": 140.061,
          "original_width": 553,
          "original_height": 193,
          "type": "layout_table"
        },
        {
          "name": "page_11_text_1.jpg",
          "height": 70.069,
          "width": 347.298,
          "x": 133.766,
          "y": 451.964,
          "original_width": 701,
          "original_height": 141,
          "type": "layout_text"
        },
        {
          "name": "page_11_text_2.jpg",
          "height": 70.865,
          "width": 347.087,
          "x": 133.892,
          "y": 344.462,
          "original_width": 700,
          "original_height": 143,
          "type": "layout_text"
        },
        {
          "name": "page_11_text_3.jpg",
          "height": 81.719,
          "width": 347.156,
          "x": 133.783,
          "y": 524.053,
          "original_width": 701,
          "original_height": 165,
          "type": "layout_text"
        },
        {
          "name": "page_11_text_4.jpg",
          "height": 35.577,
          "width": 347.321,
          "x": 133.672,
          "y": 247.287,
          "original_width": 701,
          "original_height": 71,
          "type": "layout_text"
        },
        {
          "name": "page_11_text_5.jpg",
          "height": 22.371,
          "width": 346.912,
          "x": 133.861,
          "y": 642.96,
          "original_width": 700,
          "original_height": 45,
          "type": "layout_text"
        },
        {
          "name": "page_11_sectionHeader_1.jpg",
          "height": 10.217,
          "width": 144.809,
          "x": 134.192,
          "y": 433.174,
          "original_width": 292,
          "original_height": 20,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_11_sectionHeader_2.jpg",
          "height": 11.382,
          "width": 94.683,
          "x": 134.107,
          "y": 301.727,
          "original_width": 191,
          "original_height": 22,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_11_caption_1.jpg",
          "height": 20.533,
          "width": 347.374,
          "x": 133.818,
          "y": 116.251,
          "original_width": 701,
          "original_height": 41,
          "type": "layout_caption"
        },
        {
          "name": "page_11_sectionHeader_3.jpg",
          "height": 10.436,
          "width": 103.508,
          "x": 134.155,
          "y": 624.461,
          "original_width": 209,
          "original_height": 21,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_11_sectionHeader_4.jpg",
          "height": 8.418,
          "width": 191.156,
          "x": 134.111,
          "y": 326.143,
          "original_width": 386,
          "original_height": 17,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_11_pageHeader_1.jpg",
          "height": 7.717,
          "width": 34.676,
          "x": 412.949,
          "y": 92.719,
          "original_width": 70,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_11_pageHeader_2.jpg",
          "height": 7.269,
          "width": 9.274,
          "x": 470.87,
          "y": 93.066,
          "original_width": 18,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "CONDA        11\n\nTable 1: Ablation Study of our proposed modules. SAG, SAC, and FCC are ablation modules for PAG, CAC, and OCC, respectively.",
          "md": "CONDA        11\n\nTable 1: Ablation Study of our proposed modules. SAG, SAC, and FCC are ablation modules for PAG, CAC, and OCC, respectively.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 542
          }
        },
        {
          "type": "table",
          "rows": [
            [],
            [],
            [],
            [
              "ID",
              "Modules<br/>SAG",
              "Modules<br/>PAG",
              "Modules<br/>SAC",
              "Modules<br/>CAC",
              "Modules<br/>FCC",
              "Modules<br/>OCC",
              "CoCA \\[47]<br/>Sm ↑",
              "CoCA \\[47]<br/>Eξ ↑",
              "CoCA \\[47]<br/>Fβ ↑",
              "CoCA \\[47]<br/>M ↓"
            ],
            [
              "--",
              "---------------",
              "---------------",
              "---------------",
              "---------------",
              "---------------",
              "---------------",
              "-------------------",
              "-------------------",
              "-------------------",
              "------------------"
            ],
            [
              "1",
              "",
              "",
              "",
              "",
              "",
              "",
              "0.6936",
              "0.7642",
              "0.5729",
              "0.1206"
            ],
            [
              "2",
              "✓",
              "",
              "",
              "",
              "",
              "",
              "0.7236",
              "0.8029",
              "0.6357",
              "0.1106"
            ],
            [
              "3",
              "",
              "✓",
              "",
              "",
              "",
              "",
              "0.7308",
              "0.8122",
              "0.6459",
              "0.1075"
            ],
            [
              "4",
              "",
              "✓",
              "✓",
              "",
              "",
              "",
              "0.7304",
              "0.8123",
              "0.6500",
              "0.1085"
            ],
            [
              "5",
              "",
              "✓",
              "",
              "✓",
              "",
              "",
              "0.7473",
              "0.8155",
              "0.6591",
              "0.0956"
            ],
            [
              "6",
              "",
              "✓",
              "",
              "✓",
              "✓",
              "",
              "0.7398",
              "0.8138",
              "0.6506",
              "0.1013"
            ],
            [
              "7",
              "",
              "✓",
              "",
              "✓",
              "",
              "✓",
              "0.7570",
              "0.8248",
              "0.6751",
              "0.0924"
            ]
          ],
          "md": "CONDA        11\nTable 1: Ablation Study of our proposed modules. SAG, SAC, and FCC are ablation modules for PAG, CAC, and OCC, respectively.\n\n| ID | Modules<br/>SAG | Modules<br/>PAG | Modules<br/>SAC | Modules<br/>CAC | Modules<br/>FCC | Modules<br/>OCC | CoCA \\[47]<br/>Sm ↑ | CoCA \\[47]<br/>Eξ ↑ | CoCA \\[47]<br/>Fβ ↑ | CoCA \\[47]<br/>M ↓ |\n| -- | --------------- | --------------- | --------------- | --------------- | --------------- | --------------- | ------------------- | ------------------- | ------------------- | ------------------ |\n| 1  |                 |                 |                 |                 |                 |                 | 0.6936              | 0.7642              | 0.5729              | 0.1206             |\n| 2  | ✓               |                 |                 |                 |                 |                 | 0.7236              | 0.8029              | 0.6357              | 0.1106             |\n| 3  |                 | ✓               |                 |                 |                 |                 | 0.7308              | 0.8122              | 0.6459              | 0.1075             |\n| 4  |                 | ✓               | ✓               |                 |                 |                 | 0.7304              | 0.8123              | 0.6500              | 0.1085             |\n| 5  |                 | ✓               |                 | ✓               |                 |                 | 0.7473              | 0.8155              | 0.6591              | 0.0956             |\n| 6  |                 | ✓               |                 | ✓               | ✓               |                 | 0.7398              | 0.8138              | 0.6506              | 0.1013             |\n| 7  |                 | ✓               |                 | ✓               |                 | ✓               | 0.7570              | 0.8248              | 0.6751              | 0.0924             |",
          "isPerfectTable": false,
          "csv": "\n\n\n\"ID\",\"Modules<br/>SAG\",\"Modules<br/>PAG\",\"Modules<br/>SAC\",\"Modules<br/>CAC\",\"Modules<br/>FCC\",\"Modules<br/>OCC\",\"CoCA \\[47]<br/>Sm ↑\",\"CoCA \\[47]<br/>Eξ ↑\",\"CoCA \\[47]<br/>Fβ ↑\",\"CoCA \\[47]<br/>M ↓\"\n\"--\",\"---------------\",\"---------------\",\"---------------\",\"---------------\",\"---------------\",\"---------------\",\"-------------------\",\"-------------------\",\"-------------------\",\"------------------\"\n\"1\",\"\",\"\",\"\",\"\",\"\",\"\",\"0.6936\",\"0.7642\",\"0.5729\",\"0.1206\"\n\"2\",\"✓\",\"\",\"\",\"\",\"\",\"\",\"0.7236\",\"0.8029\",\"0.6357\",\"0.1106\"\n\"3\",\"\",\"✓\",\"\",\"\",\"\",\"\",\"0.7308\",\"0.8122\",\"0.6459\",\"0.1075\"\n\"4\",\"\",\"✓\",\"✓\",\"\",\"\",\"\",\"0.7304\",\"0.8123\",\"0.6500\",\"0.1085\"\n\"5\",\"\",\"✓\",\"\",\"✓\",\"\",\"\",\"0.7473\",\"0.8155\",\"0.6591\",\"0.0956\"\n\"6\",\"\",\"✓\",\"\",\"✓\",\"✓\",\"\",\"0.7398\",\"0.8138\",\"0.6506\",\"0.1013\"\n\"7\",\"\",\"✓\",\"\",\"✓\",\"\",\"✓\",\"0.7570\",\"0.8248\",\"0.6751\",\"0.0924\"",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 347,
            "h": 572
          }
        },
        {
          "type": "text",
          "value": "where $G_i^s$ is the resized ground truth of image $I_i$. The total OCC loss $L^C$ is the sum of three stages, formulated as: $L^C = L_3^C + L_4^C + L_5^C$. More details can be found in the supplementary materials.",
          "md": "where $G_i^s$ is the resized ground truth of image $I_i$. The total OCC loss $L^C$ is the sum of three stages, formulated as: $L^C = L_3^C + L_4^C + L_5^C$. More details can be found in the supplementary materials.",
          "bBox": {
            "x": 134,
            "y": 154.03,
            "w": 346,
            "h": 157.02
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "4 Experiments",
          "md": "## 4 Experiments",
          "bBox": {
            "x": 134,
            "y": 198.03,
            "w": 94,
            "h": 113.02
          }
        },
        {
          "type": "heading",
          "lvl": 3,
          "value": "4.1 Evaluation Datasets and Metrics",
          "md": "### 4.1 Evaluation Datasets and Metrics",
          "bBox": {
            "x": 134,
            "y": 167.03,
            "w": 252,
            "h": 167.01
          }
        },
        {
          "type": "text",
          "value": "We follow [8] to evaluate our model on three benchmark datasets, i.e. CoCA [47] (1295 images of 80 groups), CoSal2015 [44] (2015 images of 50 groups), and CoSOD3k [7] (3316 images of 160 groups). We adopt four widely-used metrics for the quantitative evaluation, i.e. Structure-measure ($S_m$) [5], Maximum enhanced-alignment measure ($E_ξ$) [6], Maximum F-measure ($F_β$) [1], and Mean Absolute Error ($M$) [3].",
          "md": "We follow [8] to evaluate our model on three benchmark datasets, i.e. CoCA [47] (1295 images of 80 groups), CoSal2015 [44] (2015 images of 50 groups), and CoSOD3k [7] (3316 images of 160 groups). We adopt four widely-used metrics for the quantitative evaluation, i.e. Structure-measure ($S_m$) [5], Maximum enhanced-alignment measure ($E_ξ$) [6], Maximum F-measure ($F_β$) [1], and Mean Absolute Error ($M$) [3].",
          "bBox": {
            "x": 134,
            "y": 143.03,
            "w": 346,
            "h": 246.01
          }
        },
        {
          "type": "heading",
          "lvl": 3,
          "value": "4.2 Implementation Details",
          "md": "### 4.2 Implementation Details",
          "bBox": {
            "x": 134,
            "y": 179.03,
            "w": 144,
            "h": 263.01
          }
        },
        {
          "type": "text",
          "value": "To construct the training data, we follow [51] to use different combinations of three commonly used training datasets, i.e. DUTS class [47] (8250 images of 291 groups), COCO-9k [18] (9213 images of 65 groups), and COCO-SEG [37] (200,000 images of 78 groups), for a fair comparison with other state-of-the-art (SOTA) works. We also implement the synthesis strategy for the DUTS class dataset following [46].\n\nFor training specifics, we employ the data augmentation strategy in [21] and use 256 × 256 as the input size for the network. We employ the Adam optimizer [15] with $β_1 = 0.9$ and $β_2 = 0.99$ to optimize the network. We train our CONDA model for 300 epochs, starting with an initial learning rate of $1e − 4$, which is divided by 10 at the 60,000th iteration. Our experiments are implemented based on PyTorch [29] on a single Tesla A40 GPU, with the batchsize set to $N = 6$. The hyperparameter $K$ in CAC is set to 9.",
          "md": "To construct the training data, we follow [51] to use different combinations of three commonly used training datasets, i.e. DUTS class [47] (8250 images of 291 groups), COCO-9k [18] (9213 images of 65 groups), and COCO-SEG [37] (200,000 images of 78 groups), for a fair comparison with other state-of-the-art (SOTA) works. We also implement the synthesis strategy for the DUTS class dataset following [46].\n\nFor training specifics, we employ the data augmentation strategy in [21] and use 256 × 256 as the input size for the network. We employ the Adam optimizer [15] with $β_1 = 0.9$ and $β_2 = 0.99$ to optimize the network. We train our CONDA model for 300 epochs, starting with an initial learning rate of $1e − 4$, which is divided by 10 at the 60,000th iteration. Our experiments are implemented based on PyTorch [29] on a single Tesla A40 GPU, with the batchsize set to $N = 6$. The hyperparameter $K$ in CAC is set to 9.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 347,
            "h": 513
          }
        },
        {
          "type": "heading",
          "lvl": 3,
          "value": "4.3 Ablation Study",
          "md": "### 4.3 Ablation Study",
          "bBox": {
            "x": 134,
            "y": 186.03,
            "w": 103,
            "h": 447.01
          }
        },
        {
          "type": "text",
          "value": "We conduct ablation studies on the most challenging CoCA [47] dataset. To construct the baseline, we use the FPN [17] (with VGG-16 as encoder) as the",
          "md": "We conduct ablation studies on the most challenging CoCA [47] dataset. To construct the baseline, we use the FPN [17] (with VGG-16 as encoder) as the",
          "bBox": {
            "x": 134,
            "y": 143.03,
            "w": 346,
            "h": 520.01
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.651,
      "layout": [
        {
          "image": "page_11_table_1.jpg",
          "confidence": 0.988,
          "label": "table",
          "bbox": {
            "x": 0.274,
            "y": 0.176,
            "w": 0.447,
            "h": 0.121
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_text_1.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.568,
            "w": 0.567,
            "h": 0.09
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_text_2.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.433,
            "w": 0.567,
            "h": 0.091
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_text_3.jpg",
          "confidence": 0.987,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.659,
            "w": 0.567,
            "h": 0.105
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_text_4.jpg",
          "confidence": 0.981,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.31,
            "w": 0.567,
            "h": 0.046
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_text_5.jpg",
          "confidence": 0.972,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.809,
            "w": 0.566,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_sectionHeader_1.jpg",
          "confidence": 0.96,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.546,
            "w": 0.236,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_sectionHeader_2.jpg",
          "confidence": 0.957,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.38,
            "w": 0.154,
            "h": 0.014
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_caption_1.jpg",
          "confidence": 0.953,
          "label": "caption",
          "bbox": {
            "x": 0.218,
            "y": 0.145,
            "w": 0.567,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_sectionHeader_3.jpg",
          "confidence": 0.952,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.788,
            "w": 0.169,
            "h": 0.013
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_sectionHeader_4.jpg",
          "confidence": 0.94,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.411,
            "w": 0.312,
            "h": 0.01
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_pageHeader_1.jpg",
          "confidence": 0.923,
          "label": "pageHeader",
          "bbox": {
            "x": 0.674,
            "y": 0.117,
            "w": 0.056,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_11_pageHeader_2.jpg",
          "confidence": 0.887,
          "label": "pageHeader",
          "bbox": {
            "x": 0.769,
            "y": 0.117,
            "w": 0.015,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 12,
      "text": "12     L Li et al.\n\n    8  5234\n\ncanon                                                             Canon\n      508\n\n                                                                        4qo\n\n                                     9134~\n      Image  GT  Prediction Correspondence I Correspondence II Correspondence III\nFig. 4: Visual samples for the correspondence estimations. Correspondences\nI, II, and III visually display estimated correspondences between the main image and\nthree related images. Sparse co-salient pixels were selected and connected to their\ncorresponding pixels using colored lines for clear visualization.\n\nfoundational segmentation network and enhance it with the Region-to-Region\ncorrelation module (R2R) [16] to simply capture inter-image connections. Then,\nas shown in Table 1, we incrementally incorporate PAG, CAC, and OCC into\nthe baseline for effectiveness analysis. We trained all ablation models with the\nDUTS class and COCO9k datasets.\nEffectiveness of PAG. PAG first uses intermediate image features from the\nFPN encoder to calculate full-pixel hyperassociations. Then, aggregation net-\nworks are applied to generate deep association features for the decoder pro-\ncess. PAG effectively utilizes deep learning to model pair-wise pixel associations,\nachieving higher-level association knowledge compared to previous image fea-\nture optimization strategies. As shown in the 3rd line of Table 1, PAG shows\nsignificant performance boosts compared to the baseline model, with respective\ngains of 3.72%, 4.80%, 7.30%, and 1.31% in Sm, Eξ, Fβ, and M.\n    Furthermore, to validate our approach of progressively enhancing image fea-\ntures with the previously generated association feature for improving hyperasso-\nciation calculation, we conduct an ablation experiment called Separate Associa-\ntion Generation (SAG) where association features are generated for three stages\nwithout image feature enhancements. As shown in the 2nd&3rd lines in Table 1,\nPAG outperforms SAG, indicating that our progressive enhancement design can\nobtain better hyperassociation.\nEffectiveness of CAC. CAC aims to condense full-pixel hyperassociations in\nPAG by selecting corresponding pixels and their surrounding contexts. Results in\nthe 3rd&5th lines of Table 1 show that introducing the CAC module improves the\nperformance. Moreover, it reduces the multiply-accumulate operations (MACs)\nof aggregation networks from 91.38G in the full-pixel PAG to 77.19G1. This indi-\ncates that utilizing correspondence estimation to condense the hyperassociations\nnot only effectively reduces the computational burden but also helps obtain more\naccurate pixel associations.\n      We also analyze CAC in detail. As deep association features are necessary\nfor reliable correspondence estimation, CAC first pre-condenses the hyperasso-\n\n  We input a group of 6 related 256x256 images to measure MACs.",
      "md": "12     L Li et al.\n\n| Image                                  | GT                  | Prediction                    | Correspondence I               | Correspondence II               | Correspondence III               |\n| -------------------------------------- | ------------------- | ----------------------------- | ------------------------------ | ------------------------------- | -------------------------------- |\n| !Colorful objects        | !GT mask | !Prediction mask | !Correspondence I | !Correspondence II | !Correspondence III |\n| !Camera                  | !GT mask | !Prediction mask | !Correspondence I | !Correspondence II | !Correspondence III |\n| !Still life with flowers | !GT mask | !Prediction mask | !Correspondence I | !Correspondence II | !Correspondence III |\n\nFig. 4: Visual samples for the correspondence estimations. Correspondences I, II, and III visually display estimated correspondences between the main image and three related images. Sparse co-salient pixels were selected and connected to their corresponding pixels using colored lines for clear visualization.\n\nfoundational segmentation network and enhance it with the Region-to-Region correlation module (R2R) [16] to simply capture inter-image connections. Then, as shown in Table 1, we incrementally incorporate PAG, CAC, and OCC into the baseline for effectiveness analysis. We trained all ablation models with the DUTS class and COCO9k datasets.\n\nEffectiveness of PAG. PAG first uses intermediate image features from the FPN encoder to calculate full-pixel hyperassociations. Then, aggregation networks are applied to generate deep association features for the decoder process. PAG effectively utilizes deep learning to model pair-wise pixel associations, achieving higher-level association knowledge compared to previous image feature optimization strategies. As shown in the 3rd line of Table 1, PAG shows significant performance boosts compared to the baseline model, with respective gains of 3.72%, 4.80%, 7.30%, and 1.31% in $S_m$, $E_ξ$, $F_β$, and $M$.\n\nFurthermore, to validate our approach of progressively enhancing image features with the previously generated association feature for improving hyperassociation calculation, we conduct an ablation experiment called Separate Association Generation (SAG) where association features are generated for three stages without image feature enhancements. As shown in the 2nd&3rd lines in Table 1, PAG outperforms SAG, indicating that our progressive enhancement design can obtain better hyperassociation.\n\nEffectiveness of CAC. CAC aims to condense full-pixel hyperassociations in PAG by selecting corresponding pixels and their surrounding contexts. Results in the 3rd&5th lines of Table 1 show that introducing the CAC module improves the performance. Moreover, it reduces the multiply-accumulate operations (MACs) of aggregation networks from 91.38G in the full-pixel PAG to 77.19G¹. This indicates that utilizing correspondence estimation to condense the hyperassociations not only effectively reduces the computational burden but also helps obtain more accurate pixel associations.\n\nWe also analyze CAC in detail. As deep association features are necessary for reliable correspondence estimation, CAC first pre-condenses the hyperasso-\n\n¹ We input a group of 6 related 256x256 images to measure MACs.",
      "images": [
        {
          "name": "img_p11_1.png",
          "height": 425,
          "width": 640,
          "x": 135.562,
          "y": 115.83573999999994,
          "original_width": 640,
          "original_height": 425,
          "ocr": [
            {
              "x": 483,
              "y": 217,
              "w": 20,
              "h": 26,
              "confidence": 0.9719591734210553,
              "text": "2"
            },
            {
              "x": 490,
              "y": 248,
              "w": 24,
              "h": 30,
              "confidence": 0.9575077002555642,
              "text": "3"
            },
            {
              "x": 367,
              "y": 281,
              "w": 20,
              "h": 28,
              "confidence": 0.9999916553671397,
              "text": "8"
            },
            {
              "x": 478,
              "y": 282,
              "w": 26,
              "h": 30,
              "confidence": 0.9999992847443906,
              "text": "4"
            },
            {
              "x": 452,
              "y": 302,
              "w": 26,
              "h": 32,
              "confidence": 0.9999947547981378,
              "text": "5"
            }
          ]
        },
        {
          "name": "img_p11_2.png",
          "height": 425,
          "width": 640,
          "x": 173.502,
          "y": 115.83524999999995,
          "original_width": 640,
          "original_height": 425
        },
        {
          "name": "img_p11_3.png",
          "height": 425,
          "width": 640,
          "x": 211.43400000000003,
          "y": 115.83524999999995,
          "original_width": 640,
          "original_height": 425
        },
        {
          "name": "img_p11_4.png",
          "height": 512,
          "width": 1024,
          "x": 249.36600000000004,
          "y": 115.83663999999993,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "img_p11_5.png",
          "height": 512,
          "width": 1024,
          "x": 327.038,
          "y": 115.83663999999993,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "img_p11_6.png",
          "height": 512,
          "width": 1024,
          "x": 404.71000000000004,
          "y": 115.83663999999993,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "img_p11_7.png",
          "height": 480,
          "width": 640,
          "x": 135.56600000000003,
          "y": 148.21859999999992,
          "original_width": 640,
          "original_height": 480,
          "ocr": [
            {
              "x": 436,
              "y": 134,
              "w": 26,
              "h": 24,
              "confidence": 0.16465747875438735,
              "text": "508"
            },
            {
              "x": 344.30537128835283,
              "y": 94.12450247855259,
              "w": 40.38925742329434,
              "h": 30.75099504289483,
              "confidence": 0.8163806856495728,
              "text": "canon"
            }
          ]
        },
        {
          "name": "img_p11_8.png",
          "height": 480,
          "width": 640,
          "x": 173.50200000000004,
          "y": 148.21859999999992,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p11_9.png",
          "height": 480,
          "width": 640,
          "x": 211.43400000000003,
          "y": 148.21859999999992,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p11_10.png",
          "height": 512,
          "width": 1024,
          "x": 249.36600000000004,
          "y": 148.21563999999992,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "img_p11_11.png",
          "height": 512,
          "width": 1024,
          "x": 327.038,
          "y": 148.21563999999992,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "img_p11_12.png",
          "height": 512,
          "width": 1024,
          "x": 404.71000000000004,
          "y": 148.21563999999992,
          "original_width": 1024,
          "original_height": 512,
          "ocr": [
            {
              "x": 614,
              "y": 354,
              "w": 42,
              "h": 26,
              "confidence": 0.266215678290154,
              "text": "4qo"
            },
            {
              "x": 276.30537128835283,
              "y": 101.12450247855259,
              "w": 32.38925742329434,
              "h": 30.75099504289483,
              "confidence": 0.5764773577656532,
              "text": "Canon"
            }
          ]
        },
        {
          "name": "img_p11_13.png",
          "height": 426,
          "width": 640,
          "x": 135.562,
          "y": 180.59412159999994,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p11_14.png",
          "height": 426,
          "width": 640,
          "x": 173.502,
          "y": 180.59343999999993,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p11_15.png",
          "height": 426,
          "width": 640,
          "x": 211.43400000000003,
          "y": 180.59343999999993,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p11_16.png",
          "height": 512,
          "width": 1024,
          "x": 249.36600000000004,
          "y": 180.59463999999994,
          "original_width": 1024,
          "original_height": 512,
          "ocr": [
            {
              "x": 682.1535010280254,
              "y": 392.1219034952862,
              "w": 67.6929979439492,
              "h": 39.75619300942765,
              "confidence": 0.1357435399349065,
              "text": "9134~"
            }
          ]
        },
        {
          "name": "img_p11_17.png",
          "height": 512,
          "width": 1024,
          "x": 327.038,
          "y": 180.59463999999994,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "img_p11_18.png",
          "height": 512,
          "width": 1024,
          "x": 406.052,
          "y": 180.59463999999994,
          "original_width": 1024,
          "original_height": 512
        },
        {
          "name": "page_12.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_12_text_1.jpg",
          "height": 93.586,
          "width": 347.166,
          "x": 133.754,
          "y": 524.115,
          "original_width": 701,
          "original_height": 189,
          "type": "layout_text"
        },
        {
          "name": "page_12_text_2.jpg",
          "height": 94.118,
          "width": 346.895,
          "x": 133.774,
          "y": 342.826,
          "original_width": 700,
          "original_height": 190,
          "type": "layout_text"
        },
        {
          "name": "page_12_text_3.jpg",
          "height": 81.608,
          "width": 346.932,
          "x": 133.839,
          "y": 439.41,
          "original_width": 700,
          "original_height": 164,
          "type": "layout_text"
        },
        {
          "name": "page_12_text_4.jpg",
          "height": 57.477,
          "width": 347.092,
          "x": 133.708,
          "y": 282.191,
          "original_width": 700,
          "original_height": 116,
          "type": "layout_text"
        },
        {
          "name": "page_12_text_5.jpg",
          "height": 22.37,
          "width": 346.34,
          "x": 134.238,
          "y": 620.78,
          "original_width": 699,
          "original_height": 45,
          "type": "layout_text"
        },
        {
          "name": "page_12_caption_1.jpg",
          "height": 42.048,
          "width": 347.186,
          "x": 133.969,
          "y": 226.218,
          "original_width": 701,
          "original_height": 84,
          "type": "layout_caption"
        },
        {
          "name": "page_12_picture_1.jpg",
          "height": 104.311,
          "width": 347.22,
          "x": 135.09,
          "y": 114.803,
          "original_width": 701,
          "original_height": 210,
          "type": "layout_picture"
        },
        {
          "name": "page_12_pageHeader_1.jpg",
          "height": 6.79,
          "width": 9.639,
          "x": 134.525,
          "y": 93.178,
          "original_width": 19,
          "original_height": 13,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_12_pageHeader_2.jpg",
          "height": 7.531,
          "width": 41.586,
          "x": 167.32,
          "y": 92.796,
          "original_width": 83,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_12_footnote_1.jpg",
          "height": 9.225,
          "width": 264.341,
          "x": 144.273,
          "y": 655.738,
          "original_width": 533,
          "original_height": 18,
          "type": "layout_footnote"
        },
        {
          "name": "page_12_caption_2.jpg",
          "height": 6.765,
          "width": 67.082,
          "x": 410.181,
          "y": 213.511,
          "original_width": 135,
          "original_height": 13,
          "type": "layout_caption"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "12     L Li et al.",
          "md": "12     L Li et al.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 74,
            "h": 9
          }
        },
        {
          "type": "table",
          "rows": [
            [],
            [
              "Image",
              "GT",
              "Prediction",
              "Correspondence I",
              "Correspondence II",
              "Correspondence III"
            ],
            [
              "--------------------------------------",
              "-------------------",
              "-----------------------------",
              "------------------------------",
              "-------------------------------",
              "--------------------------------"
            ],
            [
              "!Colorful objects",
              "!GT mask",
              "!Prediction mask",
              "!Correspondence I",
              "!Correspondence II",
              "!Correspondence III"
            ],
            [
              "!Camera",
              "!GT mask",
              "!Prediction mask",
              "!Correspondence I",
              "!Correspondence II",
              "!Correspondence III"
            ],
            [
              "!Still life with flowers",
              "!GT mask",
              "!Prediction mask",
              "!Correspondence I",
              "!Correspondence II",
              "!Correspondence III"
            ]
          ],
          "md": "12     L Li et al.\n| Image                                  | GT                  | Prediction                    | Correspondence I               | Correspondence II               | Correspondence III               |\n| -------------------------------------- | ------------------- | ----------------------------- | ------------------------------ | ------------------------------- | -------------------------------- |\n| !Colorful objects        | !GT mask | !Prediction mask | !Correspondence I | !Correspondence II | !Correspondence III |\n| !Camera                  | !GT mask | !Prediction mask | !Correspondence I | !Correspondence II | !Correspondence III |\n| !Still life with flowers | !GT mask | !Prediction mask | !Correspondence I | !Correspondence II | !Correspondence III |",
          "isPerfectTable": false,
          "csv": "\n\"Image\",\"GT\",\"Prediction\",\"Correspondence I\",\"Correspondence II\",\"Correspondence III\"\n\"--------------------------------------\",\"-------------------\",\"-----------------------------\",\"------------------------------\",\"-------------------------------\",\"--------------------------------\"\n\"!Colorful objects\",\"!GT mask\",\"!Prediction mask\",\"!Correspondence I\",\"!Correspondence II\",\"!Correspondence III\"\n\"!Camera\",\"!GT mask\",\"!Prediction mask\",\"!Correspondence I\",\"!Correspondence II\",\"!Correspondence III\"\n\"!Still life with flowers\",\"!GT mask\",\"!Prediction mask\",\"!Correspondence I\",\"!Correspondence II\",\"!Correspondence III\"",
          "bBox": {
            "x": 134,
            "y": 212.03,
            "w": 346,
            "h": 451.01
          }
        },
        {
          "type": "text",
          "value": "Fig. 4: Visual samples for the correspondence estimations. Correspondences I, II, and III visually display estimated correspondences between the main image and three related images. Sparse co-salient pixels were selected and connected to their corresponding pixels using colored lines for clear visualization.\n\nfoundational segmentation network and enhance it with the Region-to-Region correlation module (R2R) [16] to simply capture inter-image connections. Then, as shown in Table 1, we incrementally incorporate PAG, CAC, and OCC into the baseline for effectiveness analysis. We trained all ablation models with the DUTS class and COCO9k datasets.\n\nEffectiveness of PAG. PAG first uses intermediate image features from the FPN encoder to calculate full-pixel hyperassociations. Then, aggregation networks are applied to generate deep association features for the decoder process. PAG effectively utilizes deep learning to model pair-wise pixel associations, achieving higher-level association knowledge compared to previous image feature optimization strategies. As shown in the 3rd line of Table 1, PAG shows significant performance boosts compared to the baseline model, with respective gains of 3.72%, 4.80%, 7.30%, and 1.31% in $S_m$, $E_ξ$, $F_β$, and $M$.\n\nFurthermore, to validate our approach of progressively enhancing image features with the previously generated association feature for improving hyperassociation calculation, we conduct an ablation experiment called Separate Association Generation (SAG) where association features are generated for three stages without image feature enhancements. As shown in the 2nd&3rd lines in Table 1, PAG outperforms SAG, indicating that our progressive enhancement design can obtain better hyperassociation.\n\nEffectiveness of CAC. CAC aims to condense full-pixel hyperassociations in PAG by selecting corresponding pixels and their surrounding contexts. Results in the 3rd&5th lines of Table 1 show that introducing the CAC module improves the performance. Moreover, it reduces the multiply-accumulate operations (MACs) of aggregation networks from 91.38G in the full-pixel PAG to 77.19G¹. This indicates that utilizing correspondence estimation to condense the hyperassociations not only effectively reduces the computational burden but also helps obtain more accurate pixel associations.\n\nWe also analyze CAC in detail. As deep association features are necessary for reliable correspondence estimation, CAC first pre-condenses the hyperasso-\n\n¹ We input a group of 6 related 256x256 images to measure MACs.",
          "md": "Fig. 4: Visual samples for the correspondence estimations. Correspondences I, II, and III visually display estimated correspondences between the main image and three related images. Sparse co-salient pixels were selected and connected to their corresponding pixels using colored lines for clear visualization.\n\nfoundational segmentation network and enhance it with the Region-to-Region correlation module (R2R) [16] to simply capture inter-image connections. Then, as shown in Table 1, we incrementally incorporate PAG, CAC, and OCC into the baseline for effectiveness analysis. We trained all ablation models with the DUTS class and COCO9k datasets.\n\nEffectiveness of PAG. PAG first uses intermediate image features from the FPN encoder to calculate full-pixel hyperassociations. Then, aggregation networks are applied to generate deep association features for the decoder process. PAG effectively utilizes deep learning to model pair-wise pixel associations, achieving higher-level association knowledge compared to previous image feature optimization strategies. As shown in the 3rd line of Table 1, PAG shows significant performance boosts compared to the baseline model, with respective gains of 3.72%, 4.80%, 7.30%, and 1.31% in $S_m$, $E_ξ$, $F_β$, and $M$.\n\nFurthermore, to validate our approach of progressively enhancing image features with the previously generated association feature for improving hyperassociation calculation, we conduct an ablation experiment called Separate Association Generation (SAG) where association features are generated for three stages without image feature enhancements. As shown in the 2nd&3rd lines in Table 1, PAG outperforms SAG, indicating that our progressive enhancement design can obtain better hyperassociation.\n\nEffectiveness of CAC. CAC aims to condense full-pixel hyperassociations in PAG by selecting corresponding pixels and their surrounding contexts. Results in the 3rd&5th lines of Table 1 show that introducing the CAC module improves the performance. Moreover, it reduces the multiply-accumulate operations (MACs) of aggregation networks from 91.38G in the full-pixel PAG to 77.19G¹. This indicates that utilizing correspondence estimation to condense the hyperassociations not only effectively reduces the computational burden but also helps obtain more accurate pixel associations.\n\nWe also analyze CAC in detail. As deep association features are necessary for reliable correspondence estimation, CAC first pre-condenses the hyperasso-\n\n¹ We input a group of 6 related 256x256 images to measure MACs.",
          "bBox": {
            "x": 134,
            "y": 136,
            "w": 346,
            "h": 527.03
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.694,
      "layout": [
        {
          "image": "page_12_text_1.jpg",
          "confidence": 0.985,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.659,
            "w": 0.567,
            "h": 0.12
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_text_2.jpg",
          "confidence": 0.985,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.43,
            "w": 0.566,
            "h": 0.121
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_text_3.jpg",
          "confidence": 0.984,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.553,
            "w": 0.566,
            "h": 0.104
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_text_4.jpg",
          "confidence": 0.982,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.353,
            "w": 0.567,
            "h": 0.075
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_text_5.jpg",
          "confidence": 0.96,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.781,
            "w": 0.566,
            "h": 0.03
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_caption_1.jpg",
          "confidence": 0.891,
          "label": "caption",
          "bbox": {
            "x": 0.218,
            "y": 0.284,
            "w": 0.567,
            "h": 0.054
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_picture_1.jpg",
          "confidence": 0.793,
          "label": "picture",
          "bbox": {
            "x": 0.22,
            "y": 0.144,
            "w": 0.567,
            "h": 0.131
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_pageHeader_1.jpg",
          "confidence": 0.788,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.015,
            "h": 0.008
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_pageHeader_2.jpg",
          "confidence": 0.728,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.067,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_footnote_1.jpg",
          "confidence": 0.673,
          "label": "footnote",
          "bbox": {
            "x": 0.235,
            "y": 0.827,
            "w": 0.431,
            "h": 0.011
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_12_caption_2.jpg",
          "confidence": 0.672,
          "label": "caption",
          "bbox": {
            "x": 0.67,
            "y": 0.269,
            "w": 0.109,
            "h": 0.008
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 13,
      "text": "                                                                                                     CONDA  13\n\n                                            Table 2: Quantitative comparison of our model with other SOTA methods.\n                                       DC, C9, and CS are DUTS class, COCO9k, and COCO-SEG training data, respectively.\nbold                                  and underline mark the best and second-best excellent results, respectively.\n\n  Methods  Training        CoCA [47]               CoSal2015 [44]                                    CoSOD3k [7]\n              Set    Sm ↑  Eξ  ↑ Fβ ↑        M ↓ Sm ↑                              Eξ ↑  Fβ ↑  M ↓   Sm ↑   Eξ ↑  Fβ ↑ M ↓\n\n    GICD      DC     0.658 0.718 0.513      0.126                           0.844 0.887 0.844 0.071  0.797 0.848  0.770 0.079\n   GCoNet     DC     0.673 0.760 0.544      0.110                           0.845 0.888 0.847 0.068  0.802 0.860  0.778 0.071\n  GCoNet+     DC     0.691 0.786 0.574      0.113                           0.875 0.918 0.876 0.054  0.828 0.881 0.807  0.068\n   CONDA      DC     0.717 0.774                                0.600 0.102 0.890 0.926 0.894 0.049 0.832  0.873  0.807 0.067\n   ICNet      C9     0.654 0.704 0.513      0.147                           0.857 0.901 0.858 0.058 0.794  0.845  0.762 0.089\n    DCFM      C9     0.710 0.783      0.598 0.085 0.838                           0.893 0.856 0.067  0.809 0.874  0.805 0.067\n  GCoNet+     C9     0.717 0.798 0.605      0.098                           0.853 0.902 0.857 0.073  0.819 0.877 0.796  0.075\n   CONDA      C9     0.730 0.801 0.622      0.092                           0.865 0.910 0.875 0.059  0.825 0.877 0.810  0.068\n    CADC    DC+C9    0.680 0.744 0.549      0.133                           0.867 0.906 0.865 0.064  0.815 0.854  0.778 0.088\n    DMT     DC+C9    0.725 0.800 0.619      0.108                           0.897 0.936 0.905 0.045  0.851 0.895  0.835 0.063\n      GCoNet+ DC+C9  0.734 0.808      0.626 0.088 0.876                           0.920 0.880 0.057  0.839 0.894  0.822 0.064\n CONDA DC+C9         0.757 0.825 0.675      0.092                             0.904 0.940 0.912 0.042 0.857 0.899 0.844 0.060\n    UGEM    DC+CS    0.726 0.808 0.599      0.096                           0.885 0.935 0.882 0.051  0.853 0.911 0.829  0.060\n      GCoNet+ DC+CS  0.738 0.814      0.637 0.081 0.881                           0.926 0.891 0.055  0.843 0.901  0.834 0.061\n         CONDA DC+CS 0.763 0.839 0.685      0.089                           0.900 0.938 0.908 0.045 0.862  0.911  0.853 0.056\n\n                                       ciations using a maximum similarity approach to obtain initial deep association\n                                         features and then performs further condensation based on the correspondences\n                                       predicted by these initial deep association features. The hyperassociation con-\n                                       densation process with only the pre-condensation operation is called Similarity-\n                                          induced Association Condensation (SAC). In Table 1, SAC only brings slight\n                                        performance improvements due to the heuristic nature of the correspondence es-\n                                       timation. Nevertheless, SAC can provide the initial association feature for CAC\nto predict reliable correspondence.\nEffectiveness       of  OCC.                                   OCC provides self-supervision for CAC to enable\n                                       more precise correspondence estimation. Results in the 5th&7th lines of Table 1\n                                         show OCC further improves the performance by leveraging more precise corre-\n                                       spondence estimations to condense hyperassociations effectively. In addition, we\n                                         conducted an ablation experiment to validate our object-aware design in CAC\n                                         by replacing OCC with full-pixel cycle consistency (FCC) loss. Comparing the\n                                         5th&6th lines of Table 1, FCC leads to a notable performance decrease due to\nbackground pixels disrupting the correspondence learning.\nVisualization of Correspondence Estimations.                                            We present some visual\n                                      samples of correspondence estimations for some co-salient pixels in Figure 4. Our\n                                        semantic correspondence estimations are meaningful and can effectively depict\nthe common attributes of co-salient objects at the pixel level.\n\n4.4    Comparison with State-of-the-Art Methods\n\n                                          We compare our model with eight recent SOTA methods, i.e. GICD [47], ICNet\n                                           [13], GCoNet [8], CADC [46], DCFM [41], DMT [16], UGEM [38], and GCoNet+\n                                      [51]. We directly utilize their officially released saliency maps for comparison.\n                                        To ensure fairness, we trained our model with different combinations of three\n                                         training datasets, following [51], to align with other compared methods. We",
      "md": "CONDA  13\n\nTable 2: Quantitative comparison of our model with other SOTA methods.\nDC, C9, and CS are DUTS class, COCO9k, and COCO-SEG training data, respectively.\nbold and underline mark the best and second-best excellent results, respectively.\n\n| Methods<br/>Sm ↑ | Training Set<br/>Eξ ↑ | CoCA \\[47]<br/>Fβ ↑ | CoCA \\[47]<br/>M ↓ | CoCA \\[47]<br/>Sm ↑ | CoCA \\[47]<br/>Eξ ↑ | CoSal2015 \\[44]<br/>Fβ ↑ | CoSal2015 \\[44]<br/>M ↓ | CoSal2015 \\[44]<br/>Sm ↑ | CoSal2015 \\[44]<br/>Eξ ↑ | CoSOD3k \\[7]<br/>Fβ ↑ | CoSOD3k \\[7]<br/>M ↓ | CoSOD3k \\[7] | CoSOD3k \\[7] |\n| ---------------- | --------------------- | ------------------- | ------------------ | ------------------- | ------------------- | ------------------------ | ----------------------- | ------------------------ | ------------------------ | --------------------- | -------------------- | ------------ | ------------ |\n| GICD             | DC                    | 0.658               | 0.718              | 0.513               | 0.126               | 0.844                    | 0.887                   | 0.844                    | 0.071                    | 0.797                 | 0.848                | 0.770        | 0.079        |\n| GCoNet           | DC                    | 0.673               | 0.760              | 0.544               | 0.110               | 0.845                    | 0.888                   | 0.847                    | 0.068                    | 0.802                 | 0.860                | 0.778        | 0.071        |\n| GCoNet+          | DC                    | 0.691               | 0.786              | 0.574               | 0.113               | 0.875                    | 0.918                   | 0.876                    | 0.054                    | 0.828                 | 0.881                | 0.807        | 0.068        |\n| CONDA            | DC                    | 0.717               | 0.774              | 0.600               | 0.102               | 0.890                    | 0.926                   | 0.894                    | 0.049                    | 0.832                 | 0.873                | 0.807        | 0.067        |\n| ICNet            | C9                    | 0.654               | 0.704              | 0.513               | 0.147               | 0.857                    | 0.901                   | 0.858                    | 0.058                    | 0.794                 | 0.845                | 0.762        | 0.089        |\n| DCFM             | C9                    | 0.710               | 0.783              | 0.598               | 0.085               | 0.838                    | 0.893                   | 0.856                    | 0.067                    | 0.809                 | 0.874                | 0.805        | 0.067        |\n| GCoNet+          | C9                    | 0.717               | 0.798              | 0.605               | 0.098               | 0.853                    | 0.902                   | 0.857                    | 0.073                    | 0.819                 | 0.877                | 0.796        | 0.075        |\n| CONDA            | C9                    | 0.730               | 0.801              | 0.622               | 0.092               | 0.865                    | 0.910                   | 0.875                    | 0.059                    | 0.825                 | 0.877                | 0.810        | 0.068        |\n| CADC             | DC+C9                 | 0.680               | 0.744              | 0.549               | 0.133               | 0.867                    | 0.906                   | 0.865                    | 0.064                    | 0.815                 | 0.854                | 0.778        | 0.088        |\n| DMT              | DC+C9                 | 0.725               | 0.800              | 0.619               | 0.108               | 0.897                    | 0.936                   | 0.905                    | 0.045                    | 0.851                 | 0.895                | 0.835        | 0.063        |\n| GCoNet+          | DC+C9                 | 0.734               | 0.808              | 0.626               | 0.088               | 0.876                    | 0.920                   | 0.880                    | 0.057                    | 0.839                 | 0.894                | 0.822        | 0.064        |\n| CONDA            | DC+C9                 | 0.757               | 0.825              | 0.675               | 0.092               | 0.904                    | 0.940                   | 0.912                    | 0.042                    | 0.857                 | 0.899                | 0.844        | 0.060        |\n| UGEM             | DC+CS                 | 0.726               | 0.808              | 0.599               | 0.096               | 0.885                    | 0.935                   | 0.882                    | 0.051                    | 0.853                 | 0.911                | 0.829        | 0.060        |\n| GCoNet+          | DC+CS                 | 0.738               | 0.814              | 0.637               | 0.081               | 0.881                    | 0.926                   | 0.891                    | 0.055                    | 0.843                 | 0.901                | 0.834        | 0.061        |\n| CONDA            | DC+CS                 | 0.763               | 0.839              | 0.685               | 0.089               | 0.900                    | 0.938                   | 0.908                    | 0.045                    | 0.862                 | 0.911                | 0.853        | 0.056        |\n\nciations using a maximum similarity approach to obtain initial deep association features and then performs further condensation based on the correspondences predicted by these initial deep association features. The hyperassociation condensation process with only the pre-condensation operation is called Similarity-induced Association Condensation (SAC). In Table 1, SAC only brings slight performance improvements due to the heuristic nature of the correspondence estimation. Nevertheless, SAC can provide the initial association feature for CAC to predict reliable correspondence.\n\nEffectiveness of OCC. OCC provides self-supervision for CAC to enable more precise correspondence estimation. Results in the 5th&7th lines of Table 1 show OCC further improves the performance by leveraging more precise correspondence estimations to condense hyperassociations effectively. In addition, we conducted an ablation experiment to validate our object-aware design in CAC by replacing OCC with full-pixel cycle consistency (FCC) loss. Comparing the 5th&6th lines of Table 1, FCC leads to a notable performance decrease due to background pixels disrupting the correspondence learning.\n\nVisualization of Correspondence Estimations. We present some visual samples of correspondence estimations for some co-salient pixels in Figure 4. Our semantic correspondence estimations are meaningful and can effectively depict the common attributes of co-salient objects at the pixel level.\n\n## 4.4 Comparison with State-of-the-Art Methods\n\nWe compare our model with eight recent SOTA methods, i.e. GICD [47], ICNet [13], GCoNet [8], CADC [46], DCFM [41], DMT [16], UGEM [38], and GCoNet+ [51]. We directly utilize their officially released saliency maps for comparison. To ensure fairness, we trained our model with different combinations of three training datasets, following [51], to align with other compared methods. We",
      "images": [
        {
          "name": "page_13.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_13_table_1.jpg",
          "height": 167.42,
          "width": 345.396,
          "x": 134.987,
          "y": 150.238,
          "original_width": 697,
          "original_height": 338,
          "type": "layout_table"
        },
        {
          "name": "page_13_text_1.jpg",
          "height": 93.492,
          "width": 346.455,
          "x": 134.137,
          "y": 327.162,
          "original_width": 699,
          "original_height": 188,
          "type": "layout_text"
        },
        {
          "name": "page_13_text_2.jpg",
          "height": 94.48,
          "width": 347.02,
          "x": 133.841,
          "y": 422.617,
          "original_width": 700,
          "original_height": 190,
          "type": "layout_text"
        },
        {
          "name": "page_13_text_3.jpg",
          "height": 58.059,
          "width": 346.827,
          "x": 134.004,
          "y": 607.157,
          "original_width": 700,
          "original_height": 117,
          "type": "layout_text"
        },
        {
          "name": "page_13_text_4.jpg",
          "height": 45.772,
          "width": 346.653,
          "x": 133.934,
          "y": 519.053,
          "original_width": 700,
          "original_height": 92,
          "type": "layout_text"
        },
        {
          "name": "page_13_sectionHeader_1.jpg",
          "height": 10.429,
          "width": 244.485,
          "x": 134.13,
          "y": 585.755,
          "original_width": 493,
          "original_height": 21,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_13_caption_1.jpg",
          "height": 31.147,
          "width": 346.395,
          "x": 134.042,
          "y": 116.326,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_caption"
        },
        {
          "name": "page_13_pageHeader_1.jpg",
          "height": 7.833,
          "width": 34.537,
          "x": 413.08,
          "y": 92.707,
          "original_width": 69,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_13_pageHeader_2.jpg",
          "height": 7.378,
          "width": 9.557,
          "x": 470.968,
          "y": 93.064,
          "original_width": 19,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "CONDA  13\n\nTable 2: Quantitative comparison of our model with other SOTA methods.\nDC, C9, and CS are DUTS class, COCO9k, and COCO-SEG training data, respectively.\nbold and underline mark the best and second-best excellent results, respectively.",
          "md": "CONDA  13\n\nTable 2: Quantitative comparison of our model with other SOTA methods.\nDC, C9, and CS are DUTS class, COCO9k, and COCO-SEG training data, respectively.\nbold and underline mark the best and second-best excellent results, respectively.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 340
          }
        },
        {
          "type": "table",
          "rows": [
            [],
            [],
            [],
            [],
            [],
            [
              "Methods<br/>Sm ↑",
              "Training Set<br/>Eξ ↑",
              "CoCA \\[47]<br/>Fβ ↑",
              "CoCA \\[47]<br/>M ↓",
              "CoCA \\[47]<br/>Sm ↑",
              "CoCA \\[47]<br/>Eξ ↑",
              "CoSal2015 \\[44]<br/>Fβ ↑",
              "CoSal2015 \\[44]<br/>M ↓",
              "CoSal2015 \\[44]<br/>Sm ↑",
              "CoSal2015 \\[44]<br/>Eξ ↑",
              "CoSOD3k \\[7]<br/>Fβ ↑",
              "CoSOD3k \\[7]<br/>M ↓",
              "CoSOD3k \\[7]",
              "CoSOD3k \\[7]"
            ],
            [
              "----------------",
              "---------------------",
              "-------------------",
              "------------------",
              "-------------------",
              "-------------------",
              "------------------------",
              "-----------------------",
              "------------------------",
              "------------------------",
              "---------------------",
              "--------------------",
              "------------",
              "------------"
            ],
            [
              "GICD",
              "DC",
              "0.658",
              "0.718",
              "0.513",
              "0.126",
              "0.844",
              "0.887",
              "0.844",
              "0.071",
              "0.797",
              "0.848",
              "0.770",
              "0.079"
            ],
            [
              "GCoNet",
              "DC",
              "0.673",
              "0.760",
              "0.544",
              "0.110",
              "0.845",
              "0.888",
              "0.847",
              "0.068",
              "0.802",
              "0.860",
              "0.778",
              "0.071"
            ],
            [
              "GCoNet+",
              "DC",
              "0.691",
              "0.786",
              "0.574",
              "0.113",
              "0.875",
              "0.918",
              "0.876",
              "0.054",
              "0.828",
              "0.881",
              "0.807",
              "0.068"
            ],
            [
              "CONDA",
              "DC",
              "0.717",
              "0.774",
              "0.600",
              "0.102",
              "0.890",
              "0.926",
              "0.894",
              "0.049",
              "0.832",
              "0.873",
              "0.807",
              "0.067"
            ],
            [
              "ICNet",
              "C9",
              "0.654",
              "0.704",
              "0.513",
              "0.147",
              "0.857",
              "0.901",
              "0.858",
              "0.058",
              "0.794",
              "0.845",
              "0.762",
              "0.089"
            ],
            [
              "DCFM",
              "C9",
              "0.710",
              "0.783",
              "0.598",
              "0.085",
              "0.838",
              "0.893",
              "0.856",
              "0.067",
              "0.809",
              "0.874",
              "0.805",
              "0.067"
            ],
            [
              "GCoNet+",
              "C9",
              "0.717",
              "0.798",
              "0.605",
              "0.098",
              "0.853",
              "0.902",
              "0.857",
              "0.073",
              "0.819",
              "0.877",
              "0.796",
              "0.075"
            ],
            [
              "CONDA",
              "C9",
              "0.730",
              "0.801",
              "0.622",
              "0.092",
              "0.865",
              "0.910",
              "0.875",
              "0.059",
              "0.825",
              "0.877",
              "0.810",
              "0.068"
            ],
            [
              "CADC",
              "DC+C9",
              "0.680",
              "0.744",
              "0.549",
              "0.133",
              "0.867",
              "0.906",
              "0.865",
              "0.064",
              "0.815",
              "0.854",
              "0.778",
              "0.088"
            ],
            [
              "DMT",
              "DC+C9",
              "0.725",
              "0.800",
              "0.619",
              "0.108",
              "0.897",
              "0.936",
              "0.905",
              "0.045",
              "0.851",
              "0.895",
              "0.835",
              "0.063"
            ],
            [
              "GCoNet+",
              "DC+C9",
              "0.734",
              "0.808",
              "0.626",
              "0.088",
              "0.876",
              "0.920",
              "0.880",
              "0.057",
              "0.839",
              "0.894",
              "0.822",
              "0.064"
            ],
            [
              "CONDA",
              "DC+C9",
              "0.757",
              "0.825",
              "0.675",
              "0.092",
              "0.904",
              "0.940",
              "0.912",
              "0.042",
              "0.857",
              "0.899",
              "0.844",
              "0.060"
            ],
            [
              "UGEM",
              "DC+CS",
              "0.726",
              "0.808",
              "0.599",
              "0.096",
              "0.885",
              "0.935",
              "0.882",
              "0.051",
              "0.853",
              "0.911",
              "0.829",
              "0.060"
            ],
            [
              "GCoNet+",
              "DC+CS",
              "0.738",
              "0.814",
              "0.637",
              "0.081",
              "0.881",
              "0.926",
              "0.891",
              "0.055",
              "0.843",
              "0.901",
              "0.834",
              "0.061"
            ],
            [
              "CONDA",
              "DC+CS",
              "0.763",
              "0.839",
              "0.685",
              "0.089",
              "0.900",
              "0.938",
              "0.908",
              "0.045",
              "0.862",
              "0.911",
              "0.853",
              "0.056"
            ]
          ],
          "md": "CONDA  13\nTable 2: Quantitative comparison of our model with other SOTA methods.\nDC, C9, and CS are DUTS class, COCO9k, and COCO-SEG training data, respectively.\nbold and underline mark the best and second-best excellent results, respectively.\n\n| Methods<br/>Sm ↑ | Training Set<br/>Eξ ↑ | CoCA \\[47]<br/>Fβ ↑ | CoCA \\[47]<br/>M ↓ | CoCA \\[47]<br/>Sm ↑ | CoCA \\[47]<br/>Eξ ↑ | CoSal2015 \\[44]<br/>Fβ ↑ | CoSal2015 \\[44]<br/>M ↓ | CoSal2015 \\[44]<br/>Sm ↑ | CoSal2015 \\[44]<br/>Eξ ↑ | CoSOD3k \\[7]<br/>Fβ ↑ | CoSOD3k \\[7]<br/>M ↓ | CoSOD3k \\[7] | CoSOD3k \\[7] |\n| ---------------- | --------------------- | ------------------- | ------------------ | ------------------- | ------------------- | ------------------------ | ----------------------- | ------------------------ | ------------------------ | --------------------- | -------------------- | ------------ | ------------ |\n| GICD             | DC                    | 0.658               | 0.718              | 0.513               | 0.126               | 0.844                    | 0.887                   | 0.844                    | 0.071                    | 0.797                 | 0.848                | 0.770        | 0.079        |\n| GCoNet           | DC                    | 0.673               | 0.760              | 0.544               | 0.110               | 0.845                    | 0.888                   | 0.847                    | 0.068                    | 0.802                 | 0.860                | 0.778        | 0.071        |\n| GCoNet+          | DC                    | 0.691               | 0.786              | 0.574               | 0.113               | 0.875                    | 0.918                   | 0.876                    | 0.054                    | 0.828                 | 0.881                | 0.807        | 0.068        |\n| CONDA            | DC                    | 0.717               | 0.774              | 0.600               | 0.102               | 0.890                    | 0.926                   | 0.894                    | 0.049                    | 0.832                 | 0.873                | 0.807        | 0.067        |\n| ICNet            | C9                    | 0.654               | 0.704              | 0.513               | 0.147               | 0.857                    | 0.901                   | 0.858                    | 0.058                    | 0.794                 | 0.845                | 0.762        | 0.089        |\n| DCFM             | C9                    | 0.710               | 0.783              | 0.598               | 0.085               | 0.838                    | 0.893                   | 0.856                    | 0.067                    | 0.809                 | 0.874                | 0.805        | 0.067        |\n| GCoNet+          | C9                    | 0.717               | 0.798              | 0.605               | 0.098               | 0.853                    | 0.902                   | 0.857                    | 0.073                    | 0.819                 | 0.877                | 0.796        | 0.075        |\n| CONDA            | C9                    | 0.730               | 0.801              | 0.622               | 0.092               | 0.865                    | 0.910                   | 0.875                    | 0.059                    | 0.825                 | 0.877                | 0.810        | 0.068        |\n| CADC             | DC+C9                 | 0.680               | 0.744              | 0.549               | 0.133               | 0.867                    | 0.906                   | 0.865                    | 0.064                    | 0.815                 | 0.854                | 0.778        | 0.088        |\n| DMT              | DC+C9                 | 0.725               | 0.800              | 0.619               | 0.108               | 0.897                    | 0.936                   | 0.905                    | 0.045                    | 0.851                 | 0.895                | 0.835        | 0.063        |\n| GCoNet+          | DC+C9                 | 0.734               | 0.808              | 0.626               | 0.088               | 0.876                    | 0.920                   | 0.880                    | 0.057                    | 0.839                 | 0.894                | 0.822        | 0.064        |\n| CONDA            | DC+C9                 | 0.757               | 0.825              | 0.675               | 0.092               | 0.904                    | 0.940                   | 0.912                    | 0.042                    | 0.857                 | 0.899                | 0.844        | 0.060        |\n| UGEM             | DC+CS                 | 0.726               | 0.808              | 0.599               | 0.096               | 0.885                    | 0.935                   | 0.882                    | 0.051                    | 0.853                 | 0.911                | 0.829        | 0.060        |\n| GCoNet+          | DC+CS                 | 0.738               | 0.814              | 0.637               | 0.081               | 0.881                    | 0.926                   | 0.891                    | 0.055                    | 0.843                 | 0.901                | 0.834        | 0.061        |\n| CONDA            | DC+CS                 | 0.763               | 0.839              | 0.685               | 0.089               | 0.900                    | 0.938                   | 0.908                    | 0.045                    | 0.862                 | 0.911                | 0.853        | 0.056        |",
          "isPerfectTable": false,
          "csv": "\n\n\n\n\n\"Methods<br/>Sm ↑\",\"Training Set<br/>Eξ ↑\",\"CoCA \\[47]<br/>Fβ ↑\",\"CoCA \\[47]<br/>M ↓\",\"CoCA \\[47]<br/>Sm ↑\",\"CoCA \\[47]<br/>Eξ ↑\",\"CoSal2015 \\[44]<br/>Fβ ↑\",\"CoSal2015 \\[44]<br/>M ↓\",\"CoSal2015 \\[44]<br/>Sm ↑\",\"CoSal2015 \\[44]<br/>Eξ ↑\",\"CoSOD3k \\[7]<br/>Fβ ↑\",\"CoSOD3k \\[7]<br/>M ↓\",\"CoSOD3k \\[7]\",\"CoSOD3k \\[7]\"\n\"----------------\",\"---------------------\",\"-------------------\",\"------------------\",\"-------------------\",\"-------------------\",\"------------------------\",\"-----------------------\",\"------------------------\",\"------------------------\",\"---------------------\",\"--------------------\",\"------------\",\"------------\"\n\"GICD\",\"DC\",\"0.658\",\"0.718\",\"0.513\",\"0.126\",\"0.844\",\"0.887\",\"0.844\",\"0.071\",\"0.797\",\"0.848\",\"0.770\",\"0.079\"\n\"GCoNet\",\"DC\",\"0.673\",\"0.760\",\"0.544\",\"0.110\",\"0.845\",\"0.888\",\"0.847\",\"0.068\",\"0.802\",\"0.860\",\"0.778\",\"0.071\"\n\"GCoNet+\",\"DC\",\"0.691\",\"0.786\",\"0.574\",\"0.113\",\"0.875\",\"0.918\",\"0.876\",\"0.054\",\"0.828\",\"0.881\",\"0.807\",\"0.068\"\n\"CONDA\",\"DC\",\"0.717\",\"0.774\",\"0.600\",\"0.102\",\"0.890\",\"0.926\",\"0.894\",\"0.049\",\"0.832\",\"0.873\",\"0.807\",\"0.067\"\n\"ICNet\",\"C9\",\"0.654\",\"0.704\",\"0.513\",\"0.147\",\"0.857\",\"0.901\",\"0.858\",\"0.058\",\"0.794\",\"0.845\",\"0.762\",\"0.089\"\n\"DCFM\",\"C9\",\"0.710\",\"0.783\",\"0.598\",\"0.085\",\"0.838\",\"0.893\",\"0.856\",\"0.067\",\"0.809\",\"0.874\",\"0.805\",\"0.067\"\n\"GCoNet+\",\"C9\",\"0.717\",\"0.798\",\"0.605\",\"0.098\",\"0.853\",\"0.902\",\"0.857\",\"0.073\",\"0.819\",\"0.877\",\"0.796\",\"0.075\"\n\"CONDA\",\"C9\",\"0.730\",\"0.801\",\"0.622\",\"0.092\",\"0.865\",\"0.910\",\"0.875\",\"0.059\",\"0.825\",\"0.877\",\"0.810\",\"0.068\"\n\"CADC\",\"DC+C9\",\"0.680\",\"0.744\",\"0.549\",\"0.133\",\"0.867\",\"0.906\",\"0.865\",\"0.064\",\"0.815\",\"0.854\",\"0.778\",\"0.088\"\n\"DMT\",\"DC+C9\",\"0.725\",\"0.800\",\"0.619\",\"0.108\",\"0.897\",\"0.936\",\"0.905\",\"0.045\",\"0.851\",\"0.895\",\"0.835\",\"0.063\"\n\"GCoNet+\",\"DC+C9\",\"0.734\",\"0.808\",\"0.626\",\"0.088\",\"0.876\",\"0.920\",\"0.880\",\"0.057\",\"0.839\",\"0.894\",\"0.822\",\"0.064\"\n\"CONDA\",\"DC+C9\",\"0.757\",\"0.825\",\"0.675\",\"0.092\",\"0.904\",\"0.940\",\"0.912\",\"0.042\",\"0.857\",\"0.899\",\"0.844\",\"0.060\"\n\"UGEM\",\"DC+CS\",\"0.726\",\"0.808\",\"0.599\",\"0.096\",\"0.885\",\"0.935\",\"0.882\",\"0.051\",\"0.853\",\"0.911\",\"0.829\",\"0.060\"\n\"GCoNet+\",\"DC+CS\",\"0.738\",\"0.814\",\"0.637\",\"0.081\",\"0.881\",\"0.926\",\"0.891\",\"0.055\",\"0.843\",\"0.901\",\"0.834\",\"0.061\"\n\"CONDA\",\"DC+CS\",\"0.763\",\"0.839\",\"0.685\",\"0.089\",\"0.900\",\"0.938\",\"0.908\",\"0.045\",\"0.862\",\"0.911\",\"0.853\",\"0.056\"",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 537
          }
        },
        {
          "type": "text",
          "value": "ciations using a maximum similarity approach to obtain initial deep association features and then performs further condensation based on the correspondences predicted by these initial deep association features. The hyperassociation condensation process with only the pre-condensation operation is called Similarity-induced Association Condensation (SAC). In Table 1, SAC only brings slight performance improvements due to the heuristic nature of the correspondence estimation. Nevertheless, SAC can provide the initial association feature for CAC to predict reliable correspondence.\n\nEffectiveness of OCC. OCC provides self-supervision for CAC to enable more precise correspondence estimation. Results in the 5th&7th lines of Table 1 show OCC further improves the performance by leveraging more precise correspondence estimations to condense hyperassociations effectively. In addition, we conducted an ablation experiment to validate our object-aware design in CAC by replacing OCC with full-pixel cycle consistency (FCC) loss. Comparing the 5th&6th lines of Table 1, FCC leads to a notable performance decrease due to background pixels disrupting the correspondence learning.\n\nVisualization of Correspondence Estimations. We present some visual samples of correspondence estimations for some co-salient pixels in Figure 4. Our semantic correspondence estimations are meaningful and can effectively depict the common attributes of co-salient objects at the pixel level.",
          "md": "ciations using a maximum similarity approach to obtain initial deep association features and then performs further condensation based on the correspondences predicted by these initial deep association features. The hyperassociation condensation process with only the pre-condensation operation is called Similarity-induced Association Condensation (SAC). In Table 1, SAC only brings slight performance improvements due to the heuristic nature of the correspondence estimation. Nevertheless, SAC can provide the initial association feature for CAC to predict reliable correspondence.\n\nEffectiveness of OCC. OCC provides self-supervision for CAC to enable more precise correspondence estimation. Results in the 5th&7th lines of Table 1 show OCC further improves the performance by leveraging more precise correspondence estimations to condense hyperassociations effectively. In addition, we conducted an ablation experiment to validate our object-aware design in CAC by replacing OCC with full-pixel cycle consistency (FCC) loss. Comparing the 5th&6th lines of Table 1, FCC leads to a notable performance decrease due to background pixels disrupting the correspondence learning.\n\nVisualization of Correspondence Estimations. We present some visual samples of correspondence estimations for some co-salient pixels in Figure 4. Our semantic correspondence estimations are meaningful and can effectively depict the common attributes of co-salient objects at the pixel level.",
          "bBox": {
            "x": 134,
            "y": 180.03,
            "w": 346,
            "h": 383.01
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "4.4 Comparison with State-of-the-Art Methods",
          "md": "## 4.4 Comparison with State-of-the-Art Methods",
          "bBox": {
            "x": 134,
            "y": 160.03,
            "w": 244,
            "h": 434.01
          }
        },
        {
          "type": "text",
          "value": "We compare our model with eight recent SOTA methods, i.e. GICD [47], ICNet [13], GCoNet [8], CADC [46], DCFM [41], DMT [16], UGEM [38], and GCoNet+ [51]. We directly utilize their officially released saliency maps for comparison. To ensure fairness, we trained our model with different combinations of three training datasets, following [51], to align with other compared methods. We",
          "md": "We compare our model with eight recent SOTA methods, i.e. GICD [47], ICNet [13], GCoNet [8], CADC [46], DCFM [41], DMT [16], UGEM [38], and GCoNet+ [51]. We directly utilize their officially released saliency maps for comparison. To ensure fairness, we trained our model with different combinations of three training datasets, following [51], to align with other compared methods. We",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 572
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.594,
      "layout": [
        {
          "image": "page_13_table_1.jpg",
          "confidence": 0.989,
          "label": "table",
          "bbox": {
            "x": 0.22,
            "y": 0.189,
            "w": 0.564,
            "h": 0.211
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_text_1.jpg",
          "confidence": 0.984,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.41,
            "w": 0.566,
            "h": 0.12
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_text_2.jpg",
          "confidence": 0.984,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.531,
            "w": 0.567,
            "h": 0.121
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_text_3.jpg",
          "confidence": 0.983,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.765,
            "w": 0.566,
            "h": 0.074
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_text_4.jpg",
          "confidence": 0.975,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.652,
            "w": 0.566,
            "h": 0.06
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_sectionHeader_1.jpg",
          "confidence": 0.956,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.739,
            "w": 0.399,
            "h": 0.013
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_caption_1.jpg",
          "confidence": 0.941,
          "label": "caption",
          "bbox": {
            "x": 0.218,
            "y": 0.145,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_pageHeader_1.jpg",
          "confidence": 0.922,
          "label": "pageHeader",
          "bbox": {
            "x": 0.674,
            "y": 0.117,
            "w": 0.056,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_13_pageHeader_2.jpg",
          "confidence": 0.893,
          "label": "pageHeader",
          "bbox": {
            "x": 0.769,
            "y": 0.117,
            "w": 0.015,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 14,
      "text": "14     L Li et al.\n\n         Accordion                rollingpin                Towel\n                      1\n\n Image\n\n GT\n\n Ours\n\n GCoNet+\n\n UGEM\n\n DMT\n\n Fig. 5: Qualitative comparisons of our model with other SOTA methods.\ndenote three training datasets, i.e. DUTS class [47], COCO9k [18], and COCO-\nSEG [37], as DC, C9, and CS, respectively, for convenience. Our training sets\ninclude DC, C9, DC+C9, DC+CS. As shown in Table 2, we can observe that our\nmodel achieves the best performance with each training set in most benchmark\ndatasets. What is even more exciting is that we achieve excellent results in the\nmost challenging CoCA dataset, surpassing the second-best models by large\nmargins, e.g. 2.5% Sm, 2.5% Eξ, and 4.8% Fβ with the DC+CS training set.\n   We also present visual comparisons in Figure 5. Our model can accurately\ndetect co-salient objects in complex scenarios, such as irregularly shaped accor-\ndions accompanied by extraneous objects (people). However, other models easily\nfail to accurately segment co-salient objects.\n\n5   Conclusion\n\nThis paper proposes a deep association learning strategy for CoSOD that di-\nrectly embeds hyperassociations into deep association features. Correspondence\nestimation is also introduced to condense hyperassociations, enabling the explo-\nration of pixel-level correspondences for CoSOD. We also utilize an object-aware\ncycle consistency loss to further refine correspondence estimations. Extensive\nexperiments have verified the effectiveness of our method.\nAcknowledgments. This work was supported in part by Key-Area Research\nand Development Program of Guangdong Province under Grant 2021B0101200001,\nthe National Natural Science Foundation of China under Grant 62136007, 62036011,\nU20B2065, 6202781, 62036005, the Key R&D Program of Shaanxi Province un-\nder Grant 2021ZDLGY01-08, and the MBZUAI-WIS Joint Program for AI Re-\nsearch under Grants WIS P008 and P009.",
      "md": "14     L Li et al.\n\nQualitative comparisons of our model with other SOTA methods\n\nFig. 5: Qualitative comparisons of our model with other SOTA methods.\n\ndenote three training datasets, i.e. DUTS class [47], COCO9k [18], and COCO-SEG [37], as DC, C9, and CS, respectively, for convenience. Our training sets include DC, C9, DC+C9, DC+CS. As shown in Table 2, we can observe that our model achieves the best performance with each training set in most benchmark datasets. What is even more exciting is that we achieve excellent results in the most challenging CoCA dataset, surpassing the second-best models by large margins, e.g. 2.5% $S_m$, 2.5% $E_\\xi$, and 4.8% $F_\\beta$ with the DC+CS training set.\n\nWe also present visual comparisons in Figure 5. Our model can accurately detect co-salient objects in complex scenarios, such as irregularly shaped accordions accompanied by extraneous objects (people). However, other models easily fail to accurately segment co-salient objects.\n\n## 5 Conclusion\n\nThis paper proposes a deep association learning strategy for CoSOD that directly embeds hyperassociations into deep association features. Correspondence estimation is also introduced to condense hyperassociations, enabling the exploration of pixel-level correspondences for CoSOD. We also utilize an object-aware cycle consistency loss to further refine correspondence estimations. Extensive experiments have verified the effectiveness of our method.\n\nAcknowledgments. This work was supported in part by Key-Area Research and Development Program of Guangdong Province under Grant 2021B0101200001, the National Natural Science Foundation of China under Grant 62136007, 62036011, U20B2065, 6202781, 62036005, the Key R&D Program of Shaanxi Province under Grant 2021ZDLGY01-08, and the MBZUAI-WIS Joint Program for AI Research under Grants WIS P008 and P009.",
      "images": [
        {
          "name": "img_p13_1.png",
          "height": 480,
          "width": 640,
          "x": 142.538,
          "y": 124.4296,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p13_10.png",
          "height": 480,
          "width": 640,
          "x": 142.53800000000004,
          "y": 156.52659999999997,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p13_19.png",
          "height": 480,
          "width": 640,
          "x": 142.5380000000001,
          "y": 188.62359999999998,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p13_28.png",
          "height": 480,
          "width": 640,
          "x": 142.53800000000012,
          "y": 221.43959999999998,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p13_37.png",
          "height": 480,
          "width": 640,
          "x": 142.53800000000015,
          "y": 253.76960000000003,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p13_46.png",
          "height": 480,
          "width": 640,
          "x": 142.5380000000002,
          "y": 285.8656000000001,
          "original_width": 640,
          "original_height": 480
        },
        {
          "name": "img_p13_3.png",
          "height": 426,
          "width": 640,
          "x": 217.14800000000002,
          "y": 124.42544000000001,
          "original_width": 640,
          "original_height": 426,
          "ocr": [
            {
              "x": 289,
              "y": 41,
              "w": 16,
              "h": 106,
              "confidence": 0.4519528031738105,
              "text": "1"
            }
          ]
        },
        {
          "name": "img_p13_4.png",
          "height": 426,
          "width": 640,
          "x": 254.45300000000003,
          "y": 124.4261216,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_5.png",
          "height": 426,
          "width": 640,
          "x": 291.75800000000004,
          "y": 124.4261216,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_6.png",
          "height": 426,
          "width": 640,
          "x": 329.064,
          "y": 124.42544000000001,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_8.png",
          "height": 426,
          "width": 640,
          "x": 403.67400000000004,
          "y": 124.4261216,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_9.png",
          "height": 426,
          "width": 640,
          "x": 440.98,
          "y": 124.4261216,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_12.png",
          "height": 426,
          "width": 640,
          "x": 217.14800000000005,
          "y": 156.52244,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_13.png",
          "height": 426,
          "width": 640,
          "x": 254.45300000000006,
          "y": 156.52244,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_14.png",
          "height": 426,
          "width": 640,
          "x": 291.75900000000007,
          "y": 156.52244,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_15.png",
          "height": 426,
          "width": 640,
          "x": 329.0640000000001,
          "y": 156.52244,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_17.png",
          "height": 426,
          "width": 640,
          "x": 403.6740000000001,
          "y": 156.52244,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_18.png",
          "height": 426,
          "width": 640,
          "x": 440.9800000000001,
          "y": 156.52244,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_21.png",
          "height": 426,
          "width": 640,
          "x": 217.1480000000001,
          "y": 188.61944,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_22.png",
          "height": 426,
          "width": 640,
          "x": 254.45300000000012,
          "y": 188.61944,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_23.png",
          "height": 426,
          "width": 640,
          "x": 291.7590000000001,
          "y": 188.61944,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_24.png",
          "height": 426,
          "width": 640,
          "x": 329.06400000000014,
          "y": 188.61944,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_26.png",
          "height": 426,
          "width": 640,
          "x": 403.67400000000015,
          "y": 188.61944,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_27.png",
          "height": 426,
          "width": 640,
          "x": 440.98000000000013,
          "y": 188.61944,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_30.png",
          "height": 426,
          "width": 640,
          "x": 217.14800000000014,
          "y": 221.43544,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_31.png",
          "height": 426,
          "width": 640,
          "x": 254.45300000000015,
          "y": 221.43544,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_32.png",
          "height": 426,
          "width": 640,
          "x": 291.7590000000001,
          "y": 221.43544,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_33.png",
          "height": 426,
          "width": 640,
          "x": 329.06400000000014,
          "y": 221.43544,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_35.png",
          "height": 426,
          "width": 640,
          "x": 403.67400000000015,
          "y": 221.43544,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "img_p13_36.png",
          "height": 426,
          "width": 640,
          "x": 440.98000000000013,
          "y": 221.43544,
          "original_width": 640,
          "original_height": 426
        },
        {
          "name": "page_14.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_14_text_1.jpg",
          "height": 69.454,
          "width": 347.263,
          "x": 133.774,
          "y": 523.374,
          "original_width": 701,
          "original_height": 140,
          "type": "layout_text"
        },
        {
          "name": "page_14_text_2.jpg",
          "height": 82.043,
          "width": 347.005,
          "x": 133.945,
          "y": 342.851,
          "original_width": 700,
          "original_height": 165,
          "type": "layout_text"
        },
        {
          "name": "page_14_picture_1.jpg",
          "height": 203.217,
          "width": 343.433,
          "x": 134.424,
          "y": 114.499,
          "original_width": 693,
          "original_height": 410,
          "type": "layout_picture"
        },
        {
          "name": "page_14_text_3.jpg",
          "height": 68.729,
          "width": 358.535,
          "x": 133.736,
          "y": 595.176,
          "original_width": 724,
          "original_height": 138,
          "type": "layout_text"
        },
        {
          "name": "page_14_text_4.jpg",
          "height": 46.415,
          "width": 347.065,
          "x": 133.808,
          "y": 426.955,
          "original_width": 700,
          "original_height": 93,
          "type": "layout_text"
        },
        {
          "name": "page_14_sectionHeader_1.jpg",
          "height": 9.799,
          "width": 84.413,
          "x": 134.47,
          "y": 495.638,
          "original_width": 170,
          "original_height": 19,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_14_caption_1.jpg",
          "height": 9.337,
          "width": 336.073,
          "x": 139.173,
          "y": 323.799,
          "original_width": 678,
          "original_height": 18,
          "type": "layout_caption"
        },
        {
          "name": "page_14_pageHeader_1.jpg",
          "height": 7.123,
          "width": 9.988,
          "x": 134.273,
          "y": 93.156,
          "original_width": 20,
          "original_height": 14,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_14_pageHeader_2.jpg",
          "height": 7.689,
          "width": 41.531,
          "x": 167.25,
          "y": 92.788,
          "original_width": 83,
          "original_height": 15,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "14     L Li et al.\n\nQualitative comparisons of our model with other SOTA methods\n\nFig. 5: Qualitative comparisons of our model with other SOTA methods.\n\ndenote three training datasets, i.e. DUTS class [47], COCO9k [18], and COCO-SEG [37], as DC, C9, and CS, respectively, for convenience. Our training sets include DC, C9, DC+C9, DC+CS. As shown in Table 2, we can observe that our model achieves the best performance with each training set in most benchmark datasets. What is even more exciting is that we achieve excellent results in the most challenging CoCA dataset, surpassing the second-best models by large margins, e.g. 2.5% $S_m$, 2.5% $E_\\xi$, and 4.8% $F_\\beta$ with the DC+CS training set.\n\nWe also present visual comparisons in Figure 5. Our model can accurately detect co-salient objects in complex scenarios, such as irregularly shaped accordions accompanied by extraneous objects (people). However, other models easily fail to accurately segment co-salient objects.",
          "md": "14     L Li et al.\n\nQualitative comparisons of our model with other SOTA methods\n\nFig. 5: Qualitative comparisons of our model with other SOTA methods.\n\ndenote three training datasets, i.e. DUTS class [47], COCO9k [18], and COCO-SEG [37], as DC, C9, and CS, respectively, for convenience. Our training sets include DC, C9, DC+C9, DC+CS. As shown in Table 2, we can observe that our model achieves the best performance with each training set in most benchmark datasets. What is even more exciting is that we achieve excellent results in the most challenging CoCA dataset, surpassing the second-best models by large margins, e.g. 2.5% $S_m$, 2.5% $E_\\xi$, and 4.8% $F_\\beta$ with the DC+CS training set.\n\nWe also present visual comparisons in Figure 5. Our model can accurately detect co-salient objects in complex scenarios, such as irregularly shaped accordions accompanied by extraneous objects (people). However, other models easily fail to accurately segment co-salient objects.",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 414.01
          }
        },
        {
          "type": "heading",
          "lvl": 2,
          "value": "5 Conclusion",
          "md": "## 5 Conclusion",
          "bBox": {
            "x": 134,
            "y": 493.04,
            "w": 84,
            "h": 12
          }
        },
        {
          "type": "text",
          "value": "This paper proposes a deep association learning strategy for CoSOD that directly embeds hyperassociations into deep association features. Correspondence estimation is also introduced to condense hyperassociations, enabling the exploration of pixel-level correspondences for CoSOD. We also utilize an object-aware cycle consistency loss to further refine correspondence estimations. Extensive experiments have verified the effectiveness of our method.\n\nAcknowledgments. This work was supported in part by Key-Area Research and Development Program of Guangdong Province under Grant 2021B0101200001, the National Natural Science Foundation of China under Grant 62136007, 62036011, U20B2065, 6202781, 62036005, the Key R&D Program of Shaanxi Province under Grant 2021ZDLGY01-08, and the MBZUAI-WIS Joint Program for AI Research under Grants WIS P008 and P009.",
          "md": "This paper proposes a deep association learning strategy for CoSOD that directly embeds hyperassociations into deep association features. Correspondence estimation is also introduced to condense hyperassociations, enabling the exploration of pixel-level correspondences for CoSOD. We also utilize an object-aware cycle consistency loss to further refine correspondence estimations. Extensive experiments have verified the effectiveness of our method.\n\nAcknowledgments. This work was supported in part by Key-Area Research and Development Program of Guangdong Province under Grant 2021B0101200001, the National Natural Science Foundation of China under Grant 62136007, 62036011, U20B2065, 6202781, 62036005, the Key R&D Program of Shaanxi Province under Grant 2021ZDLGY01-08, and the MBZUAI-WIS Joint Program for AI Research under Grants WIS P008 and P009.",
          "bBox": {
            "x": 134,
            "y": 127,
            "w": 358,
            "h": 536.04
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.971,
      "layout": [
        {
          "image": "page_14_text_1.jpg",
          "confidence": 0.983,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.657,
            "w": 0.567,
            "h": 0.09
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_text_2.jpg",
          "confidence": 0.981,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.43,
            "w": 0.567,
            "h": 0.106
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_picture_1.jpg",
          "confidence": 0.977,
          "label": "picture",
          "bbox": {
            "x": 0.219,
            "y": 0.143,
            "w": 0.561,
            "h": 0.257
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_text_3.jpg",
          "confidence": 0.975,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.75,
            "w": 0.585,
            "h": 0.088
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_text_4.jpg",
          "confidence": 0.973,
          "label": "text",
          "bbox": {
            "x": 0.218,
            "y": 0.536,
            "w": 0.567,
            "h": 0.061
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_sectionHeader_1.jpg",
          "confidence": 0.948,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.625,
            "w": 0.137,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_caption_1.jpg",
          "confidence": 0.926,
          "label": "caption",
          "bbox": {
            "x": 0.227,
            "y": 0.407,
            "w": 0.549,
            "h": 0.012
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_pageHeader_1.jpg",
          "confidence": 0.776,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.016,
            "h": 0.008
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_14_pageHeader_2.jpg",
          "confidence": 0.686,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.067,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 15,
      "text": "                                                                           CONDA         15\n\n        References\n\n        1. Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.: Frequency-tuned salient re-\n            gion detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 1597–1604 (2009)\n         2. Chen,   W., Xu,  H.,  Zhou,  Z., Liu, Y., Sun,  B., Kang,   W.,  Xie, X.: Cost-\n         former: Cost transformer for cost aggregation in multi-view stereo. arXiv preprint\n            arXiv:2305.10320 (2023)\n        3. Cheng, M.M., Warrell, J., Lin, W.Y., Zheng, S., Vineet, V., Crook, N.: Efficient\n          salient region detection with soft image abstraction. In: Int. Conf. Comput. Vis.\n            pp. 1529–1536 (2013)\n         4. Cho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., Kim, S.: Cats: Cost aggregation\n            transformers for visual correspondence 34, 9011–9023 (2021)\n        5. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: A new way\n            to evaluate foreground maps. In: Int. Conf. Comput. Vis. pp. 4548–4557 (2017)\n       6. Fan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.: Enhanced-alignment\n            measure for binary foreground map evaluation. In: IJCAI. pp. 698–704 (2018)\n         7. Fan, D.P., Li, T., Lin, Z., Ji, G.P., Zhang, D., Cheng, M.M., Fu, H., Shen, J.:\n           Re-thinking co-salient object detection. IEEE Trans. Pattern Anal. Mach. Intell.\n            44(8), 4339–4354 (2021)\n           8. Fan, Q., Fan, D.P., Fu, H., Tang, C.K., Shao, L., Tai, Y.W.: Group collabora-\n         tive learning for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern\n            Recog. pp. 12288–12298 (2021)\n         9. Fu, H., Cao, X., Tu, Z.: Cluster-based co-saliency detection. IEEE Trans. Image\n            Process. 22(10), 3766–3778 (2013)\n       10. Han, J., Cheng, G., Li, Z., Zhang, D.: A unified metric learning-based framework\n          for co-saliency detection. IEEE Trans. Circuit Syst. Video Technol. 28(10), 2473–\n            2483 (2017)\n   11. Hong, S., Cho, S., Nam, J., Lin, S., Kim, S.: Cost aggregation with 4d convolutional\n           swin transformer for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 108–\n            126 (2022)\n        12. Hong, S., Nam, J., Cho, S., Hong, S., Jeon, S., Min, D., Kim, S.: Neural match-\n            ing fields: Implicit representation of matching fields for visual correspondence 35,\n            13512–13526 (2022)\n       13. Jin, W.D., Xu, J., Cheng, M.M., Zhang, Y., Guo, W.: Icnet: Intra-saliency corre-\n        lation network for co-saliency detection. Adv. Neural Inform. Process. Syst. (2020)\n      14. Kim, S., Min, J., Cho, M.: Transformatcher: Match-to-match attention for semantic\n            correspondence. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 8697–8707 (2022)\n       15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\n            arXiv:1412.6980 (2014)\n       16. Li, L., Han, J., Zhang, N., Liu, N., Khan, S., Cholakkal, H., Anwer, R.M., Khan,\n          F.S.: Discriminative co-saliency and background mining transformer for co-salient\n                 object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 7247–7256\n            (2023)\n17. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyra-\n              mid networks for object detection. In: IEEE Conf. Comput. Vis. Pattern Recog.\n            pp. 2117–2125 (2017)\n     18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,\n           Zitnick, C.L.: Microsoft coco: Common objects in context. In: Eur. Conf. Comput.\n            Vis. pp. 740–755 (2014)",
      "md": "# References\n\n1. Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.: Frequency-tuned salient region detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 1597–1604 (2009)\n\n2. Chen, W., Xu, H., Zhou, Z., Liu, Y., Sun, B., Kang, W., Xie, X.: Cost-former: Cost transformer for cost aggregation in multi-view stereo. arXiv preprint arXiv:2305.10320 (2023)\n\n3. Cheng, M.M., Warrell, J., Lin, W.Y., Zheng, S., Vineet, V., Crook, N.: Efficient salient region detection with soft image abstraction. In: Int. Conf. Comput. Vis. pp. 1529–1536 (2013)\n\n4. Cho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., Kim, S.: Cats: Cost aggregation transformers for visual correspondence 34, 9011–9023 (2021)\n\n5. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: A new way to evaluate foreground maps. In: Int. Conf. Comput. Vis. pp. 4548–4557 (2017)\n\n6. Fan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.: Enhanced-alignment measure for binary foreground map evaluation. In: IJCAI. pp. 698–704 (2018)\n\n7. Fan, D.P., Li, T., Lin, Z., Ji, G.P., Zhang, D., Cheng, M.M., Fu, H., Shen, J.: Re-thinking co-salient object detection. IEEE Trans. Pattern Anal. Mach. Intell. 44(8), 4339–4354 (2021)\n\n8. Fan, Q., Fan, D.P., Fu, H., Tang, C.K., Shao, L., Tai, Y.W.: Group collaborative learning for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 12288–12298 (2021)\n\n9. Fu, H., Cao, X., Tu, Z.: Cluster-based co-saliency detection. IEEE Trans. Image Process. 22(10), 3766–3778 (2013)\n\n10. Han, J., Cheng, G., Li, Z., Zhang, D.: A unified metric learning-based framework for co-saliency detection. IEEE Trans. Circuit Syst. Video Technol. 28(10), 2473–2483 (2017)\n\n11. Hong, S., Cho, S., Nam, J., Lin, S., Kim, S.: Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 108–126 (2022)\n\n12. Hong, S., Nam, J., Cho, S., Hong, S., Jeon, S., Min, D., Kim, S.: Neural matching fields: Implicit representation of matching fields for visual correspondence 35, 13512–13526 (2022)\n\n13. Jin, W.D., Xu, J., Cheng, M.M., Zhang, Y., Guo, W.: Icnet: Intra-saliency correlation network for co-saliency detection. Adv. Neural Inform. Process. Syst. (2020)\n\n14. Kim, S., Min, J., Cho, M.: Transformatcher: Match-to-match attention for semantic correspondence. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 8697–8707 (2022)\n\n15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\n16. Li, L., Han, J., Zhang, N., Liu, N., Khan, S., Cholakkal, H., Anwer, R.M., Khan, F.S.: Discriminative co-saliency and background mining transformer for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 7247–7256 (2023)\n\n17. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 2117–2125 (2017)\n\n18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Eur. Conf. Comput. Vis. pp. 740–755 (2014)",
      "images": [
        {
          "name": "page_15.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_15_listItem_1.jpg",
          "height": 31.296,
          "width": 341.895,
          "x": 138.826,
          "y": 333.2,
          "original_width": 690,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_2.jpg",
          "height": 31.359,
          "width": 341.549,
          "x": 138.963,
          "y": 199.486,
          "original_width": 689,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_3.jpg",
          "height": 30.699,
          "width": 346.343,
          "x": 134.494,
          "y": 422.513,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_4.jpg",
          "height": 31.525,
          "width": 341.896,
          "x": 139.032,
          "y": 299.747,
          "original_width": 690,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_5.jpg",
          "height": 31.36,
          "width": 345.89,
          "x": 134.322,
          "y": 600.557,
          "original_width": 698,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_6.jpg",
          "height": 30.639,
          "width": 341.587,
          "x": 139.029,
          "y": 166.678,
          "original_width": 689,
          "original_height": 61,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_7.jpg",
          "height": 30.928,
          "width": 346.209,
          "x": 134.697,
          "y": 389.517,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_8.jpg",
          "height": 30.694,
          "width": 346.058,
          "x": 134.342,
          "y": 455.92,
          "original_width": 698,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_9.jpg",
          "height": 41.621,
          "width": 346.541,
          "x": 134.266,
          "y": 556.473,
          "original_width": 699,
          "original_height": 84,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_10.jpg",
          "height": 20.264,
          "width": 341.574,
          "x": 138.906,
          "y": 277.847,
          "original_width": 689,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_11.jpg",
          "height": 20.167,
          "width": 341.645,
          "x": 139.015,
          "y": 255.664,
          "original_width": 689,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_12.jpg",
          "height": 20.316,
          "width": 341.929,
          "x": 138.94,
          "y": 232.971,
          "original_width": 690,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_13.jpg",
          "height": 31.304,
          "width": 346.31,
          "x": 134.261,
          "y": 633.963,
          "original_width": 699,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_14.jpg",
          "height": 19.772,
          "width": 345.968,
          "x": 134.525,
          "y": 534.043,
          "original_width": 698,
          "original_height": 39,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_15.jpg",
          "height": 20.205,
          "width": 341.948,
          "x": 138.619,
          "y": 366.522,
          "original_width": 690,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_16.jpg",
          "height": 20.893,
          "width": 340.937,
          "x": 139.327,
          "y": 143.8,
          "original_width": 688,
          "original_height": 42,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_listItem_17.jpg",
          "height": 19.799,
          "width": 345.935,
          "x": 134.362,
          "y": 511.912,
          "original_width": 698,
          "original_height": 39,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_sectionHeader_1.jpg",
          "height": 9.161,
          "width": 63.385,
          "x": 134.172,
          "y": 116.648,
          "original_width": 128,
          "original_height": 18,
          "type": "layout_sectionHeader"
        },
        {
          "name": "page_15_listItem_18.jpg",
          "height": 20.707,
          "width": 345.863,
          "x": 134.452,
          "y": 489.144,
          "original_width": 698,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_15_pageHeader_1.jpg",
          "height": 7.98,
          "width": 34.442,
          "x": 413.163,
          "y": 92.646,
          "original_width": 69,
          "original_height": 16,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_15_pageHeader_2.jpg",
          "height": 7.203,
          "width": 9.702,
          "x": 470.864,
          "y": 93.198,
          "original_width": 19,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "heading",
          "lvl": 1,
          "value": "References",
          "md": "# References",
          "bBox": {
            "x": 134,
            "y": 114.04,
            "w": 63,
            "h": 12
          }
        },
        {
          "type": "text",
          "value": "1. Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.: Frequency-tuned salient region detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 1597–1604 (2009)\n\n2. Chen, W., Xu, H., Zhou, Z., Liu, Y., Sun, B., Kang, W., Xie, X.: Cost-former: Cost transformer for cost aggregation in multi-view stereo. arXiv preprint arXiv:2305.10320 (2023)\n\n3. Cheng, M.M., Warrell, J., Lin, W.Y., Zheng, S., Vineet, V., Crook, N.: Efficient salient region detection with soft image abstraction. In: Int. Conf. Comput. Vis. pp. 1529–1536 (2013)\n\n4. Cho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., Kim, S.: Cats: Cost aggregation transformers for visual correspondence 34, 9011–9023 (2021)\n\n5. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: A new way to evaluate foreground maps. In: Int. Conf. Comput. Vis. pp. 4548–4557 (2017)\n\n6. Fan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.: Enhanced-alignment measure for binary foreground map evaluation. In: IJCAI. pp. 698–704 (2018)\n\n7. Fan, D.P., Li, T., Lin, Z., Ji, G.P., Zhang, D., Cheng, M.M., Fu, H., Shen, J.: Re-thinking co-salient object detection. IEEE Trans. Pattern Anal. Mach. Intell. 44(8), 4339–4354 (2021)\n\n8. Fan, Q., Fan, D.P., Fu, H., Tang, C.K., Shao, L., Tai, Y.W.: Group collaborative learning for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 12288–12298 (2021)\n\n9. Fu, H., Cao, X., Tu, Z.: Cluster-based co-saliency detection. IEEE Trans. Image Process. 22(10), 3766–3778 (2013)\n\n10. Han, J., Cheng, G., Li, Z., Zhang, D.: A unified metric learning-based framework for co-saliency detection. IEEE Trans. Circuit Syst. Video Technol. 28(10), 2473–2483 (2017)\n\n11. Hong, S., Cho, S., Nam, J., Lin, S., Kim, S.: Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 108–126 (2022)\n\n12. Hong, S., Nam, J., Cho, S., Hong, S., Jeon, S., Min, D., Kim, S.: Neural matching fields: Implicit representation of matching fields for visual correspondence 35, 13512–13526 (2022)\n\n13. Jin, W.D., Xu, J., Cheng, M.M., Zhang, Y., Guo, W.: Icnet: Intra-saliency correlation network for co-saliency detection. Adv. Neural Inform. Process. Syst. (2020)\n\n14. Kim, S., Min, J., Cho, M.: Transformatcher: Match-to-match attention for semantic correspondence. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 8697–8707 (2022)\n\n15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\n16. Li, L., Han, J., Zhang, N., Liu, N., Khan, S., Cholakkal, H., Anwer, R.M., Khan, F.S.: Discriminative co-saliency and background mining transformer for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 7247–7256 (2023)\n\n17. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 2117–2125 (2017)\n\n18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Eur. Conf. Comput. Vis. pp. 740–755 (2014)",
          "md": "1. Achanta, R., Hemami, S., Estrada, F., Susstrunk, S.: Frequency-tuned salient region detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 1597–1604 (2009)\n\n2. Chen, W., Xu, H., Zhou, Z., Liu, Y., Sun, B., Kang, W., Xie, X.: Cost-former: Cost transformer for cost aggregation in multi-view stereo. arXiv preprint arXiv:2305.10320 (2023)\n\n3. Cheng, M.M., Warrell, J., Lin, W.Y., Zheng, S., Vineet, V., Crook, N.: Efficient salient region detection with soft image abstraction. In: Int. Conf. Comput. Vis. pp. 1529–1536 (2013)\n\n4. Cho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., Kim, S.: Cats: Cost aggregation transformers for visual correspondence 34, 9011–9023 (2021)\n\n5. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: A new way to evaluate foreground maps. In: Int. Conf. Comput. Vis. pp. 4548–4557 (2017)\n\n6. Fan, D.P., Gong, C., Cao, Y., Ren, B., Cheng, M.M., Borji, A.: Enhanced-alignment measure for binary foreground map evaluation. In: IJCAI. pp. 698–704 (2018)\n\n7. Fan, D.P., Li, T., Lin, Z., Ji, G.P., Zhang, D., Cheng, M.M., Fu, H., Shen, J.: Re-thinking co-salient object detection. IEEE Trans. Pattern Anal. Mach. Intell. 44(8), 4339–4354 (2021)\n\n8. Fan, Q., Fan, D.P., Fu, H., Tang, C.K., Shao, L., Tai, Y.W.: Group collaborative learning for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 12288–12298 (2021)\n\n9. Fu, H., Cao, X., Tu, Z.: Cluster-based co-saliency detection. IEEE Trans. Image Process. 22(10), 3766–3778 (2013)\n\n10. Han, J., Cheng, G., Li, Z., Zhang, D.: A unified metric learning-based framework for co-saliency detection. IEEE Trans. Circuit Syst. Video Technol. 28(10), 2473–2483 (2017)\n\n11. Hong, S., Cho, S., Nam, J., Lin, S., Kim, S.: Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 108–126 (2022)\n\n12. Hong, S., Nam, J., Cho, S., Hong, S., Jeon, S., Min, D., Kim, S.: Neural matching fields: Implicit representation of matching fields for visual correspondence 35, 13512–13526 (2022)\n\n13. Jin, W.D., Xu, J., Cheng, M.M., Zhang, Y., Guo, W.: Icnet: Intra-saliency correlation network for co-saliency detection. Adv. Neural Inform. Process. Syst. (2020)\n\n14. Kim, S., Min, J., Cho, M.: Transformatcher: Match-to-match attention for semantic correspondence. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 8697–8707 (2022)\n\n15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\n16. Li, L., Han, J., Zhang, N., Liu, N., Khan, S., Cholakkal, H., Anwer, R.M., Khan, F.S.: Discriminative co-saliency and background mining transformer for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 7247–7256 (2023)\n\n17. Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 2117–2125 (2017)\n\n18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Eur. Conf. Comput. Vis. pp. 740–755 (2014)",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 347,
            "h": 572
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [
        {
          "url": "http://arxiv.org/abs/2305.10320",
          "text": "arXiv:2305.10320 (2023) 3. Cheng, M.M., Warrell, J., Lin, W.Y., Zheng, S., Vineet, V., Crook, N.: Efficient"
        },
        {
          "url": "http://arxiv.org/abs/1412.6980",
          "text": "arXiv:1412.6980 (2014) 16. Li, L., Han, J., Zhang, N., Liu, N., Khan, S., Cholakkal, H., Anwer, R.M., Khan,"
        }
      ],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.994,
      "layout": [
        {
          "image": "page_15_listItem_1.jpg",
          "confidence": 0.972,
          "label": "listItem",
          "bbox": {
            "x": 0.226,
            "y": 0.419,
            "w": 0.558,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_2.jpg",
          "confidence": 0.971,
          "label": "listItem",
          "bbox": {
            "x": 0.227,
            "y": 0.25,
            "w": 0.558,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_3.jpg",
          "confidence": 0.97,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.531,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_4.jpg",
          "confidence": 0.969,
          "label": "listItem",
          "bbox": {
            "x": 0.227,
            "y": 0.377,
            "w": 0.558,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_5.jpg",
          "confidence": 0.969,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.756,
            "w": 0.565,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_6.jpg",
          "confidence": 0.968,
          "label": "listItem",
          "bbox": {
            "x": 0.227,
            "y": 0.208,
            "w": 0.558,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_7.jpg",
          "confidence": 0.968,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.489,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_8.jpg",
          "confidence": 0.968,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.573,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_9.jpg",
          "confidence": 0.968,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.7,
            "w": 0.566,
            "h": 0.054
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_10.jpg",
          "confidence": 0.966,
          "label": "listItem",
          "bbox": {
            "x": 0.226,
            "y": 0.348,
            "w": 0.558,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_11.jpg",
          "confidence": 0.965,
          "label": "listItem",
          "bbox": {
            "x": 0.227,
            "y": 0.32,
            "w": 0.558,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_12.jpg",
          "confidence": 0.965,
          "label": "listItem",
          "bbox": {
            "x": 0.227,
            "y": 0.292,
            "w": 0.558,
            "h": 0.026
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_13.jpg",
          "confidence": 0.963,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.799,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_14.jpg",
          "confidence": 0.962,
          "label": "listItem",
          "bbox": {
            "x": 0.219,
            "y": 0.674,
            "w": 0.565,
            "h": 0.024
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_15.jpg",
          "confidence": 0.961,
          "label": "listItem",
          "bbox": {
            "x": 0.226,
            "y": 0.46,
            "w": 0.558,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_16.jpg",
          "confidence": 0.957,
          "label": "listItem",
          "bbox": {
            "x": 0.227,
            "y": 0.18,
            "w": 0.557,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_17.jpg",
          "confidence": 0.951,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.643,
            "w": 0.565,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_sectionHeader_1.jpg",
          "confidence": 0.945,
          "label": "sectionHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.147,
            "w": 0.103,
            "h": 0.011
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_listItem_18.jpg",
          "confidence": 0.94,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.616,
            "w": 0.565,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_pageHeader_1.jpg",
          "confidence": 0.918,
          "label": "pageHeader",
          "bbox": {
            "x": 0.675,
            "y": 0.116,
            "w": 0.056,
            "h": 0.01
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_15_pageHeader_2.jpg",
          "confidence": 0.886,
          "label": "pageHeader",
          "bbox": {
            "x": 0.769,
            "y": 0.117,
            "w": 0.015,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 16,
      "text": "     16      L Li et al.\n\n 19. Liu, J.J., Hou, Q., Cheng, M.M., Feng, J., Jiang, J.: A simple pooling-based design\n         for real-time salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog.\n         pp. 3917–3926 (2019)\n     20. Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object\n         detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 678–686 (2016)\n     21. Liu, N., Han, J., Yang, M.H.: Picanet: Learning pixel-wise contextual attention\n         for saliency detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3089–3098\n         (2018)\n 22. Liu, N., Luo, Z., Zhang, N., Han, J.: Vst++: Efficient and stronger visual saliency\n         transformer. IEEE Trans. Pattern Anal. Mach. Intell. (2024)\n    23. Liu, N., Zhang, N., Wan, K., Shao, L., Han, J.: Visual saliency transformer. In:\n         Int. Conf. Comput. Vis. pp. 4722–4732 (2021)\n       24. Liu, Y., Zhu, L., Yamada, M., Yang, Y.: Semantic correspondence as an optimal\n         transport problem. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4463–4472\n         (2020)\n   25. Liu, Y., Liu, N., Yao, X., Han, J.: Intermediate prototype mining transformer for\n         few-shot semantic segmentation 35, 38020–38031 (2022)\n      26. Luo, Z., Liu, N., Zhao, W., Yang, X., Zhang, D., Fan, D.P., Khan, F., Han, J.:\n         Vscode: General visual salient and camouflaged object detection with 2d prompt\n         learning. In: IEEE Conf. Comput. Vis. Pattern Recog. (2024)\n     27. Min, J., Kang, D., Cho, M.: Hypercorrelation squeeze for few-shot segmentation.\n         In: Int. Conf. Comput. Vis. pp. 6941–6952 (2021)\n    28. Moon, S., Sohn, S.S., Zhou, H., Yoon, S., Pavlovic, V., Khan, M.H., Kapadia, M.:\n         Hm: Hybrid masking for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp.\n         506–523. Springer (2022)\n     29. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\n         T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-\n         performance deep learning library 32 (2019)\n        30. Shen, X., Darmon, F., Efros, A.A., Aubry, M.: Ransac-flow: generic two-stage\n         image alignment. In: Eur. Conf. Comput. Vis. pp. 618–637. Springer (2020)\n    31. Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H.,\n         Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow\n         estimation. arXiv preprint arXiv:2303.08340 (2023)\n       32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\n         image recognition. In: Int. Conf. Learn. Represent. (2015)\n 33. Su, Y., Deng, J., Sun, R., Lin, G., Su, H., Wu, Q.: A unified transformer framework\n         for group-based segmentation: Co-segmentation, co-saliency detection and video\n         salient object detection. IEEE Trans. Multimedia (2023)\n    34. Sun, G., Liu, Y., Tang, H., Chhatkuli, A., Zhang, L., Van Gool, L.: Mining rela-\n         tions among cross-frame affinities for video semantic segmentation. In: Eur. Conf.\n         Comput. Vis. pp. 522–539. Springer (2022)\n  35. Truong, P., Danelljan, M., Yu, F., Van Gool, L.: Warp consistency for unsupervised\n         learning of dense correspondences. In: Int. Conf. Comput. Vis. pp. 10346–10356\n         (2021)\n36. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n         Ł., Polosukhin, I.: Attention is all you need. Adv. Neural Inform. Process. Syst. 30\n         (2017)\n 37. Wang, C., Zha, Z.J., Liu, D., Xie, H.: Robust deep co-saliency detection with group\n         semantic. In: AAAI. vol. 33, pp. 8917–8924 (2019)",
      "md": "16      L Li et al.\n\n19. Liu, J.J., Hou, Q., Cheng, M.M., Feng, J., Jiang, J.: A simple pooling-based design for real-time salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3917–3926 (2019)\n\n20. Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 678–686 (2016)\n\n21. Liu, N., Han, J., Yang, M.H.: Picanet: Learning pixel-wise contextual attention for saliency detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3089–3098 (2018)\n\n22. Liu, N., Luo, Z., Zhang, N., Han, J.: Vst++: Efficient and stronger visual saliency transformer. IEEE Trans. Pattern Anal. Mach. Intell. (2024)\n\n23. Liu, N., Zhang, N., Wan, K., Shao, L., Han, J.: Visual saliency transformer. In: Int. Conf. Comput. Vis. pp. 4722–4732 (2021)\n\n24. Liu, Y., Zhu, L., Yamada, M., Yang, Y.: Semantic correspondence as an optimal transport problem. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4463–4472 (2020)\n\n25. Liu, Y., Liu, N., Yao, X., Han, J.: Intermediate prototype mining transformer for few-shot semantic segmentation 35, 38020–38031 (2022)\n\n26. Luo, Z., Liu, N., Zhao, W., Yang, X., Zhang, D., Fan, D.P., Khan, F., Han, J.: Vscode: General visual salient and camouflaged object detection with 2d prompt learning. In: IEEE Conf. Comput. Vis. Pattern Recog. (2024)\n\n27. Min, J., Kang, D., Cho, M.: Hypercorrelation squeeze for few-shot segmentation. In: Int. Conf. Comput. Vis. pp. 6941–6952 (2021)\n\n28. Moon, S., Sohn, S.S., Zhou, H., Yoon, S., Pavlovic, V., Khan, M.H., Kapadia, M.: Hm: Hybrid masking for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 506–523. Springer (2022)\n\n29. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library 32 (2019)\n\n30. Shen, X., Darmon, F., Efros, A.A., Aubry, M.: Ransac-flow: generic two-stage image alignment. In: Eur. Conf. Comput. Vis. pp. 618–637. Springer (2020)\n\n31. Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H., Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. arXiv preprint arXiv:2303.08340 (2023)\n\n32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: Int. Conf. Learn. Represent. (2015)\n\n33. Su, Y., Deng, J., Sun, R., Lin, G., Su, H., Wu, Q.: A unified transformer framework for group-based segmentation: Co-segmentation, co-saliency detection and video salient object detection. IEEE Trans. Multimedia (2023)\n\n34. Sun, G., Liu, Y., Tang, H., Chhatkuli, A., Zhang, L., Van Gool, L.: Mining relations among cross-frame affinities for video semantic segmentation. In: Eur. Conf. Comput. Vis. pp. 522–539. Springer (2022)\n\n35. Truong, P., Danelljan, M., Yu, F., Van Gool, L.: Warp consistency for unsupervised learning of dense correspondences. In: Int. Conf. Comput. Vis. pp. 10346–10356 (2021)\n\n36. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Adv. Neural Inform. Process. Syst. 30 (2017)\n\n37. Wang, C., Zha, Z.J., Liu, D., Xie, H.: Robust deep co-saliency detection with group semantic. In: AAAI. vol. 33, pp. 8917–8924 (2019)",
      "images": [
        {
          "name": "page_16.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_16_listItem_1.jpg",
          "height": 31.021,
          "width": 346.446,
          "x": 134.211,
          "y": 455.036,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_2.jpg",
          "height": 31.026,
          "width": 346.681,
          "x": 134.247,
          "y": 510.947,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_3.jpg",
          "height": 30.955,
          "width": 346.693,
          "x": 134.263,
          "y": 253.268,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_4.jpg",
          "height": 30.844,
          "width": 346.554,
          "x": 134.264,
          "y": 365.433,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_5.jpg",
          "height": 30.74,
          "width": 346.73,
          "x": 134.244,
          "y": 611.343,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_6.jpg",
          "height": 20.34,
          "width": 346.341,
          "x": 134.311,
          "y": 432.52,
          "original_width": 699,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_7.jpg",
          "height": 31.78,
          "width": 346.723,
          "x": 134.193,
          "y": 309.28,
          "original_width": 700,
          "original_height": 64,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_8.jpg",
          "height": 30.92,
          "width": 346.718,
          "x": 134.389,
          "y": 174.768,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_9.jpg",
          "height": 31.396,
          "width": 346.456,
          "x": 134.107,
          "y": 544.485,
          "original_width": 699,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_10.jpg",
          "height": 30.818,
          "width": 346.386,
          "x": 134.225,
          "y": 399.076,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_11.jpg",
          "height": 20.154,
          "width": 346.485,
          "x": 134.394,
          "y": 286.626,
          "original_width": 699,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_12.jpg",
          "height": 31.183,
          "width": 346.663,
          "x": 134.448,
          "y": 118.756,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_13.jpg",
          "height": 20.547,
          "width": 346.309,
          "x": 134.267,
          "y": 208.108,
          "original_width": 699,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_14.jpg",
          "height": 20.323,
          "width": 346.864,
          "x": 134.057,
          "y": 644.959,
          "original_width": 700,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_15.jpg",
          "height": 30.535,
          "width": 346.721,
          "x": 134.172,
          "y": 577.997,
          "original_width": 700,
          "original_height": 61,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_16.jpg",
          "height": 20.962,
          "width": 346.203,
          "x": 134.309,
          "y": 488.178,
          "original_width": 699,
          "original_height": 42,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_17.jpg",
          "height": 20.396,
          "width": 346.779,
          "x": 134.208,
          "y": 230.919,
          "original_width": 700,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_18.jpg",
          "height": 19.975,
          "width": 345.917,
          "x": 134.395,
          "y": 342.824,
          "original_width": 698,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_listItem_19.jpg",
          "height": 20.601,
          "width": 346.747,
          "x": 134.092,
          "y": 152.307,
          "original_width": 700,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_16_pageHeader_1.jpg",
          "height": 7.762,
          "width": 41.553,
          "x": 167.31,
          "y": 92.781,
          "original_width": 83,
          "original_height": 15,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_16_pageHeader_2.jpg",
          "height": 6.966,
          "width": 9.95,
          "x": 134.418,
          "y": 93.246,
          "original_width": 20,
          "original_height": 14,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "16      L Li et al.\n\n19. Liu, J.J., Hou, Q., Cheng, M.M., Feng, J., Jiang, J.: A simple pooling-based design for real-time salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3917–3926 (2019)\n\n20. Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 678–686 (2016)\n\n21. Liu, N., Han, J., Yang, M.H.: Picanet: Learning pixel-wise contextual attention for saliency detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3089–3098 (2018)\n\n22. Liu, N., Luo, Z., Zhang, N., Han, J.: Vst++: Efficient and stronger visual saliency transformer. IEEE Trans. Pattern Anal. Mach. Intell. (2024)\n\n23. Liu, N., Zhang, N., Wan, K., Shao, L., Han, J.: Visual saliency transformer. In: Int. Conf. Comput. Vis. pp. 4722–4732 (2021)\n\n24. Liu, Y., Zhu, L., Yamada, M., Yang, Y.: Semantic correspondence as an optimal transport problem. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4463–4472 (2020)\n\n25. Liu, Y., Liu, N., Yao, X., Han, J.: Intermediate prototype mining transformer for few-shot semantic segmentation 35, 38020–38031 (2022)\n\n26. Luo, Z., Liu, N., Zhao, W., Yang, X., Zhang, D., Fan, D.P., Khan, F., Han, J.: Vscode: General visual salient and camouflaged object detection with 2d prompt learning. In: IEEE Conf. Comput. Vis. Pattern Recog. (2024)\n\n27. Min, J., Kang, D., Cho, M.: Hypercorrelation squeeze for few-shot segmentation. In: Int. Conf. Comput. Vis. pp. 6941–6952 (2021)\n\n28. Moon, S., Sohn, S.S., Zhou, H., Yoon, S., Pavlovic, V., Khan, M.H., Kapadia, M.: Hm: Hybrid masking for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 506–523. Springer (2022)\n\n29. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library 32 (2019)\n\n30. Shen, X., Darmon, F., Efros, A.A., Aubry, M.: Ransac-flow: generic two-stage image alignment. In: Eur. Conf. Comput. Vis. pp. 618–637. Springer (2020)\n\n31. Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H., Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. arXiv preprint arXiv:2303.08340 (2023)\n\n32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: Int. Conf. Learn. Represent. (2015)\n\n33. Su, Y., Deng, J., Sun, R., Lin, G., Su, H., Wu, Q.: A unified transformer framework for group-based segmentation: Co-segmentation, co-saliency detection and video salient object detection. IEEE Trans. Multimedia (2023)\n\n34. Sun, G., Liu, Y., Tang, H., Chhatkuli, A., Zhang, L., Van Gool, L.: Mining relations among cross-frame affinities for video semantic segmentation. In: Eur. Conf. Comput. Vis. pp. 522–539. Springer (2022)\n\n35. Truong, P., Danelljan, M., Yu, F., Van Gool, L.: Warp consistency for unsupervised learning of dense correspondences. In: Int. Conf. Comput. Vis. pp. 10346–10356 (2021)\n\n36. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Adv. Neural Inform. Process. Syst. 30 (2017)\n\n37. Wang, C., Zha, Z.J., Liu, D., Xie, H.: Robust deep co-saliency detection with group semantic. In: AAAI. vol. 33, pp. 8917–8924 (2019)",
          "md": "16      L Li et al.\n\n19. Liu, J.J., Hou, Q., Cheng, M.M., Feng, J., Jiang, J.: A simple pooling-based design for real-time salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3917–3926 (2019)\n\n20. Liu, N., Han, J.: Dhsnet: Deep hierarchical saliency network for salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 678–686 (2016)\n\n21. Liu, N., Han, J., Yang, M.H.: Picanet: Learning pixel-wise contextual attention for saliency detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 3089–3098 (2018)\n\n22. Liu, N., Luo, Z., Zhang, N., Han, J.: Vst++: Efficient and stronger visual saliency transformer. IEEE Trans. Pattern Anal. Mach. Intell. (2024)\n\n23. Liu, N., Zhang, N., Wan, K., Shao, L., Han, J.: Visual saliency transformer. In: Int. Conf. Comput. Vis. pp. 4722–4732 (2021)\n\n24. Liu, Y., Zhu, L., Yamada, M., Yang, Y.: Semantic correspondence as an optimal transport problem. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4463–4472 (2020)\n\n25. Liu, Y., Liu, N., Yao, X., Han, J.: Intermediate prototype mining transformer for few-shot semantic segmentation 35, 38020–38031 (2022)\n\n26. Luo, Z., Liu, N., Zhao, W., Yang, X., Zhang, D., Fan, D.P., Khan, F., Han, J.: Vscode: General visual salient and camouflaged object detection with 2d prompt learning. In: IEEE Conf. Comput. Vis. Pattern Recog. (2024)\n\n27. Min, J., Kang, D., Cho, M.: Hypercorrelation squeeze for few-shot segmentation. In: Int. Conf. Comput. Vis. pp. 6941–6952 (2021)\n\n28. Moon, S., Sohn, S.S., Zhou, H., Yoon, S., Pavlovic, V., Khan, M.H., Kapadia, M.: Hm: Hybrid masking for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 506–523. Springer (2022)\n\n29. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library 32 (2019)\n\n30. Shen, X., Darmon, F., Efros, A.A., Aubry, M.: Ransac-flow: generic two-stage image alignment. In: Eur. Conf. Comput. Vis. pp. 618–637. Springer (2020)\n\n31. Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H., Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. arXiv preprint arXiv:2303.08340 (2023)\n\n32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: Int. Conf. Learn. Represent. (2015)\n\n33. Su, Y., Deng, J., Sun, R., Lin, G., Su, H., Wu, Q.: A unified transformer framework for group-based segmentation: Co-segmentation, co-saliency detection and video salient object detection. IEEE Trans. Multimedia (2023)\n\n34. Sun, G., Liu, Y., Tang, H., Chhatkuli, A., Zhang, L., Van Gool, L.: Mining relations among cross-frame affinities for video semantic segmentation. In: Eur. Conf. Comput. Vis. pp. 522–539. Springer (2022)\n\n35. Truong, P., Danelljan, M., Yu, F., Van Gool, L.: Warp consistency for unsupervised learning of dense correspondences. In: Int. Conf. Comput. Vis. pp. 10346–10356 (2021)\n\n36. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Adv. Neural Inform. Process. Syst. 30 (2017)\n\n37. Wang, C., Zha, Z.J., Liu, D., Xie, H.: Robust deep co-saliency detection with group semantic. In: AAAI. vol. 33, pp. 8917–8924 (2019)",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 346,
            "h": 572
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [
        {
          "url": "http://arxiv.org/abs/2303.08340",
          "text": "32. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale"
        }
      ],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.998,
      "layout": [
        {
          "image": "page_16_listItem_1.jpg",
          "confidence": 0.969,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.572,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_2.jpg",
          "confidence": 0.969,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.642,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_3.jpg",
          "confidence": 0.967,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.318,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_4.jpg",
          "confidence": 0.967,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.459,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_5.jpg",
          "confidence": 0.966,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.77,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_6.jpg",
          "confidence": 0.965,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.544,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_7.jpg",
          "confidence": 0.964,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.388,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_8.jpg",
          "confidence": 0.963,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.218,
            "w": 0.567,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_9.jpg",
          "confidence": 0.961,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.685,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_10.jpg",
          "confidence": 0.961,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.501,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_11.jpg",
          "confidence": 0.96,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.359,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_12.jpg",
          "confidence": 0.959,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.147,
            "w": 0.567,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_13.jpg",
          "confidence": 0.959,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.261,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_14.jpg",
          "confidence": 0.958,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.811,
            "w": 0.566,
            "h": 0.028
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_15.jpg",
          "confidence": 0.956,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.727,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_16.jpg",
          "confidence": 0.954,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.614,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_17.jpg",
          "confidence": 0.954,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.289,
            "w": 0.566,
            "h": 0.028
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_18.jpg",
          "confidence": 0.953,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.43,
            "w": 0.565,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_listItem_19.jpg",
          "confidence": 0.951,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.19,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_pageHeader_1.jpg",
          "confidence": 0.755,
          "label": "pageHeader",
          "bbox": {
            "x": 0.273,
            "y": 0.117,
            "w": 0.067,
            "h": 0.009
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_16_pageHeader_2.jpg",
          "confidence": 0.695,
          "label": "pageHeader",
          "bbox": {
            "x": 0.219,
            "y": 0.117,
            "w": 0.016,
            "h": 0.008
          },
          "isLikelyNoise": false
        }
      ]
    },
    {
      "page": 17,
      "text": "                                                                        CONDA         17\n\n     38. Wu, Y., Song, H., Liu, B., Zhang, K., Liu, D.: Co-salient object detection with\n          uncertainty-aware group exchange-masking. In: IEEE Conf. Comput. Vis. Pattern\n          Recog. pp. 19639–19648 (2023)\n    39. Xiong, Z., Li, H., Zhu, X.X.: Doubly deformable aggregation of covariance matri-\n          ces for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 133–150. Springer\n          (2022)\n        40. Xu, G., Wang, X., Ding, X., Yang, X.: Iterative geometry encoding volume for\n          stereo matching. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 21919–21928\n          (2023)\n        41. Yu, S., Xiao, J., Zhang, B., Lim, E.G.: Democracy does matter: Comprehensive\n          feature mining for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern\n          Recog. pp. 979–988 (2022)\n    42. Zhang, D., Fu, H., Han, J., Borji, A., Li, X.: A review of co-saliency detection\n          algorithms: Fundamentals, applications, and challenges. ACM Trans Intell Syst\n          Technol 9(4), 1–31 (2018)\n  43. Zhang, D., Han, J., Han, J., Shao, L.: Cosaliency detection based on intrasaliency\n          prior transfer and deep intersaliency mining. IEEE Trans. Neural Networks Learn.\n          Syst. 27(6), 1163–1176 (2015)\n    44. Zhang, D., Han, J., Li, C., Wang, J., Li, X.: Detection of co-salient objects by\n          looking deep and wide. Int. J. Comput. Vis. 120(2), 215–232 (2016)\n      45. Zhang, D., Meng, D., Han, J.: Co-saliency detection via a self-paced multiple-\n          instance learning framework. IEEE Trans. Pattern Anal. Mach. Intell. 39(5), 865–\n          878 (2016)\n    46. Zhang, N., Han, J., Liu, N., Shao, L.: Summarize and search: Learning consensus-\n          aware dynamic convolution for co-saliency detection. In: Int. Conf. Comput. Vis.\n          pp. 4167–4176 (2021)\n    47. Zhang, Z., Jin, W., Xu, J., Cheng, M.M.: Gradient-induced co-saliency detection.\n          In: Eur. Conf. Comput. Vis. pp. 455–472 (2020)\n 48. Zhao, D., Song, Z., Ji, Z., Zhao, G., Ge, W., Yu, Y.: Multi-scale matching networks\n          for semantic correspondence. In: Int. Conf. Comput. Vis. pp. 3354–3364 (2021)\n       49. Zhao, J.X., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: Egnet: Edge\n          guidance network for salient object detection. In: Int. Conf. Comput. Vis. pp.\n          8779–8788 (2019)\n  50. Zhao, W., Zhang, J., Li, L., Barnes, N., Liu, N., Han, J.: Weakly supervised video\n          salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 16826–\n          16835 (2021)\n51. Zheng, P., Fu, H., Fan, D.P., Fan, Q., Qin, J., Tai, Y.W., Tang, C.K., Van Gool, L.:\n          Gconet+: A stronger group collaborative co-salient object detector. IEEE Trans.\n          Pattern Anal. Mach. Intell. (2023)\n52. Zhuge, M., Fan, D.P., Liu, N., Zhang, D., Xu, D., Shao, L.: Salient object detection\n          via integrity learning. IEEE Trans. Pattern Anal. Mach. Intell. 45(3), 3738–3752\n          (2022)",
      "md": "38. Wu, Y., Song, H., Liu, B., Zhang, K., Liu, D.: Co-salient object detection with uncertainty-aware group exchange-masking. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 19639–19648 (2023)\n\n39. Xiong, Z., Li, H., Zhu, X.X.: Doubly deformable aggregation of covariance matrices for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 133–150. Springer (2022)\n\n40. Xu, G., Wang, X., Ding, X., Yang, X.: Iterative geometry encoding volume for stereo matching. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 21919–21928 (2023)\n\n41. Yu, S., Xiao, J., Zhang, B., Lim, E.G.: Democracy does matter: Comprehensive feature mining for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 979–988 (2022)\n\n42. Zhang, D., Fu, H., Han, J., Borji, A., Li, X.: A review of co-saliency detection algorithms: Fundamentals, applications, and challenges. ACM Trans Intell Syst Technol 9(4), 1–31 (2018)\n\n43. Zhang, D., Han, J., Han, J., Shao, L.: Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining. IEEE Trans. Neural Networks Learn. Syst. 27(6), 1163–1176 (2015)\n\n44. Zhang, D., Han, J., Li, C., Wang, J., Li, X.: Detection of co-salient objects by looking deep and wide. Int. J. Comput. Vis. 120(2), 215–232 (2016)\n\n45. Zhang, D., Meng, D., Han, J.: Co-saliency detection via a self-paced multiple-instance learning framework. IEEE Trans. Pattern Anal. Mach. Intell. 39(5), 865–878 (2016)\n\n46. Zhang, N., Han, J., Liu, N., Shao, L.: Summarize and search: Learning consensus-aware dynamic convolution for co-saliency detection. In: Int. Conf. Comput. Vis. pp. 4167–4176 (2021)\n\n47. Zhang, Z., Jin, W., Xu, J., Cheng, M.M.: Gradient-induced co-saliency detection. In: Eur. Conf. Comput. Vis. pp. 455–472 (2020)\n\n48. Zhao, D., Song, Z., Ji, Z., Zhao, G., Ge, W., Yu, Y.: Multi-scale matching networks for semantic correspondence. In: Int. Conf. Comput. Vis. pp. 3354–3364 (2021)\n\n49. Zhao, J.X., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: Egnet: Edge guidance network for salient object detection. In: Int. Conf. Comput. Vis. pp. 8779–8788 (2019)\n\n50. Zhao, W., Zhang, J., Li, L., Barnes, N., Liu, N., Han, J.: Weakly supervised video salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 16826–16835 (2021)\n\n51. Zheng, P., Fu, H., Fan, D.P., Fan, Q., Qin, J., Tai, Y.W., Tang, C.K., Van Gool, L.: Gconet+: A stronger group collaborative co-salient object detector. IEEE Trans. Pattern Anal. Mach. Intell. (2023)\n\n52. Zhuge, M., Fan, D.P., Liu, N., Zhang, D., Xu, D., Shao, L.: Salient object detection via integrity learning. IEEE Trans. Pattern Anal. Mach. Intell. 45(3), 3738–3752 (2022)",
      "images": [
        {
          "name": "page_17.jpg",
          "height": 792,
          "width": 612,
          "x": 0,
          "y": 0,
          "original_width": 1236,
          "original_height": 1600,
          "type": "full_page_screenshot"
        },
        {
          "name": "page_17_listItem_1.jpg",
          "height": 30.941,
          "width": 346.297,
          "x": 134.467,
          "y": 184.468,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_2.jpg",
          "height": 30.987,
          "width": 346.832,
          "x": 134.204,
          "y": 250.431,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_3.jpg",
          "height": 31.302,
          "width": 346.644,
          "x": 134.336,
          "y": 217.261,
          "original_width": 700,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_4.jpg",
          "height": 31.434,
          "width": 346.451,
          "x": 134.338,
          "y": 546.313,
          "original_width": 699,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_5.jpg",
          "height": 31.054,
          "width": 346.118,
          "x": 134.697,
          "y": 283.061,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_6.jpg",
          "height": 30.858,
          "width": 346.719,
          "x": 134.437,
          "y": 480.294,
          "original_width": 700,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_7.jpg",
          "height": 31.203,
          "width": 346.754,
          "x": 134.111,
          "y": 513.029,
          "original_width": 700,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_8.jpg",
          "height": 20.474,
          "width": 346.387,
          "x": 134.467,
          "y": 315.971,
          "original_width": 699,
          "original_height": 41,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_9.jpg",
          "height": 31.098,
          "width": 346.146,
          "x": 134.237,
          "y": 371.028,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_10.jpg",
          "height": 31.3,
          "width": 346.405,
          "x": 134.222,
          "y": 337.77,
          "original_width": 699,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_11.jpg",
          "height": 31.225,
          "width": 346.801,
          "x": 134.161,
          "y": 447.298,
          "original_width": 700,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_12.jpg",
          "height": 19.97,
          "width": 346.03,
          "x": 134.651,
          "y": 425.56,
          "original_width": 698,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_13.jpg",
          "height": 20.151,
          "width": 345.606,
          "x": 134.448,
          "y": 403.729,
          "original_width": 697,
          "original_height": 40,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_14.jpg",
          "height": 31.616,
          "width": 346.93,
          "x": 134.295,
          "y": 118.467,
          "original_width": 700,
          "original_height": 63,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_listItem_15.jpg",
          "height": 31.05,
          "width": 346.482,
          "x": 134.321,
          "y": 151.757,
          "original_width": 699,
          "original_height": 62,
          "type": "layout_listItem"
        },
        {
          "name": "page_17_pageHeader_1.jpg",
          "height": 8.11,
          "width": 34.58,
          "x": 413.067,
          "y": 92.605,
          "original_width": 69,
          "original_height": 16,
          "type": "layout_pageHeader"
        },
        {
          "name": "page_17_pageHeader_2.jpg",
          "height": 7.427,
          "width": 9.79,
          "x": 470.914,
          "y": 92.958,
          "original_width": 19,
          "original_height": 15,
          "type": "layout_pageHeader"
        }
      ],
      "charts": [],
      "items": [
        {
          "type": "text",
          "value": "38. Wu, Y., Song, H., Liu, B., Zhang, K., Liu, D.: Co-salient object detection with uncertainty-aware group exchange-masking. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 19639–19648 (2023)\n\n39. Xiong, Z., Li, H., Zhu, X.X.: Doubly deformable aggregation of covariance matrices for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 133–150. Springer (2022)\n\n40. Xu, G., Wang, X., Ding, X., Yang, X.: Iterative geometry encoding volume for stereo matching. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 21919–21928 (2023)\n\n41. Yu, S., Xiao, J., Zhang, B., Lim, E.G.: Democracy does matter: Comprehensive feature mining for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 979–988 (2022)\n\n42. Zhang, D., Fu, H., Han, J., Borji, A., Li, X.: A review of co-saliency detection algorithms: Fundamentals, applications, and challenges. ACM Trans Intell Syst Technol 9(4), 1–31 (2018)\n\n43. Zhang, D., Han, J., Han, J., Shao, L.: Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining. IEEE Trans. Neural Networks Learn. Syst. 27(6), 1163–1176 (2015)\n\n44. Zhang, D., Han, J., Li, C., Wang, J., Li, X.: Detection of co-salient objects by looking deep and wide. Int. J. Comput. Vis. 120(2), 215–232 (2016)\n\n45. Zhang, D., Meng, D., Han, J.: Co-saliency detection via a self-paced multiple-instance learning framework. IEEE Trans. Pattern Anal. Mach. Intell. 39(5), 865–878 (2016)\n\n46. Zhang, N., Han, J., Liu, N., Shao, L.: Summarize and search: Learning consensus-aware dynamic convolution for co-saliency detection. In: Int. Conf. Comput. Vis. pp. 4167–4176 (2021)\n\n47. Zhang, Z., Jin, W., Xu, J., Cheng, M.M.: Gradient-induced co-saliency detection. In: Eur. Conf. Comput. Vis. pp. 455–472 (2020)\n\n48. Zhao, D., Song, Z., Ji, Z., Zhao, G., Ge, W., Yu, Y.: Multi-scale matching networks for semantic correspondence. In: Int. Conf. Comput. Vis. pp. 3354–3364 (2021)\n\n49. Zhao, J.X., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: Egnet: Edge guidance network for salient object detection. In: Int. Conf. Comput. Vis. pp. 8779–8788 (2019)\n\n50. Zhao, W., Zhang, J., Li, L., Barnes, N., Liu, N., Han, J.: Weakly supervised video salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 16826–16835 (2021)\n\n51. Zheng, P., Fu, H., Fan, D.P., Fan, Q., Qin, J., Tai, Y.W., Tang, C.K., Van Gool, L.: Gconet+: A stronger group collaborative co-salient object detector. IEEE Trans. Pattern Anal. Mach. Intell. (2023)\n\n52. Zhuge, M., Fan, D.P., Liu, N., Zhang, D., Xu, D., Shao, L.: Salient object detection via integrity learning. IEEE Trans. Pattern Anal. Mach. Intell. 45(3), 3738–3752 (2022)",
          "md": "38. Wu, Y., Song, H., Liu, B., Zhang, K., Liu, D.: Co-salient object detection with uncertainty-aware group exchange-masking. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 19639–19648 (2023)\n\n39. Xiong, Z., Li, H., Zhu, X.X.: Doubly deformable aggregation of covariance matrices for few-shot segmentation. In: Eur. Conf. Comput. Vis. pp. 133–150. Springer (2022)\n\n40. Xu, G., Wang, X., Ding, X., Yang, X.: Iterative geometry encoding volume for stereo matching. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 21919–21928 (2023)\n\n41. Yu, S., Xiao, J., Zhang, B., Lim, E.G.: Democracy does matter: Comprehensive feature mining for co-salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 979–988 (2022)\n\n42. Zhang, D., Fu, H., Han, J., Borji, A., Li, X.: A review of co-saliency detection algorithms: Fundamentals, applications, and challenges. ACM Trans Intell Syst Technol 9(4), 1–31 (2018)\n\n43. Zhang, D., Han, J., Han, J., Shao, L.: Cosaliency detection based on intrasaliency prior transfer and deep intersaliency mining. IEEE Trans. Neural Networks Learn. Syst. 27(6), 1163–1176 (2015)\n\n44. Zhang, D., Han, J., Li, C., Wang, J., Li, X.: Detection of co-salient objects by looking deep and wide. Int. J. Comput. Vis. 120(2), 215–232 (2016)\n\n45. Zhang, D., Meng, D., Han, J.: Co-saliency detection via a self-paced multiple-instance learning framework. IEEE Trans. Pattern Anal. Mach. Intell. 39(5), 865–878 (2016)\n\n46. Zhang, N., Han, J., Liu, N., Shao, L.: Summarize and search: Learning consensus-aware dynamic convolution for co-saliency detection. In: Int. Conf. Comput. Vis. pp. 4167–4176 (2021)\n\n47. Zhang, Z., Jin, W., Xu, J., Cheng, M.M.: Gradient-induced co-saliency detection. In: Eur. Conf. Comput. Vis. pp. 455–472 (2020)\n\n48. Zhao, D., Song, Z., Ji, Z., Zhao, G., Ge, W., Yu, Y.: Multi-scale matching networks for semantic correspondence. In: Int. Conf. Comput. Vis. pp. 3354–3364 (2021)\n\n49. Zhao, J.X., Liu, J.J., Fan, D.P., Cao, Y., Yang, J., Cheng, M.M.: Egnet: Edge guidance network for salient object detection. In: Int. Conf. Comput. Vis. pp. 8779–8788 (2019)\n\n50. Zhao, W., Zhang, J., Li, L., Barnes, N., Liu, N., Han, J.: Weakly supervised video salient object detection. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 16826–16835 (2021)\n\n51. Zheng, P., Fu, H., Fan, D.P., Fan, Q., Qin, J., Tai, Y.W., Tang, C.K., Van Gool, L.: Gconet+: A stronger group collaborative co-salient object detector. IEEE Trans. Pattern Anal. Mach. Intell. (2023)\n\n52. Zhuge, M., Fan, D.P., Liu, N., Zhang, D., Xu, D., Shao, L.: Salient object detection via integrity learning. IEEE Trans. Pattern Anal. Mach. Intell. 45(3), 3738–3752 (2022)",
          "bBox": {
            "x": 134,
            "y": 91.03,
            "w": 347,
            "h": 485
          }
        }
      ],
      "status": "OK",
      "originalOrientationAngle": 0,
      "links": [],
      "width": 612,
      "height": 792,
      "triggeredAutoMode": false,
      "parsingMode": "premium",
      "structuredData": null,
      "noStructuredContent": false,
      "noTextContent": false,
      "confidence": 0.998,
      "layout": [
        {
          "image": "page_17_listItem_1.jpg",
          "confidence": 0.971,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.231,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_2.jpg",
          "confidence": 0.97,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.314,
            "w": 0.567,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_3.jpg",
          "confidence": 0.97,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.272,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_4.jpg",
          "confidence": 0.969,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.688,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_5.jpg",
          "confidence": 0.969,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.356,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_6.jpg",
          "confidence": 0.967,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.604,
            "w": 0.567,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_7.jpg",
          "confidence": 0.966,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.646,
            "w": 0.566,
            "h": 0.04
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_8.jpg",
          "confidence": 0.965,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.397,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_9.jpg",
          "confidence": 0.965,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.465,
            "w": 0.565,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_10.jpg",
          "confidence": 0.964,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.424,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_11.jpg",
          "confidence": 0.96,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.563,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_12.jpg",
          "confidence": 0.956,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.535,
            "w": 0.566,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_13.jpg",
          "confidence": 0.955,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.507,
            "w": 0.565,
            "h": 0.027
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_14.jpg",
          "confidence": 0.953,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.147,
            "w": 0.567,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_listItem_15.jpg",
          "confidence": 0.951,
          "label": "listItem",
          "bbox": {
            "x": 0.218,
            "y": 0.189,
            "w": 0.566,
            "h": 0.041
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_pageHeader_1.jpg",
          "confidence": 0.925,
          "label": "pageHeader",
          "bbox": {
            "x": 0.674,
            "y": 0.116,
            "w": 0.056,
            "h": 0.01
          },
          "isLikelyNoise": false
        },
        {
          "image": "page_17_pageHeader_2.jpg",
          "confidence": 0.893,
          "label": "pageHeader",
          "bbox": {
            "x": 0.769,
            "y": 0.117,
            "w": 0.015,
            "h": 0.009
          },
          "isLikelyNoise": false
        }
      ]
    }
  ],
  "job_metadata": {
    "credits_used": 16530,
    "job_credits_usage": 0,
    "job_pages": 0,
    "job_auto_mode_triggered_pages": 0,
    "job_is_cache_hit": true
  }
}