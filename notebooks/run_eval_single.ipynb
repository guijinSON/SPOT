{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d38a60-7d18-43e8-a8ad-d3edcec29a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import litellm\n",
    "from datasets import load_dataset\n",
    "from prompts import llm_reviewer_template, llm_judge_template\n",
    "from litellm import batch_completion, completion\n",
    "from src import extract_response_dict, compute_all_metrics, clean_records\n",
    "\n",
    "os.environ['OPENROUTER_API_KEY'] = \"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\"\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "def compute_all_metrics(resp_df: pd.DataFrame):\n",
    "    # 1) Work on a fresh copy\n",
    "    df = resp_df.copy()\n",
    "\n",
    "    # 2) Ensure 'matches' and 'errors' are always lists\n",
    "    df[\"matches\"] = df[\"matches\"].apply(lambda x: eval(x) if isinstance(x, str) else [])\n",
    "    df[\"match_descriptions\"] = df[\"match_descriptions\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    df[\"errors\"]  = df[\"errors\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    # 3) Add all per-paper counts\n",
    "    df = df.assign(\n",
    "        k_i = df[\"error_annotation\"].apply(lambda x: len(x)),\n",
    "        TP_i = [max(len(m),len(md)) for m,md in df[['matches','match_descriptions']].values],\n",
    "    )\n",
    "    df[\"FP_i\"] = df[\"errors\"].apply(lambda x: len(x)) - df[\"TP_i\"]\n",
    "    df[\"FN_i\"] = df[\"k_i\"] - df[\"TP_i\"]\n",
    "\n",
    "    N = len(df)\n",
    "\n",
    "    # 4) Micro-averaged Precision & Recall\n",
    "    TP_total = df[\"TP_i\"].sum()\n",
    "    FP_total = df[\"FP_i\"].sum()\n",
    "    FN_total = df[\"FN_i\"].sum()\n",
    "    precision_micro = TP_total / (TP_total + FP_total) if (TP_total + FP_total) > 0 else 0.0\n",
    "    recall_micro    = TP_total / (TP_total + FN_total) if (TP_total + FN_total) > 0 else 0.0\n",
    "\n",
    "    # 5) Macro-averaged Precision & Recall (per-paper average)\n",
    "    per_prec = df.apply(\n",
    "        lambda row: row[\"TP_i\"] / (row[\"TP_i\"] + row[\"FP_i\"])\n",
    "        if (row[\"TP_i\"] + row[\"FP_i\"]) > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "    per_rec = df.apply(\n",
    "        lambda row: row[\"TP_i\"] / (row[\"TP_i\"] + row[\"FN_i\"])\n",
    "        if (row[\"TP_i\"] + row[\"FN_i\"]) > 0 else 0.0,\n",
    "        axis=1\n",
    "    )\n",
    "    precision_macro = per_prec.mean()\n",
    "    recall_macro    = per_rec.mean()\n",
    "\n",
    "    # 6) Perfect-Paper Rate (PPR)\n",
    "    perfect_mask = (df[\"TP_i\"] == df[\"k_i\"])\n",
    "    PPR = perfect_mask.sum() / N\n",
    "\n",
    "    # 7) Collect per-paper stats\n",
    "    per_paper = df[[\n",
    "        \"doi/arxiv_id\", \"k_i\", \"TP_i\", \"FP_i\", \"FN_i\"\n",
    "    ]].to_dict(orient=\"records\")\n",
    "\n",
    "    return {\n",
    "        \"N\":                 N,\n",
    "        \"precision_micro\":   precision_micro,\n",
    "        \"recall_micro\":      recall_micro,\n",
    "        \"precision_macro\":   precision_macro,\n",
    "        \"recall_macro\":      recall_macro,\n",
    "        \"PPR\":               PPR,\n",
    "        \"per_paper\":         per_paper\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30184186-ffcc-4f26-99a3-4909446aeee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since amphora/errata_0504_v0 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/amphora___errata_0504_v0/default/0.0.0/8e986e83d3a384bcf1e2d2e8e6ae0348bb6e3a9e (last modified on Mon May  5 03:28:02 2025).\n"
     ]
    }
   ],
   "source": [
    "orig_df = load_dataset(\n",
    "            'amphora/errata_0504_v0',\n",
    "            split='train',\n",
    "            token='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'\n",
    "        ).to_pandas()\n",
    "            \n",
    "list_cols = ['paper_category', 'error_location', 'error_annotation']\n",
    "agg_dict = {\n",
    "    col: (list if col in list_cols else 'first')\n",
    "    for col in orig_df.columns\n",
    "    if col != 'doi/arxiv_id'\n",
    "}\n",
    "flat_orig_df = (\n",
    "    orig_df\n",
    "    .groupby('doi/arxiv_id', as_index=False)\n",
    "    .agg(agg_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e698805-db53-4d46-a5c4-79f68afb91ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses with openrouter/openai/o4-mini-high\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [1:45:19<00:00, 76.14s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses with gpt-4.1\n",
      "Micro Precision: 0.042\n",
      "Micro Recall:    0.132\n",
      "Macro Precision: 0.050\n",
      "Macro Recall:    0.139\n",
      "PPR:             0.108\n"
     ]
    }
   ],
   "source": [
    "model_name = 'openrouter/openai/o4-mini-high'\n",
    "judge_name = \"gpt-4.1\"\n",
    "safe_model_name = model_name.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "# for idx in range(0,8):\n",
    "idx = 3\n",
    "qrys = []\n",
    "papers = []\n",
    "for _,row in flat_orig_df.iterrows():\n",
    "    qry = [\n",
    "            {'role': 'system', 'content': llm_reviewer_template},\n",
    "            {'role': 'user', 'content': clean_records(row.paper_content)}\n",
    "        ]\n",
    "    qrys.append(qry)\n",
    "    papers.append(row['doi/arxiv_id'])\n",
    "\n",
    "print(f\"Generating responses with {model_name}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "responses = []\n",
    "for qry in tqdm(qrys):\n",
    "    for attempt in range(1, 6):\n",
    "        try:\n",
    "            res = completion(model=model_name, messages=qry, min_tokens=8)\n",
    "            responses.append(res)\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt == 5:\n",
    "                responses.append(None)\n",
    "            # otherwise, retry\n",
    "\n",
    "parsed_results = [extract_response_dict(resp) for resp in responses]\n",
    "resp_df = pd.DataFrame({\n",
    "    \"doi/arxiv_id\": papers,\n",
    "    \"parsed\":    [r.get(\"parsed\", False)        for r in parsed_results],\n",
    "    \"is_error\":  [bool(r.get(\"has_error\", True)) for r in parsed_results],\n",
    "    \"errors\":    [r.get(\"errors\", [])            for r in parsed_results],\n",
    "})\n",
    "resp_df = resp_df.merge(flat_orig_df, on=['doi/arxiv_id'])\n",
    "\n",
    "error_cases = resp_df.loc[resp_df[\"parsed\"] & resp_df[\"is_error\"], :]\n",
    "qrys = []\n",
    "for _, row in error_cases.iterrows():\n",
    "    annotations = [\n",
    "        {\"location\": loc, \"description\": desc}\n",
    "        for loc, desc in zip(row.error_location, row.error_annotation)\n",
    "    ]\n",
    "    predictions = row.errors\n",
    "    payload = {\n",
    "        \"annotations\": annotations,\n",
    "        \"predictions\": predictions\n",
    "    }\n",
    "    user_content = json.dumps(payload, ensure_ascii=False, indent=2)\n",
    "    qry = [\n",
    "        {\"role\": \"system\", \"content\": llm_judge_template},\n",
    "        {\"role\": \"user\",   \"content\": user_content},\n",
    "    ]\n",
    "    qrys.append(qry)\n",
    "\n",
    "print(f\"Generating responses with {judge_name}\")\n",
    "# Run judge model\n",
    "responses = batch_completion(\n",
    "    model=judge_name,\n",
    "    messages=qrys\n",
    ")\n",
    "parsed_results = [extract_response_dict(resp) for resp in responses]\n",
    "\n",
    "# Assemble judge results\n",
    "records = []\n",
    "for doi, res in zip(resp_df[\"doi/arxiv_id\"], parsed_results):\n",
    "    matches = res.get(\"matches\", [])\n",
    "    descriptions = [m.get(\"description\", \"\") for m in matches]\n",
    "    records.append({\n",
    "        \"doi/arxiv_id\": doi,\n",
    "        \"matches\": matches,\n",
    "        \"match_descriptions\": descriptions\n",
    "    })\n",
    "judge_df = pd.DataFrame.from_records(records)\n",
    "resp_df = resp_df.merge(judge_df, on=\"doi/arxiv_id\", how=\"left\")\n",
    "metrics = compute_all_metrics(resp_df)\n",
    "print(f\"Micro Precision: {metrics['precision_micro']:.3f}\")\n",
    "print(f\"Micro Recall:    {metrics['recall_micro']:.3f}\")\n",
    "print(f\"Macro Precision: {metrics['precision_macro']:.3f}\")\n",
    "print(f\"Macro Recall:    {metrics['recall_macro']:.3f}\")\n",
    "print(f\"PPR:             {metrics['PPR']:.3f}\")\n",
    "\n",
    "# Export results and metrics\n",
    "resp_df.to_csv(f\"results_scaling/{safe_model_name}_resp_df_{idx}.csv\", index=False)\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(f\"results_scaling/{safe_model_name}_metrics_{idx}.csv\", index=False)\n",
    "# print(f\"Exported data to results_resp_df.csv and {args.output_prefix}_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9456bebd-e8fe-46ad-9e87-115b09d11922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5096977059a5478c8953ef3ec9ca6d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a031118f31aa4277b0660cc36517caf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/238M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291f419c8bdc4ad18802460959eafcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/272268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = load_dataset('HAERAE-HUB/corearoadbike-qa-original',split='train',token='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').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3648395-6d69-4ba4-974d-9c465ea718b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['뒷변속기 교체 공임비 문의', 'TEQU', '25-04-19 12:01',\n",
       "       '샵마다 다소 상이하겠지만.. 일반적으로 뒷드레일러만 교체할때 대략 공임비 및 작업시간이 얼마나 될까요? 작업(분해)이 복잡할까요?? 11단 림브입니다!',\n",
       "       '[]',\n",
       "       \"['샵 갈필요있나요? 케이블 이상없다면 그냥 드레일러 바꾸고 케이블연결 하고 하면 됨. 샵가시면 교체하고 기어변속 보면 3만원?', 'Immanuel 《 아 간단한 작업인데 생각보다 드는군요? a', '직접하는게 절약됨 해보면 별거 아닙니다']\",\n",
       "       'https://m.corearoadbike.com/board/board.php?g_id=Menu01&t_id=Menu01Top1&no=1848362',\n",
       "       '2025-04-19 14:13:44', 1], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c6d8e-4e96-43fa-b09d-646be44a084f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
